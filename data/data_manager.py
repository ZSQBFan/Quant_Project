# data/data_manager.py

import pandas as pd
import backtrader as bt
from datetime import timedelta
import queue
import threading
from tqdm import tqdm
import logging
import sqlite3
import os
import sys
import numpy as np

from .database_handler import DatabaseHandler
from .trading_calendars import TushareTradingCalendar, AkshareTradingCalendar
from .data_providers import AkshareDataProvider, TushareDataProvider, SQLiteDataProvider

# ==============================================================================
# è¾…åŠ©å‡½æ•°
# ==============================================================================


def _calculate_forward_returns(df: pd.DataFrame,
                               periods: list) -> pd.DataFrame:
    """
    è®¡ç®—å•ä¸ªèµ„äº§çš„æœªæ¥æ”¶ç›Šç‡ã€‚
    ç”¨äº calculate_universe_forward_returns ä¸­çš„ apply æ“ä½œã€‚
    """
    # ç¡®ä¿æŒ‰æ—¥æœŸæ’åº
    df = df.sort_index()
    for p in periods:
        # shift(-p) å°†æœªæ¥çš„ä»·æ ¼å‘ä¸Šå¹³ç§»ï¼Œå¯¹é½åˆ°å½“å‰æ—¥æœŸ
        future_price = df['close'].shift(-p)
        df[f'forward_return_{p}d'] = (future_price / df['close']) - 1
    return df


# ==============================================================================
# æ ¸å¿ƒç±»: DataProviderManager
# ==============================================================================


class DataProviderManager:
    """
    ç»Ÿä¸€æ•°æ®ç®¡ç†å™¨ã€‚
    è´Ÿè´£æ•°æ®çš„ä¸‹è½½ã€æ¸…æ´—ã€å…¥åº“ï¼Œä»¥åŠå‘å› å­è®¡ç®—å™¨æä¾›æŒ‰éœ€åŠ è½½çš„æ•°æ®ã€‚
    """

    def __init__(self,
                 provider_configs,
                 symbols,
                 start_date,
                 end_date,
                 db_path='quant_data.db',
                 num_checker_threads=4,
                 num_downloader_threads=8,
                 batch_size=100,
                 auto_detect_universe: bool = True):

        self.start_date = pd.to_datetime(start_date).strftime('%Y-%m-%d')
        self.end_date = pd.to_datetime(end_date).strftime('%Y-%m-%d')
        self.provider_configs = provider_configs
        self.db_handler = DatabaseHandler(db_path)
        self.table_name = 'stock_daily_prices'  # åŸºç¡€è¡Œæƒ…è¡¨å

        # --- [å¤šçº¿ç¨‹ä¸‹è½½ç›¸å…³é…ç½®] ---
        self.num_checker_threads = num_checker_threads
        self.num_downloader_threads = num_downloader_threads
        self.batch_size = batch_size
        self.symbols_queue = queue.Queue()
        self.download_tasks_queue = queue.Queue()
        self.results_queue = queue.Queue()
        self.producers_finished_event = threading.Event()
        self.check_progress_bar = None
        self.download_progress_bar = None

        # --- [æ•°æ®æºåˆå§‹åŒ–] ---
        self._local = threading.local()

        # åˆå§‹åŒ–äº¤æ˜“æ—¥å† (ä¼˜å…ˆå°è¯• Tushareï¼Œå¤±è´¥åˆ™ç”¨ Akshare)
        try:
            self.calendar_provider = TushareTradingCalendar(self.db_handler)
        except:
            self.calendar_provider = AkshareTradingCalendar(self.db_handler)

        # --- [è‚¡ç¥¨æ± åˆå§‹åŒ–] ---
        if not symbols and auto_detect_universe:
            logging.info("â„¹ï¸ 'symbols' ä¸ºç©ºã€‚æ­£åœ¨ä»æ•°æ®åº“è‡ªåŠ¨æ£€æµ‹è‚¡ç¥¨æ± ...")
            try:
                # ä» stock_daily_prices è¡¨ä¸­è·å–æ‰€æœ‰å»é‡çš„ code
                query = f"SELECT DISTINCT code FROM {self.table_name}"
                df = self.db_handler.query_data(query)
                if df is not None and not df.empty:
                    self.symbols = df['code'].tolist()
                    logging.info(f"âœ… è‡ªåŠ¨æ£€æµ‹åˆ° {len(self.symbols)} åªè‚¡ç¥¨ã€‚")
                else:
                    logging.warning("âš ï¸ æ•°æ®åº“ä¸ºç©ºï¼Œä¸”æœªæä¾› symbols åˆ—è¡¨ã€‚")
                    self.symbols = []
            except Exception as e:
                logging.error(f"âŒ è‡ªåŠ¨æ£€æµ‹è‚¡ç¥¨æ± å¤±è´¥: {e}")
                self.symbols = []
        else:
            self.symbols = symbols if symbols else []

        # ======================================================================
        # ã€ã€ã€æ ¸å¿ƒé…ç½®ï¼šåˆ—åæ˜ å°„å­—å…¸ (å®Œæ•´ç‰ˆ)ã€‘ã€‘ã€‘
        # æ ¼å¼: 'å› å­è®¡ç®—ç”¨çš„é€šç”¨åˆ—å': ('æ•°æ®åº“è¡¨å', 'CSMARåŸå§‹å­—æ®µä»£ç ')
        # ======================================================================
        self.COLUMN_MAPPING = {
            # --- 1. æ—¥çº¿è¡Œæƒ…è¡¨ (stock_daily_prices) ---
            'open': ('stock_daily_prices', 'open'),
            'high': ('stock_daily_prices', 'high'),
            'low': ('stock_daily_prices', 'low'),
            'close': ('stock_daily_prices', 'close'),
            'volume': ('stock_daily_prices', 'volume'),
            'turnover': ('stock_daily_prices', 'turnover'),
            'pct_change': ('stock_daily_prices', 'pct_change'),
            'turnover_rate': ('stock_daily_prices', 'turnover_rate'),
            'amplitude': ('stock_daily_prices', 'amplitude'),
            'price_change': ('stock_daily_prices', 'price_change'),

            # --- 2. è¡Œä¸š/å…ƒæ•°æ®è¡¨ (stock_kind) ---
            'industry': ('stock_kind', 'Nnindnme'),  # è¡Œä¸šåç§°
            'industry_code': ('stock_kind', 'Nnindcd'),  # è¡Œä¸šä»£ç 
            'stk_name': ('stock_kind', 'Stknme'),  # è‚¡ç¥¨ç®€ç§°
            'list_date': ('stock_kind', 'Listdt'),  # ä¸Šå¸‚æ—¥æœŸ
            'ownership': ('stock_kind', 'OWNERSHIPTYPE'),  # ä¼ä¸šæ€§è´¨
            'market_type': ('stock_kind', 'Markettype'),  # å¸‚åœºç±»å‹
            'status': ('stock_kind', 'Statco'),  # ä¸Šå¸‚çŠ¶æ€

            # --- 3. èµ„äº§è´Ÿå€ºè¡¨ (Stock_BalanceSheet) ---
            # æ ¸å¿ƒæƒç›Šä¸èµ„æœ¬
            'total_equity_parent':
            ('Stock_BalanceSheet', 'A003100000'),  # å½’æ¯æ‰€æœ‰è€…æƒç›Š (B/P, ROEåˆ†æ¯)
            'share_capital':
            ('Stock_BalanceSheet', 'A003101000'),  # å®æ”¶èµ„æœ¬/è‚¡æœ¬ (è®¡ç®—å¸‚å€¼)

            # èµ„äº§ç«¯
            'total_assets': ('Stock_BalanceSheet', 'A001000000'),  # èµ„äº§æ€»è®¡
            'current_assets': ('Stock_BalanceSheet', 'A001100000'),  # æµåŠ¨èµ„äº§
            'fixed_assets': ('Stock_BalanceSheet', 'A001212000'),  # å›ºå®šèµ„äº§å‡€é¢
            'intangible_assets':
            ('Stock_BalanceSheet', 'A001218000'),  # æ— å½¢èµ„äº§å‡€é¢
            'goodwill': ('Stock_BalanceSheet', 'A001220000'),  # å•†èª‰å‡€é¢
            'inventory': ('Stock_BalanceSheet', 'A001123000'),  # å­˜è´§å‡€é¢
            'accounts_receivable':
            ('Stock_BalanceSheet', 'A001111000'),  # åº”æ”¶è´¦æ¬¾å‡€é¢

            # è´Ÿå€ºç«¯
            'total_liabilities': ('Stock_BalanceSheet', 'A002000000'),  # è´Ÿå€ºåˆè®¡
            'current_liabilities': ('Stock_BalanceSheet',
                                    'A002100000'),  # æµåŠ¨è´Ÿå€º

            # --- 4. åˆ©æ¶¦è¡¨ (stock_ProfitSheet) ---
            # æ”¶å…¥ä¸æˆæœ¬
            'total_revenue': ('stock_ProfitSheet',
                              'B001100000'),  # è¥ä¸šæ€»æ”¶å…¥ (æˆé•¿å› å­)
            'cost_of_goods_sold': ('stock_ProfitSheet', 'B001201000'),  # è¥ä¸šæˆæœ¬

            # åˆ©æ¶¦å±‚çº§
            'operating_profit': ('stock_ProfitSheet', 'B001300000'),  # è¥ä¸šåˆ©æ¶¦
            'total_profit': ('stock_ProfitSheet', 'B001000000'),  # åˆ©æ¶¦æ€»é¢
            'net_profit_parent': ('stock_ProfitSheet',
                                  'B002000101'),  # å½’æ¯å‡€åˆ©æ¶¦ (E/P, ROEåˆ†å­)

            # è´¹ç”¨ä¸ç¨
            'selling_expenses': ('stock_ProfitSheet', 'B001209000'),  # é”€å”®è´¹ç”¨
            'admin_expenses': ('stock_ProfitSheet', 'B001210000'),  # ç®¡ç†è´¹ç”¨
            'rd_finance_expenses': ('stock_ProfitSheet',
                                    'B001216000'),  # ç ”å‘/è´¢åŠ¡è´¹ç”¨
            'income_tax_expense': ('stock_ProfitSheet', 'B002100000'),  # æ‰€å¾—ç¨è´¹ç”¨

            # --- 5. ç°é‡‘æµé‡è¡¨ (stock_CashFlowDirect) ---
            # ç»è¥æ´»åŠ¨
            'net_cash_flow_ops': ('stock_CashFlowDirect',
                                  'C001000000'),  # ç»è¥æ´»åŠ¨ç°é‡‘æµå‡€é¢ (CFO)

            # æŠ•èµ„æ´»åŠ¨
            'net_cash_flow_inv': ('stock_CashFlowDirect',
                                  'C002000000'),  # æŠ•èµ„æ´»åŠ¨ç°é‡‘æµå‡€é¢ (CFI)
            'capex': ('stock_CashFlowDirect',
                      'C002006000'),  # è´­å»ºé•¿æœŸèµ„äº§æ”¯ä»˜ (CapEx)

            # ç­¹èµ„æ´»åŠ¨
            'net_cash_flow_fin': ('stock_CashFlowDirect',
                                  'C003000000'),  # ç­¹èµ„æ´»åŠ¨ç°é‡‘æµå‡€é¢ (CFF)
            'borrowing_cash': ('stock_CashFlowDirect',
                               'C003002000'),  # å–å¾—å€Ÿæ¬¾æ”¶åˆ°çš„ç°é‡‘
            'dividends_paid': ('stock_CashFlowDirect',
                               'C003005000'),  # åˆ†é…è‚¡åˆ©/åˆ©æ¯æ”¯ä»˜
            'other_fin_payment': ('stock_CashFlowDirect',
                                  'C003006000'),  # æ”¯ä»˜å…¶ä»–ç­¹èµ„ç°é‡‘
        }

    def _get_provider(self, name):
        """è·å–çº¿ç¨‹æœ¬åœ°çš„æ•°æ®æä¾›è€…å®ä¾‹ (ä¿æŒçº¿ç¨‹å®‰å…¨)ã€‚"""
        if not hasattr(self._local, 'providers'):
            self._local.providers = {}

        # ç®€å•çš„ç¼“å­˜æœºåˆ¶
        if name not in self._local.providers:
            for cls, kwargs in self.provider_configs:
                # åŒ¹é…é…ç½®ä¸­çš„ç±»å
                if cls.__name__ == name or (name == 'sqlite' and cls.__name__
                                            == 'SQLiteDataProvider'):
                    self._local.providers[name] = cls(**kwargs)
                    break
        return self._local.providers.get(name)

    # ==========================================================================
    # ã€ã€ã€æ ¸å¿ƒæ–¹æ³• 1ï¼šæŒ‰éœ€è·å–å•åªè‚¡ç¥¨è¡Œæƒ…ã€‘ã€‘ã€‘
    # ==========================================================================
    def get_dataframe(self,
                      symbol: str,
                      columns: list = None) -> pd.DataFrame | None:
        """
        ä»æ•°æ®åº“è·å–å•åªè‚¡ç¥¨çš„ã€æ—¥çº¿è¡Œæƒ…æ•°æ®ã€‘ã€‚
        æ”¯æŒåˆ—ç­›é€‰ï¼Œä»…æŸ¥è¯¢ stock_daily_prices è¡¨ã€‚

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            columns: éœ€è¦çš„åˆ—ååˆ—è¡¨ (ä¾‹å¦‚ ['close', 'volume'])ã€‚å¦‚æœä¸ä¼ åˆ™æŸ¥æ‰€æœ‰ã€‚

        Returns:
            pd.DataFrame: ç´¢å¼•ä¸º date çš„æ•°æ®æ¡†
        """
        table_name = 'stock_daily_prices'

        # 1. æ„å»º SQL æŸ¥è¯¢å­—æ®µ
        if columns:
            # æ€»æ˜¯åŒ…å« dateï¼Œå› ä¸ºå®ƒæ˜¯ç´¢å¼•
            query_cols = ['date']
            for col in columns:
                # æŸ¥è¡¨ï¼šåªå¤„ç†å±äº stock_daily_prices çš„åˆ—
                mapping = self.COLUMN_MAPPING.get(col)
                if mapping and mapping[0] == table_name:
                    query_cols.append(mapping[1])  # æ·»åŠ åŸå§‹åˆ—å

                # ä¿åº•é€»è¾‘ï¼šå¦‚æœåˆ—åä¸åœ¨æ˜ å°„ä¸­ï¼Œä½†çœ‹èµ·æ¥åƒåŸºç¡€è¡Œæƒ…ï¼Œä¹Ÿå°è¯•æŸ¥è¯¢
                elif col in [
                        'open', 'high', 'low', 'close', 'volume', 'turnover'
                ]:
                    query_cols.append(col)

            # å»é‡å¹¶è½¬ä¸ºå­—ç¬¦ä¸²
            query_cols = list(set(query_cols))
            cols_str = ", ".join(query_cols)
        else:
            # å¦‚æœæœªæŒ‡å®šï¼Œé»˜è®¤æŸ¥æ‰€æœ‰
            cols_str = "*"

        # 2. æ„å»º SQL è¯­å¥
        # ä½¿ç”¨ BETWEEN ä¼˜åŒ–æ—¥æœŸèŒƒå›´æŸ¥è¯¢
        query = f"SELECT {cols_str} FROM {table_name} WHERE code = ? AND date BETWEEN ? AND ?"
        params = (symbol, self.start_date, self.end_date)

        try:
            # æ‰§è¡ŒæŸ¥è¯¢
            df = self.db_handler.query_data(query, params)
            if df is None or df.empty:
                return None

            # ç¡®ä¿ date æ˜¯ datetime ç±»å‹å¹¶è®¾ä¸ºç´¢å¼•
            if 'date' in df.columns:
                df['date'] = pd.to_datetime(df['date'])
                df.set_index('date', inplace=True)

            return df
        except Exception as e:
            logging.error(f"âŒ è·å– {symbol} æ•°æ®å¤±è´¥: {e}")
            return None

    # ==========================================================================
    # ã€ã€ã€æ ¸å¿ƒæ–¹æ³• 2 (å‡çº§ç‰ˆ)ï¼šè·å–å…¨å¸‚åœºåˆå¹¶æ•°æ® (æ”¯æŒåŸºæœ¬é¢)ã€‘ã€‘ã€‘
    # ==========================================================================
    def get_all_data_for_universe(
            self,
            universe: list,
            required_columns: list = None) -> pd.DataFrame:
        logging.info(f"âš™ï¸ [æ•°æ®åŠ è½½] æ­£åœ¨è§£æ {len(universe)} åªè‚¡ç¥¨çš„æ•°æ®éœ€æ±‚...")

        # 1. éœ€æ±‚åˆ†æ
        table_col_map = {}
        load_industry = False

        if required_columns:
            for col in required_columns:
                if col == 'industry':
                    load_industry = True
                    continue
                mapping = self.COLUMN_MAPPING.get(col)
                if mapping:
                    table_name, db_col = mapping
                    if table_name not in table_col_map:
                        table_col_map[table_name] = []
                    table_col_map[table_name].append(col)
                elif col in ['open', 'high', 'low', 'close', 'volume']:
                    if 'stock_daily_prices' not in table_col_map:
                        table_col_map['stock_daily_prices'] = []
                    table_col_map['stock_daily_prices'].append(col)

        # 2. é¢„åŠ è½½è¡Œä¸šæ•°æ®
        industry_map = {}
        if load_industry:
            logging.info("  > æ­£åœ¨é¢„åŠ è½½è¡Œä¸šæ•°æ®...")
            ind_query = "SELECT Stkcd, Nnindnme FROM stock_kind"
            ind_df = self.db_handler.query_data(ind_query)
            if ind_df is not None and not ind_df.empty:
                ind_df['Stkcd'] = ind_df['Stkcd'].astype(str).str.zfill(6)
                industry_map = ind_df.set_index('Stkcd')['Nnindnme'].to_dict()

        # ======================================================================
        # ã€ã€ã€ä¼˜åŒ–é‡ç‚¹ã€‘ã€‘ã€‘: åŸºæœ¬é¢æ•°æ®å…¨é‡é¢„åŠ è½½
        # é¿å…åœ¨å¾ªç¯ä¸­è¿›è¡Œ 5000+ æ¬¡ SQL æŸ¥è¯¢
        # ======================================================================
        fundamental_cache = {
        }  # ç»“æ„: { 'Stock_BalanceSheet': { '600519': df, ... }, ... }

        for table_name, cols in table_col_map.items():
            if table_name == 'stock_daily_prices': continue

            logging.info(f"  > ğŸš€ [é¢„åŠ è½½] æ­£åœ¨å…¨é‡è¯»å– {table_name} ({cols})...")
            # é¢„åŠ è½½è¯¥è¡¨æ‰€æœ‰è‚¡ç¥¨çš„æ•°æ®
            df_all = self._preload_fundamental_table(table_name, cols)

            if df_all is not None and not df_all.empty:
                # æŒ‰è‚¡ç¥¨ä»£ç åˆ†ç»„ï¼Œå­˜å…¥å­—å…¸
                # å‡è®¾ Stkcd æ˜¯ç´¢å¼•çš„ä¸€éƒ¨åˆ†æˆ–åˆ—
                # _preload_fundamental_table è¿”å›çš„ df åŒ…å« 'Stkcd' å’Œ 'date'

                # å°†å¤§è¡¨æ‹†åˆ†ä¸ºå°å­—å…¸ï¼Œkeyæ˜¯è‚¡ç¥¨ä»£ç  (str)
                #è¿™ä¸€æ­¥è™½ç„¶è€—æ—¶ï¼Œä½†æ¯”å‡ åƒæ¬¡SQLå¿«å¾—å¤š
                grouped = df_all.groupby('Stkcd')
                fundamental_cache[table_name] = {
                    str(k).zfill(6): v
                    for k, v in grouped
                }
                logging.info(
                    f"    > å·²ç¼“å­˜ {len(fundamental_cache[table_name])} åªè‚¡ç¥¨çš„ {table_name} æ•°æ®ã€‚"
                )

        # 3. ä¸»å¾ªç¯
        all_dfs = []
        logging.info(f"ğŸš€ å¼€å§‹åˆå¹¶æ•°æ®...")

        # åªéœ€è¦æŸ¥è¡Œæƒ…çš„åˆ—
        price_cols = table_col_map.get('stock_daily_prices', [])

        for symbol in tqdm(universe, desc="[Data Load]"):
            # A. è·å–åŸºç¡€è¡Œæƒ… (è¿™ä¸ªå¿…é¡»é€ä¸ªæŸ¥ï¼Œå› ä¸ºå…¨é‡æŸ¥è¡Œæƒ…è¡¨å†…å­˜ä¼šçˆ†)
            df_price = self.get_dataframe(symbol, columns=price_cols)

            if df_price is None or df_price.empty:
                continue

            # B. å†…å­˜åˆå¹¶åŸºæœ¬é¢æ•°æ®
            for table_name in fundamental_cache:
                # ä»ç¼“å­˜å­—å…¸é‡Œç›´æ¥å–ï¼Œä¸éœ€è¦ SQL
                stock_fund_data = fundamental_cache[table_name].get(symbol)

                if stock_fund_data is not None:
                    # stock_fund_data å·²ç»æ¸…æ´—è¿‡ï¼Œindexæ˜¯ date
                    # åˆ é™¤ Stkcd åˆ—é˜²æ­¢é‡å
                    if 'Stkcd' in stock_fund_data.columns:
                        stock_fund_data = stock_fund_data.drop(
                            columns=['Stkcd'])

                    df_price = df_price.join(stock_fund_data, how='left')
                    df_price = df_price.ffill()  # å¡«å……

            # C. åˆå¹¶è¡Œä¸š
            if load_industry:
                df_price['industry'] = industry_map.get(symbol)

            df_price['asset'] = symbol
            all_dfs.append(df_price)

        if not all_dfs:
            return pd.DataFrame()

        logging.info("âš™ï¸ æ­£åœ¨å †å æ•°æ®æ¡†...")
        final_df = pd.concat(all_dfs)
        final_df.reset_index(inplace=True)
        final_df.set_index(['date', 'asset'], inplace=True)
        final_df.sort_index(inplace=True)

        return final_df

    def _preload_fundamental_table(self, table_name: str,
                                   required_cols: list) -> pd.DataFrame | None:
        """
        ã€è¾…åŠ©æ–¹æ³•ã€‘å…¨é‡è¯»å–ä¸€å¼ åŸºæœ¬é¢è¡¨ï¼Œå¹¶æ¸…æ´—åˆ—åã€‚
        """
        db_cols = []
        rename_map = {}
        for col in required_cols:
            mapping = self.COLUMN_MAPPING.get(col)
            if mapping:
                db_cols.append(mapping[1])
                rename_map[mapping[1]] = col

        if not db_cols: return None

        # è¯»å– Stkcd, Accper å’Œæ‰€éœ€åˆ—
        cols_str = ", ".join(['Stkcd', 'Accper'] + db_cols)
        # é™åˆ¶æ—¶é—´èŒƒå›´ä¼˜åŒ–æ€§èƒ½ (å¯é€‰ï¼Œè¿™é‡ŒæŸ¥å…¨é‡æ¯”è¾ƒå®‰å…¨)
        query = f"SELECT {cols_str} FROM {table_name} ORDER BY Accper"

        try:
            df = self.db_handler.query_data(query)
            if df is None or df.empty: return None

            # æ¸…æ´—
            df['date'] = pd.to_datetime(df['Accper'])
            df.set_index('date', inplace=True)
            df.drop(columns=['Accper'], inplace=True, errors='ignore')
            df.rename(columns=rename_map, inplace=True)

            # å¼ºåˆ¶è½¬æ•°å€¼
            cols_to_numeric = list(rename_map.values())
            df[cols_to_numeric] = df[cols_to_numeric].apply(pd.to_numeric,
                                                            errors='coerce')

            # å»é‡ (Stkcd + Date å”¯ä¸€)
            # å› ä¸ºæˆ‘ä»¬è¦ groupby Stkcdï¼Œæ‰€ä»¥è¿™é‡Œå…ˆä¸ drop Stkcd
            if not df.empty:
                # reset_index ä¼šæŠŠ date (index) å˜æˆåˆ—ï¼Œä»è€Œå¯ä»¥å¯¹ ['date', 'Stkcd'] è”åˆå»é‡
                df = df.reset_index()
                df = df.drop_duplicates(subset=['date', 'Stkcd'], keep='last')
                df = df.set_index('date')

            return df
        except Exception as e:
            logging.error(f"å…¨é‡é¢„åŠ è½½å¤±è´¥ {table_name}: {e}")
            return None

    def _get_fundamental_data(self, symbol: str, table_name: str,
                              required_cols: list) -> pd.DataFrame | None:
        """
        ã€è¾…åŠ©æ–¹æ³•ã€‘æŸ¥è¯¢å•å¼ åŸºæœ¬é¢è¡¨çš„æ•°æ®ã€‚
        è¿”å›: indexä¸ºdate(Accper)çš„DataFrameï¼Œåˆ—åä¸ºé€šç”¨åˆ—å(å¦‚ share_capital)ã€‚
        """
        # 1. æ‰¾åˆ°è¿™äº›é€šç”¨åˆ—å¯¹åº”çš„æ•°æ®åº“åŸå§‹åˆ—å
        db_cols = []
        rename_map = {}

        for col in required_cols:
            mapping = self.COLUMN_MAPPING.get(col)
            if mapping:
                original_col = mapping[1]
                db_cols.append(original_col)
                rename_map[original_col] = col

        if not db_cols: return None

        # 2. æ„å»ºæŸ¥è¯¢
        cols_str = ", ".join(['Accper'] + db_cols)

        # å°è¯•å¤„ç† symbol æ ¼å¼ (CSMARé€šå¸¸ç”¨æ•´æ•°å­˜Stkcd)
        try:
            symbol_int = int(symbol)
        except:
            symbol_int = symbol

        query = f"SELECT {cols_str} FROM {table_name} WHERE Stkcd = ? ORDER BY Accper"

        try:
            df = self.db_handler.query_data(query, (symbol_int, ))
            if df is None or df.empty: return None

            # 3. æ¸…æ´—æ•°æ®
            df['date'] = pd.to_datetime(df['Accper'])
            df.set_index('date', inplace=True)
            df.drop(columns=['Accper'], inplace=True, errors='ignore')

            # é‡å‘½åå›é€šç”¨åˆ—å
            df.rename(columns=rename_map, inplace=True)

            # è½¬æ¢æ•°æ®ç±»å‹ä¸ºæ•°å€¼å‹ (é˜²æ­¢å­—ç¬¦ä¸²)
            df = df.apply(pd.to_numeric, errors='coerce')

            # å»é‡ (é˜²æ­¢åŒä¸€å¤©å‘å¸ƒä¸¤æ¬¡è´¢æŠ¥å¯¼è‡´çš„ç´¢å¼•å†²çª)
            df = df[~df.index.duplicated(keep='last')]

            return df
        except Exception as e:
            return None

    def calculate_universe_forward_returns(
            self, universe: list,
            forward_return_periods: list) -> pd.DataFrame:
        """
        ç»Ÿä¸€è®¡ç®—æœªæ¥æ”¶ç›Šç‡ã€‚
        ä¼˜åŒ–ï¼šåªåŠ è½½ close åˆ—ã€‚
        """
        logging.info(f"âš™ï¸ [æ”¶ç›Šç‡è®¡ç®—] æ­£åœ¨ä¸º {len(universe)} åªè‚¡ç¥¨è®¡ç®—æœªæ¥æ”¶ç›Š...")

        # ä»…åŠ è½½ close åˆ—ï¼Œæå¤§æå‡é€Ÿåº¦
        all_data = self.get_all_data_for_universe(universe,
                                                  required_columns=['close'])

        if all_data is None or all_data.empty:
            logging.error("âŒ æ— æ³•åŠ è½½æ•°æ®ç”¨äºè®¡ç®—æ”¶ç›Šç‡ã€‚")
            return None

        # ä½¿ç”¨ groupby().apply()
        # group_keys=False é˜²æ­¢ç´¢å¼•å±‚çº§å¢åŠ 
        returns_df = all_data.groupby(level='asset', group_keys=False).apply(
            lambda x: _calculate_forward_returns(x, forward_return_periods))

        # ç­›é€‰å‡ºéœ€è¦çš„åˆ— (date, asset, forward_return_*)
        # ç”±äº apply åç´¢å¼•æ˜¯ (date, asset)ï¼Œæˆ‘ä»¬éœ€è¦ reset_index æ¥è·å¾—è¿™ä¸¤åˆ—
        returns_df.reset_index(inplace=True)

        cols_to_keep = ['date', 'asset'] + [
            f'forward_return_{p}d' for p in forward_return_periods
        ]
        return returns_df[cols_to_keep]

    # ==========================================================================
    # è·å–è¡Œä¸šæ˜ å°„ (å…¼å®¹æ—§ä»£ç )
    # ==========================================================================
    def get_industry_mapping(self) -> pd.DataFrame | None:
        logging.info("  > âš™ï¸ æ­£åœ¨åŠ è½½è¡Œä¸šæ˜ å°„æ•°æ®...")
        query = "SELECT Stkcd, Nnindnme FROM stock_kind"
        try:
            df = self.db_handler.query_data(query)
            if df is not None:
                df['asset'] = df['Stkcd'].astype(str).str.zfill(6)
                df.rename(columns={'Nnindnme': 'industry'}, inplace=True)
                return df[['asset', 'industry']]
            return None
        except Exception as e:
            logging.error(f"âŒ åŠ è½½è¡Œä¸šæ•°æ®å¤±è´¥: {e}")
            return None

    # ==========================================================================
    # ä¸‹é¢æ˜¯æ•°æ®ä¸‹è½½/æ›´æ–°æµç¨‹ (ä¿æŒåŸæœ‰é€»è¾‘)
    # ==========================================================================
    def prepare_data_for_universe(self):
        if not self.provider_configs:
            logging.info("â„¹ï¸ æœªé…ç½®æ•°æ®æºï¼Œè·³è¿‡ä¸‹è½½ã€‚")
            return

        logging.info("--- ğŸ å¼€å§‹æ•°æ®å‡†å¤‡æµç¨‹ (ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼) ---")

        # 1. è·å–æ—¥å†
        logging.info("ğŸ—“ï¸ æ­£åœ¨è·å–äº¤æ˜“æ—¥å†...")
        try:
            all_trade_dates_str = self.calendar_provider.get_trading_days(
                self.start_date, self.end_date)
            self.all_trade_dates_set = set(
                pd.to_datetime(all_trade_dates_str).date)
        except Exception as e:
            logging.critical(f"âŒ è·å–äº¤æ˜“æ—¥å†å¤±è´¥: {e}")
            return

        # 2. å¡«å……æ£€æŸ¥é˜Ÿåˆ—
        for symbol in self.symbols:
            self.symbols_queue.put(symbol)

        # 3. å¯åŠ¨æ£€æŸ¥çº¿ç¨‹
        logging.info(f"ğŸ” å¯åŠ¨ {self.num_checker_threads} ä¸ªæ£€æŸ¥çº¿ç¨‹...")
        with tqdm(total=len(self.symbols),
                  desc="[æ•°æ®æ£€æŸ¥]",
                  ncols=100,
                  file=sys.stdout) as pbar:
            self.check_progress_bar = pbar
            threads = []
            for i in range(self.num_checker_threads):
                t = threading.Thread(target=self._checker_worker,
                                     name=f"Checker-{i}",
                                     daemon=True)
                t.start()
                threads.append(t)
            for t in threads:
                t.join()

        self.check_progress_bar = None

        # 4. å¯åŠ¨ä¸‹è½½å’Œå†™å…¥
        total_downloads = self.download_tasks_queue.qsize()
        if total_downloads > 0:
            logging.info(f"ğŸ“¥ å…± {total_downloads} åªè‚¡ç¥¨éœ€è¦ä¸‹è½½/æ›´æ–°ã€‚")
            logging.info(
                f"ğŸš€ å¯åŠ¨ {self.num_downloader_threads} ä¸ªä¸‹è½½çº¿ç¨‹ å’Œ 1 ä¸ªå†™å…¥çº¿ç¨‹...")

            writer_thread = threading.Thread(target=self._consumer_worker,
                                             name="DB-Writer",
                                             daemon=True)
            writer_thread.start()

            with tqdm(total=total_downloads,
                      desc="[æ•°æ®ä¸‹è½½]",
                      ncols=100,
                      file=sys.stdout) as pbar:
                self.download_progress_bar = pbar
                dl_threads = []
                for i in range(self.num_downloader_threads):
                    t = threading.Thread(target=self._producer_worker,
                                         name=f"Downloader-{i}",
                                         daemon=True)
                    t.start()
                    dl_threads.append(t)
                for t in dl_threads:
                    t.join()

            self.producers_finished_event.set()
            writer_thread.join()
            logging.info("--- âœ… æ•°æ®å‡†å¤‡æµç¨‹ç»“æŸ ---")
        else:
            logging.info("âœ… æ‰€æœ‰æ•°æ®å·²å®Œæ•´ï¼Œæ— éœ€ä¸‹è½½ã€‚")

    def _checker_worker(self):
        while True:
            try:
                symbol = self.symbols_queue.get_nowait()
            except queue.Empty:
                break

            missing = self._find_missing_date_ranges(symbol)
            if missing:
                self.download_tasks_queue.put((symbol, missing))

            self.symbols_queue.task_done()
            if self.check_progress_bar: self.check_progress_bar.update(1)

    def _find_missing_date_ranges(self, symbol):
        # ç®€åŒ–çš„æ£€æŸ¥é€»è¾‘ï¼šæŸ¥è¯¢å·²æœ‰æ•°æ®çš„æ—¥æœŸé›†åˆï¼Œä¸æ—¥å†å¯¹æ¯”
        query = f"SELECT date FROM {self.table_name} WHERE code = ?"
        df = self.db_handler.query_data(query, (symbol, ))
        if df is None or df.empty:
            return [(pd.to_datetime(self.start_date).date(),
                     pd.to_datetime(self.end_date).date())]

        existing_dates = set(pd.to_datetime(df['date']).dt.date)
        missing_dates = sorted(list(self.all_trade_dates_set - existing_dates))

        if not missing_dates: return []

        # å°†ç¦»æ•£æ—¥æœŸåˆå¹¶ä¸ºåŒºé—´ (ç®€åŒ–å¤„ç†ï¼Œè¿™é‡Œç›´æ¥è¿”å›èµ·æ­¢æ—¶é—´ï¼Œå®é™…ä¸‹è½½ä¼šè¦†ç›–ä¸­é—´å·²æœ‰çš„)
        return [(missing_dates[0], missing_dates[-1])]

    def _producer_worker(self):
        while True:
            try:
                task = self.download_tasks_queue.get_nowait()
            except queue.Empty:
                break

            symbol, ranges = task
            # ç®€å•èµ·è§ï¼Œå–ç¬¬ä¸€ä¸ªåŒºé—´çš„èµ·æ­¢
            start, end = ranges[0]

            # ä¼˜å…ˆå°è¯• SQLite (å¦‚æœæ˜¯æœ¬åœ°æº)ï¼Œå¦åˆ™å°è¯• Tushare/Akshare
            # è¿™é‡Œç®€åŒ–é€»è¾‘ï¼Œç›´æ¥éå† providers
            df = None
            for name, provider in self._local.providers.items():
                try:
                    df = provider.get_daily_price(symbol,
                                                  start.strftime('%Y%m%d'),
                                                  end.strftime('%Y%m%d'))
                    if df is not None and not df.empty:
                        break
                except:
                    continue

            if df is not None and not df.empty:
                self.results_queue.put(df)

            self.download_tasks_queue.task_done()
            if self.download_progress_bar: self.download_progress_bar.update(1)

    def _consumer_worker(self):
        batch = []
        while not (self.producers_finished_event.is_set()
                   and self.results_queue.empty()):
            try:
                df = self.results_queue.get(timeout=1)
                batch.append(df)
                if len(batch) >= self.batch_size:
                    self._save_batch(batch)
                    batch = []
            except queue.Empty:
                continue
        if batch: self._save_batch(batch)

    def _save_batch(self, batch):
        if not batch: return
        try:
            full_df = pd.concat(batch)
            # ç¡®ä¿ code åˆ—å­˜åœ¨ (provider è¿”å›çš„æ•°æ®åº”è¯¥åŒ…å« code)
            self.db_handler.save_data(full_df, self.table_name)
        except Exception as e:
            logging.error(f"âŒ æ‰¹é‡å†™å…¥å¤±è´¥: {e}")

    def __del__(self):
        if hasattr(self, 'db_handler'):
            self.db_handler.close_connection()
