# data/data_manager.py

import pandas as pd
import backtrader as bt
from datetime import timedelta
import queue
import threading
from tqdm import tqdm
import logging
import sqlite3
import os
import sys
import numpy as np

from .database_handler import DatabaseHandler
from .trading_calendars import TushareTradingCalendar, AkshareTradingCalendar
from .data_providers import AkshareDataProvider, TushareDataProvider, SQLiteDataProvider

# ==============================================================================
# è¾…åŠ©å‡½æ•°
# ==============================================================================


def _calculate_forward_returns(df: pd.DataFrame,
                               periods: list) -> pd.DataFrame:
    """
    è®¡ç®—å•ä¸ªèµ„äº§çš„æœªæ¥æ”¶ç›Šç‡ã€‚
    ç”¨äº calculate_universe_forward_returns ä¸­çš„ apply æ“ä½œã€‚
    """
    # ç¡®ä¿æŒ‰æ—¥æœŸæ’åº
    df = df.sort_index()
    for p in periods:
        # shift(-p) å°†æœªæ¥çš„ä»·æ ¼å‘ä¸Šå¹³ç§»ï¼Œå¯¹é½åˆ°å½“å‰æ—¥æœŸ
        future_price = df['close'].shift(-p)
        df[f'forward_return_{p}d'] = (future_price / df['close']) - 1
    return df


# ==============================================================================
# æ ¸å¿ƒç±»: DataProviderManager
# ==============================================================================


class DataProviderManager:
    """
    ç»Ÿä¸€æ•°æ®ç®¡ç†å™¨ã€‚
    è´Ÿè´£æ•°æ®çš„ä¸‹è½½ã€æ¸…æ´—ã€å…¥åº“ï¼Œä»¥åŠå‘å› å­è®¡ç®—å™¨æä¾›æŒ‰éœ€åŠ è½½çš„æ•°æ®ã€‚
    """

    def __init__(self,
                 provider_configs,
                 symbols,
                 start_date,
                 end_date,
                 db_path='quant_data.db',
                 num_checker_threads=4,
                 num_downloader_threads=8,
                 batch_size=100,
                 auto_detect_universe: bool = True):

        self.start_date = pd.to_datetime(start_date).strftime('%Y-%m-%d')
        self.end_date = pd.to_datetime(end_date).strftime('%Y-%m-%d')
        self.provider_configs = provider_configs
        self.db_handler = DatabaseHandler(db_path)
        self.table_name = 'stock_daily_prices'  # åŸºç¡€è¡Œæƒ…è¡¨å

        # --- [å¤šçº¿ç¨‹ä¸‹è½½ç›¸å…³é…ç½®] ---
        self.num_checker_threads = num_checker_threads
        self.num_downloader_threads = num_downloader_threads
        self.batch_size = batch_size
        self.symbols_queue = queue.Queue()
        self.download_tasks_queue = queue.Queue()
        self.results_queue = queue.Queue()
        self.producers_finished_event = threading.Event()
        self.check_progress_bar = None
        self.download_progress_bar = None

        # --- [æ•°æ®æºåˆå§‹åŒ–] ---
        self._local = threading.local()

        # åˆå§‹åŒ–äº¤æ˜“æ—¥å† (ä¼˜å…ˆå°è¯• Tushareï¼Œå¤±è´¥åˆ™ç”¨ Akshare)
        try:
            self.calendar_provider = TushareTradingCalendar(self.db_handler)
        except:
            self.calendar_provider = AkshareTradingCalendar(self.db_handler)

        # --- [è‚¡ç¥¨æ± åˆå§‹åŒ–] ---
        if not symbols and auto_detect_universe:
            logging.info("â„¹ï¸ 'symbols' ä¸ºç©ºã€‚æ­£åœ¨ä»æ•°æ®åº“è‡ªåŠ¨æ£€æµ‹è‚¡ç¥¨æ± ...")
            try:
                # ä» stock_daily_prices è¡¨ä¸­è·å–æ‰€æœ‰å»é‡çš„ code
                query = f"SELECT DISTINCT code FROM {self.table_name}"
                df = self.db_handler.query_data(query)
                if df is not None and not df.empty:
                    self.symbols = df['code'].tolist()
                    logging.info(f"âœ… è‡ªåŠ¨æ£€æµ‹åˆ° {len(self.symbols)} åªè‚¡ç¥¨ã€‚")
                else:
                    logging.warning("âš ï¸ æ•°æ®åº“ä¸ºç©ºï¼Œä¸”æœªæä¾› symbols åˆ—è¡¨ã€‚")
                    self.symbols = []
            except Exception as e:
                logging.error(f"âŒ è‡ªåŠ¨æ£€æµ‹è‚¡ç¥¨æ± å¤±è´¥: {e}")
                self.symbols = []
        else:
            self.symbols = symbols if symbols else []

        # ======================================================================
        # ã€ã€ã€æ ¸å¿ƒé…ç½®ï¼šåˆ—åæ˜ å°„å­—å…¸ã€‘ã€‘ã€‘
        # ç”¨äºå‘Šè¯‰ç¨‹åºï¼šå½“æˆ‘ä»¬æƒ³è¦æŸä¸ªâ€œå› å­æ‰€éœ€çš„åˆ—â€æ—¶ï¼Œåº”è¯¥å»å“ªä¸ªè¡¨ã€å“ªä¸ªå­—æ®µæ‰¾ã€‚
        # ======================================================================
        self.COLUMN_MAPPING = {
            # --- 1. æ—¥çº¿è¡Œæƒ…è¡¨ (stock_daily_prices) ---
            'open': ('stock_daily_prices', 'open'),
            'high': ('stock_daily_prices', 'high'),
            'low': ('stock_daily_prices', 'low'),
            'close': ('stock_daily_prices', 'close'),
            'volume': ('stock_daily_prices', 'volume'),
            'turnover': ('stock_daily_prices', 'turnover'),
            'pct_change': ('stock_daily_prices', 'pct_change'),
            'turnover_rate': ('stock_daily_prices', 'turnover_rate'),

            # --- 2. è¡Œä¸š/å…ƒæ•°æ®è¡¨ (stock_kind) ---
            'industry': ('stock_kind', 'Nnindnme'),  # è¡Œä¸šåç§°
            'stk_name': ('stock_kind', 'Stknme'),  # è‚¡ç¥¨ç®€ç§°
            'list_date': ('stock_kind', 'Listdt'),  # ä¸Šå¸‚æ—¥æœŸ

            # --- 3. èµ„äº§è´Ÿå€ºè¡¨ (Stock_BalanceSheet) ---
            'total_equity_parent':
            ('Stock_BalanceSheet', 'A003100000'),  # å½’æ¯æ‰€æœ‰è€…æƒç›Š (B/P, ROEåˆ†æ¯)
            'total_assets': ('Stock_BalanceSheet', 'A001000000'),  # èµ„äº§æ€»è®¡
            'total_liabilities': ('Stock_BalanceSheet', 'A002000000'),  # è´Ÿå€ºåˆè®¡
            'share_capital':
            ('Stock_BalanceSheet', 'A003101000'),  # å®æ”¶èµ„æœ¬/è‚¡æœ¬ (è®¡ç®—å¸‚å€¼)
            'current_assets': ('Stock_BalanceSheet', 'A001100000'),  # æµåŠ¨èµ„äº§
            'current_liabilities':
            ('Stock_BalanceSheet', 'A002100000'),  # æµåŠ¨è´Ÿå€º
            'inventory': ('Stock_BalanceSheet', 'A001123000'),  # å­˜è´§å‡€é¢
            'accounts_receivable':
            ('Stock_BalanceSheet', 'A001111000'),  # åº”æ”¶è´¦æ¬¾å‡€é¢
            'fixed_assets': ('Stock_BalanceSheet', 'A001212000'),  # å›ºå®šèµ„äº§å‡€é¢
            'intangible_assets': ('Stock_BalanceSheet',
                                  'A001218000'),  # æ— å½¢èµ„äº§å‡€é¢
            'goodwill': ('Stock_BalanceSheet', 'A001220000'),  # å•†èª‰å‡€é¢

            # --- 4. åˆ©æ¶¦è¡¨ (stock_ProfitSheet) ---
            'total_revenue': ('stock_ProfitSheet',
                              'B001100000'),  # è¥ä¸šæ€»æ”¶å…¥ (æˆé•¿å› å­)
            'cost_of_goods_sold': ('stock_ProfitSheet', 'B001201000'),  # è¥ä¸šæˆæœ¬
            'operating_profit': ('stock_ProfitSheet', 'B001300000'),  # è¥ä¸šåˆ©æ¶¦
            'total_profit': ('stock_ProfitSheet', 'B001000000'),  # åˆ©æ¶¦æ€»é¢
            'net_profit_parent': ('stock_ProfitSheet',
                                  'B002000101'),  # å½’æ¯å‡€åˆ©æ¶¦ (E/P, ROEåˆ†å­)
            'income_tax_expense': ('stock_ProfitSheet', 'B002100000'),  # æ‰€å¾—ç¨è´¹ç”¨
            'selling_expenses': ('stock_ProfitSheet', 'B001209000'),  # é”€å”®è´¹ç”¨
            'admin_expenses': ('stock_ProfitSheet', 'B001210000'),  # ç®¡ç†è´¹ç”¨
            'rd_expenses': ('stock_ProfitSheet', 'B001216000'),  # ç ”å‘è´¹ç”¨

            # --- 5. ç°é‡‘æµé‡è¡¨ (stock_CashFlowDirect) ---
            'net_cash_flow_ops': ('stock_CashFlowDirect',
                                  'C001000000'),  # ç»è¥æ´»åŠ¨ç°é‡‘æµå‡€é¢ (CFO)
            'net_cash_flow_inv': ('stock_CashFlowDirect',
                                  'C002000000'),  # æŠ•èµ„æ´»åŠ¨ç°é‡‘æµå‡€é¢ (CFI)
            'net_cash_flow_fin': ('stock_CashFlowDirect',
                                  'C003000000'),  # ç­¹èµ„æ´»åŠ¨ç°é‡‘æµå‡€é¢ (CFF)
            'capex': ('stock_CashFlowDirect',
                      'C002006000'),  # è´­å»ºé•¿æœŸèµ„äº§æ”¯ä»˜ (CapEx)
            'dividends_paid': ('stock_CashFlowDirect',
                               'C003005000'),  # åˆ†é…è‚¡åˆ©/åˆ©æ¯æ”¯ä»˜
        }

    def _get_provider(self, name):
        """è·å–çº¿ç¨‹æœ¬åœ°çš„æ•°æ®æä¾›è€…å®ä¾‹ (ä¿æŒçº¿ç¨‹å®‰å…¨)ã€‚"""
        if not hasattr(self._local, 'providers'):
            self._local.providers = {}

        # ç®€å•çš„ç¼“å­˜æœºåˆ¶
        if name not in self._local.providers:
            for cls, kwargs in self.provider_configs:
                # åŒ¹é…é…ç½®ä¸­çš„ç±»å
                if cls.__name__ == name or (name == 'sqlite' and cls.__name__
                                            == 'SQLiteDataProvider'):
                    self._local.providers[name] = cls(**kwargs)
                    break
        return self._local.providers.get(name)

    # ==========================================================================
    # ã€ã€ã€æ ¸å¿ƒæ–¹æ³• 1ï¼šæŒ‰éœ€è·å–å•åªè‚¡ç¥¨è¡Œæƒ…ã€‘ã€‘ã€‘
    # ==========================================================================
    def get_dataframe(self,
                      symbol: str,
                      columns: list = None) -> pd.DataFrame | None:
        """
        ä»æ•°æ®åº“è·å–å•åªè‚¡ç¥¨çš„ã€æ—¥çº¿è¡Œæƒ…æ•°æ®ã€‘ã€‚
        æ”¯æŒåˆ—ç­›é€‰ï¼Œä»…æŸ¥è¯¢ stock_daily_prices è¡¨ã€‚

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            columns: éœ€è¦çš„åˆ—ååˆ—è¡¨ (ä¾‹å¦‚ ['close', 'volume'])ã€‚å¦‚æœä¸ä¼ åˆ™æŸ¥æ‰€æœ‰ã€‚

        Returns:
            pd.DataFrame: ç´¢å¼•ä¸º date çš„æ•°æ®æ¡†
        """
        table_name = 'stock_daily_prices'

        # 1. æ„å»º SQL æŸ¥è¯¢å­—æ®µ
        if columns:
            # æ€»æ˜¯åŒ…å« dateï¼Œå› ä¸ºå®ƒæ˜¯ç´¢å¼•
            query_cols = ['date']
            for col in columns:
                # æŸ¥è¡¨ï¼šåªå¤„ç†å±äº stock_daily_prices çš„åˆ—
                mapping = self.COLUMN_MAPPING.get(col)
                if mapping and mapping[0] == table_name:
                    query_cols.append(mapping[1])  # æ·»åŠ åŸå§‹åˆ—å

                # ä¿åº•é€»è¾‘ï¼šå¦‚æœåˆ—åä¸åœ¨æ˜ å°„ä¸­ï¼Œä½†çœ‹èµ·æ¥åƒåŸºç¡€è¡Œæƒ…ï¼Œä¹Ÿå°è¯•æŸ¥è¯¢
                elif col in [
                        'open', 'high', 'low', 'close', 'volume', 'turnover'
                ]:
                    query_cols.append(col)

            # å»é‡å¹¶è½¬ä¸ºå­—ç¬¦ä¸²
            query_cols = list(set(query_cols))
            cols_str = ", ".join(query_cols)
        else:
            # å¦‚æœæœªæŒ‡å®šï¼Œé»˜è®¤æŸ¥æ‰€æœ‰
            cols_str = "*"

        # 2. æ„å»º SQL è¯­å¥
        # ä½¿ç”¨ BETWEEN ä¼˜åŒ–æ—¥æœŸèŒƒå›´æŸ¥è¯¢
        query = f"SELECT {cols_str} FROM {table_name} WHERE code = ? AND date BETWEEN ? AND ?"
        params = (symbol, self.start_date, self.end_date)

        try:
            # æ‰§è¡ŒæŸ¥è¯¢
            df = self.db_handler.query_data(query, params)
            if df is None or df.empty:
                return None

            # ç¡®ä¿ date æ˜¯ datetime ç±»å‹å¹¶è®¾ä¸ºç´¢å¼•
            if 'date' in df.columns:
                df['date'] = pd.to_datetime(df['date'])
                df.set_index('date', inplace=True)

            return df
        except Exception as e:
            logging.error(f"âŒ è·å– {symbol} æ•°æ®å¤±è´¥: {e}")
            return None

    # ==========================================================================
    # ã€ã€ã€æ ¸å¿ƒæ–¹æ³• 2ï¼šè·å–å…¨å¸‚åœºåˆå¹¶æ•°æ®ã€‘ã€‘ã€‘
    # ==========================================================================
    def get_all_data_for_universe(
            self,
            universe: list,
            required_columns: list = None) -> pd.DataFrame:
        """
        ä¸ºæ•´ä¸ªè‚¡ç¥¨æ± è·å–åˆå¹¶äº†æ‰€æœ‰æ‰€éœ€æ•°æ®(è¡Œæƒ…+è¡Œä¸š+åŸºæœ¬é¢)çš„å¤§å®½è¡¨ã€‚
        é‡‡ç”¨ Pandas-Native æ–¹å¼ï¼šåˆ†åˆ«è¯»å–ï¼Œå†…å­˜åˆå¹¶ã€‚
        """
        # --- [æ§åˆ¶å°è¾“å‡º] å‘ŠçŸ¥ç”¨æˆ·æ­£åœ¨åˆ†ææ•°æ®éœ€æ±‚ ---
        logging.info(f"âš™ï¸ [æ•°æ®åŠ è½½] æ­£åœ¨è§£æ {len(universe)} åªè‚¡ç¥¨çš„æ•°æ®éœ€æ±‚...")

        # 1. åˆ†æéœ€æ±‚ï¼šæˆ‘ä»¬éœ€è¦åŠ è½½å“ªäº›è¡¨çš„æ•°æ®ï¼Ÿ
        load_industry = False

        if required_columns:
            # æ£€æŸ¥æ˜¯å¦è¯·æ±‚äº† 'industry'
            if 'industry' in required_columns:
                load_industry = True

        # 2. é¢„åŠ è½½é™æ€æ•°æ® (ä¼˜åŒ–ï¼šé¿å…åœ¨å¾ªç¯ä¸­ N æ¬¡æŸ¥è¯¢æ•°æ®åº“)
        industry_map = {}
        if load_industry:
            # --- [æ§åˆ¶å°è¾“å‡º] å‘ŠçŸ¥ç”¨æˆ·æ­£åœ¨é¢„åŠ è½½è¡Œä¸šæ•°æ® ---
            logging.info("  > æ­£åœ¨é¢„åŠ è½½è¡Œä¸šæ•°æ® (stock_kind)...")

            # æŸ¥è¯¢ Stkcd (ä»£ç ) å’Œ Nnindnme (è¡Œä¸šå)
            ind_query = "SELECT Stkcd, Nnindnme FROM stock_kind"
            ind_df = self.db_handler.query_data(ind_query)

            if ind_df is not None and not ind_df.empty:
                # æ¸…æ´—ä»£ç æ ¼å¼ï¼Œç¡®ä¿ä¸ universe ä¸­çš„ symbol æ ¼å¼ä¸€è‡´ (å¦‚è¡¥é›¶)
                # å‡è®¾ universe ä¸­çš„ symbol æ˜¯ 6 ä½æ•°å­—ç¬¦ä¸²
                ind_df['Stkcd'] = ind_df['Stkcd'].astype(str).str.zfill(6)
                # è½¬ä¸ºå­—å…¸: {'000001': 'é“¶è¡Œ', ...}
                industry_map = ind_df.set_index('Stkcd')['Nnindnme'].to_dict()
                logging.info(f"  > âœ… æˆåŠŸåŠ è½½ {len(industry_map)} æ¡è¡Œä¸šè®°å½•ã€‚")
            else:
                logging.warning("  > âš ï¸ è­¦å‘Š: æœªèƒ½åŠ è½½åˆ°è¡Œä¸šæ•°æ®ï¼Œ'industry' åˆ—å°†ä¸ºç©ºã€‚")

        # 3. å¾ªç¯è·å–æ¯åªè‚¡ç¥¨çš„æ•°æ®å¹¶ç»„è£…
        all_dfs = []

        # --- [æ§åˆ¶å°è¾“å‡º] å¼€å§‹ä¸»å¾ªç¯ï¼Œæ˜¾ç¤ºè¿›åº¦æ¡ ---
        logging.info(f"ğŸš€ å¼€å§‹åŠ è½½å¹¶åˆå¹¶æ•°æ® (æŒ‰éœ€åŠ è½½åˆ—: {required_columns})...")

        # è¿‡æ»¤å‡ºåªå±äºè¡Œæƒ…è¡¨çš„åˆ—ï¼Œä¼ ç»™ get_dataframe
        # è¿™æ ·é¿å…æŠŠ 'industry' è¿™ç§åˆ—ä¼ ç»™ SQL æŠ¥é”™
        price_cols = []
        if required_columns:
            for c in required_columns:
                mapping = self.COLUMN_MAPPING.get(c)
                if mapping and mapping[0] == 'stock_daily_prices':
                    price_cols.append(c)
                elif c in ['open', 'high', 'low', 'close', 'volume']:  # åŸºç¡€åˆ—ä¿åº•
                    price_cols.append(c)

        for symbol in tqdm(universe, desc="[Data Load]"):
            # A. è·å–åŸºç¡€è¡Œæƒ… (å·²æŒ‰éœ€ç­›é€‰åˆ—)
            df = self.get_dataframe(symbol, columns=price_cols)

            if df is None or df.empty:
                continue

            # B. åˆå¹¶è¡Œä¸šæ•°æ® (Pandas Native: å­—å…¸æ˜ å°„)
            if load_industry:
                # ä½¿ç”¨ map æ¯” apply æ›´å¿«
                # get(symbol) è·å–è¯¥è‚¡ç¥¨çš„è¡Œä¸šï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¸º None
                ind = industry_map.get(symbol)
                df['industry'] = ind

            # C. (æœªæ¥) åˆå¹¶åŸºæœ¬é¢æ•°æ®
            # è¿™é‡Œå°†æ˜¯ pd.merge_asof çš„ä½ç½®ï¼Œç”¨äºå¯¹é½è´¢æŠ¥æ—¥æœŸ

            # æ·»åŠ  asset åˆ—ï¼Œç”¨äºæ„å»º MultiIndex
            df['asset'] = symbol
            all_dfs.append(df)

        if not all_dfs:
            logging.error("âŒ æœªèƒ½åŠ è½½ä»»ä½•æ•°æ®ã€‚")
            return pd.DataFrame()

        # 4. æœ€ç»ˆåˆå¹¶æ‰€æœ‰è‚¡ç¥¨çš„æ•°æ®
        # --- [æ§åˆ¶å°è¾“å‡º] å‘ŠçŸ¥ç”¨æˆ·æ­£åœ¨è¿›è¡Œæœ€ç»ˆåˆå¹¶ ---
        logging.info("âš™ï¸ æ­£åœ¨åˆå¹¶æ‰€æœ‰è‚¡ç¥¨çš„æ•°æ®æ¡†...")

        final_df = pd.concat(all_dfs)

        # ã€ã€ã€ä¿®å¤ç‚¹ã€‘ã€‘ã€‘: ä¹‹å‰è¿™é‡Œæœ‰ä¸¤è¡Œ set_indexï¼Œå¯¼è‡´äº† KeyError
        # ç°åœ¨çš„é€»è¾‘ï¼š
        # 1. concat åï¼Œç´¢å¼•æ˜¯ dateï¼Œåˆ—æœ‰ asset (å’Œå…¶ä»–æ•°æ®)
        # 2. reset_index() -> ç´¢å¼•å˜æˆ 0,1,2...ï¼Œdate å˜å›æ™®é€šåˆ—
        # 3. set_index(['date', 'asset']) -> å»ºç«‹å¤šé‡ç´¢å¼•

        final_df.reset_index(inplace=True)
        final_df.set_index(['date', 'asset'], inplace=True)

        # æ’åº (è¿™å¯¹ rolling è®¡ç®—è‡³å…³é‡è¦)
        final_df.sort_index(inplace=True)

        logging.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(final_df)} è¡Œã€‚")
        return final_df

    def calculate_universe_forward_returns(
            self, universe: list,
            forward_return_periods: list) -> pd.DataFrame:
        """
        ç»Ÿä¸€è®¡ç®—æœªæ¥æ”¶ç›Šç‡ã€‚
        ä¼˜åŒ–ï¼šåªåŠ è½½ close åˆ—ã€‚
        """
        logging.info(f"âš™ï¸ [æ”¶ç›Šç‡è®¡ç®—] æ­£åœ¨ä¸º {len(universe)} åªè‚¡ç¥¨è®¡ç®—æœªæ¥æ”¶ç›Š...")

        # ä»…åŠ è½½ close åˆ—ï¼Œæå¤§æå‡é€Ÿåº¦
        all_data = self.get_all_data_for_universe(universe,
                                                  required_columns=['close'])

        if all_data is None or all_data.empty:
            logging.error("âŒ æ— æ³•åŠ è½½æ•°æ®ç”¨äºè®¡ç®—æ”¶ç›Šç‡ã€‚")
            return None

        # ä½¿ç”¨ groupby().apply()
        # group_keys=False é˜²æ­¢ç´¢å¼•å±‚çº§å¢åŠ 
        returns_df = all_data.groupby(level='asset', group_keys=False).apply(
            lambda x: _calculate_forward_returns(x, forward_return_periods))

        # ç­›é€‰å‡ºéœ€è¦çš„åˆ— (date, asset, forward_return_*)
        # ç”±äº apply åç´¢å¼•æ˜¯ (date, asset)ï¼Œæˆ‘ä»¬éœ€è¦ reset_index æ¥è·å¾—è¿™ä¸¤åˆ—
        returns_df.reset_index(inplace=True)

        cols_to_keep = ['date', 'asset'] + [
            f'forward_return_{p}d' for p in forward_return_periods
        ]
        return returns_df[cols_to_keep]

    # ==========================================================================
    # è·å–è¡Œä¸šæ˜ å°„ (å…¼å®¹æ—§ä»£ç )
    # ==========================================================================
    def get_industry_mapping(self) -> pd.DataFrame | None:
        logging.info("  > âš™ï¸ æ­£åœ¨åŠ è½½è¡Œä¸šæ˜ å°„æ•°æ®...")
        query = "SELECT Stkcd, Nnindnme FROM stock_kind"
        try:
            df = self.db_handler.query_data(query)
            if df is not None:
                df['asset'] = df['Stkcd'].astype(str).str.zfill(6)
                df.rename(columns={'Nnindnme': 'industry'}, inplace=True)
                return df[['asset', 'industry']]
            return None
        except Exception as e:
            logging.error(f"âŒ åŠ è½½è¡Œä¸šæ•°æ®å¤±è´¥: {e}")
            return None

    # ==========================================================================
    # ä¸‹é¢æ˜¯æ•°æ®ä¸‹è½½/æ›´æ–°æµç¨‹ (ä¿æŒåŸæœ‰é€»è¾‘)
    # ==========================================================================
    def prepare_data_for_universe(self):
        if not self.provider_configs:
            logging.info("â„¹ï¸ æœªé…ç½®æ•°æ®æºï¼Œè·³è¿‡ä¸‹è½½ã€‚")
            return

        logging.info("--- ğŸ å¼€å§‹æ•°æ®å‡†å¤‡æµç¨‹ (ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼) ---")

        # 1. è·å–æ—¥å†
        logging.info("ğŸ—“ï¸ æ­£åœ¨è·å–äº¤æ˜“æ—¥å†...")
        try:
            all_trade_dates_str = self.calendar_provider.get_trading_days(
                self.start_date, self.end_date)
            self.all_trade_dates_set = set(
                pd.to_datetime(all_trade_dates_str).date)
        except Exception as e:
            logging.critical(f"âŒ è·å–äº¤æ˜“æ—¥å†å¤±è´¥: {e}")
            return

        # 2. å¡«å……æ£€æŸ¥é˜Ÿåˆ—
        for symbol in self.symbols:
            self.symbols_queue.put(symbol)

        # 3. å¯åŠ¨æ£€æŸ¥çº¿ç¨‹
        logging.info(f"ğŸ” å¯åŠ¨ {self.num_checker_threads} ä¸ªæ£€æŸ¥çº¿ç¨‹...")
        with tqdm(total=len(self.symbols),
                  desc="[æ•°æ®æ£€æŸ¥]",
                  ncols=100,
                  file=sys.stdout) as pbar:
            self.check_progress_bar = pbar
            threads = []
            for i in range(self.num_checker_threads):
                t = threading.Thread(target=self._checker_worker,
                                     name=f"Checker-{i}",
                                     daemon=True)
                t.start()
                threads.append(t)
            for t in threads:
                t.join()

        self.check_progress_bar = None

        # 4. å¯åŠ¨ä¸‹è½½å’Œå†™å…¥
        total_downloads = self.download_tasks_queue.qsize()
        if total_downloads > 0:
            logging.info(f"ğŸ“¥ å…± {total_downloads} åªè‚¡ç¥¨éœ€è¦ä¸‹è½½/æ›´æ–°ã€‚")
            logging.info(
                f"ğŸš€ å¯åŠ¨ {self.num_downloader_threads} ä¸ªä¸‹è½½çº¿ç¨‹ å’Œ 1 ä¸ªå†™å…¥çº¿ç¨‹...")

            writer_thread = threading.Thread(target=self._consumer_worker,
                                             name="DB-Writer",
                                             daemon=True)
            writer_thread.start()

            with tqdm(total=total_downloads,
                      desc="[æ•°æ®ä¸‹è½½]",
                      ncols=100,
                      file=sys.stdout) as pbar:
                self.download_progress_bar = pbar
                dl_threads = []
                for i in range(self.num_downloader_threads):
                    t = threading.Thread(target=self._producer_worker,
                                         name=f"Downloader-{i}",
                                         daemon=True)
                    t.start()
                    dl_threads.append(t)
                for t in dl_threads:
                    t.join()

            self.producers_finished_event.set()
            writer_thread.join()
            logging.info("--- âœ… æ•°æ®å‡†å¤‡æµç¨‹ç»“æŸ ---")
        else:
            logging.info("âœ… æ‰€æœ‰æ•°æ®å·²å®Œæ•´ï¼Œæ— éœ€ä¸‹è½½ã€‚")

    def _checker_worker(self):
        while True:
            try:
                symbol = self.symbols_queue.get_nowait()
            except queue.Empty:
                break

            missing = self._find_missing_date_ranges(symbol)
            if missing:
                self.download_tasks_queue.put((symbol, missing))

            self.symbols_queue.task_done()
            if self.check_progress_bar: self.check_progress_bar.update(1)

    def _find_missing_date_ranges(self, symbol):
        # ç®€åŒ–çš„æ£€æŸ¥é€»è¾‘ï¼šæŸ¥è¯¢å·²æœ‰æ•°æ®çš„æ—¥æœŸé›†åˆï¼Œä¸æ—¥å†å¯¹æ¯”
        query = f"SELECT date FROM {self.table_name} WHERE code = ?"
        df = self.db_handler.query_data(query, (symbol, ))
        if df is None or df.empty:
            return [(pd.to_datetime(self.start_date).date(),
                     pd.to_datetime(self.end_date).date())]

        existing_dates = set(pd.to_datetime(df['date']).dt.date)
        missing_dates = sorted(list(self.all_trade_dates_set - existing_dates))

        if not missing_dates: return []

        # å°†ç¦»æ•£æ—¥æœŸåˆå¹¶ä¸ºåŒºé—´ (ç®€åŒ–å¤„ç†ï¼Œè¿™é‡Œç›´æ¥è¿”å›èµ·æ­¢æ—¶é—´ï¼Œå®é™…ä¸‹è½½ä¼šè¦†ç›–ä¸­é—´å·²æœ‰çš„)
        return [(missing_dates[0], missing_dates[-1])]

    def _producer_worker(self):
        while True:
            try:
                task = self.download_tasks_queue.get_nowait()
            except queue.Empty:
                break

            symbol, ranges = task
            # ç®€å•èµ·è§ï¼Œå–ç¬¬ä¸€ä¸ªåŒºé—´çš„èµ·æ­¢
            start, end = ranges[0]

            # ä¼˜å…ˆå°è¯• SQLite (å¦‚æœæ˜¯æœ¬åœ°æº)ï¼Œå¦åˆ™å°è¯• Tushare/Akshare
            # è¿™é‡Œç®€åŒ–é€»è¾‘ï¼Œç›´æ¥éå† providers
            df = None
            for name, provider in self._local.providers.items():
                try:
                    df = provider.get_daily_price(symbol,
                                                  start.strftime('%Y%m%d'),
                                                  end.strftime('%Y%m%d'))
                    if df is not None and not df.empty:
                        break
                except:
                    continue

            if df is not None and not df.empty:
                self.results_queue.put(df)

            self.download_tasks_queue.task_done()
            if self.download_progress_bar: self.download_progress_bar.update(1)

    def _consumer_worker(self):
        batch = []
        while not (self.producers_finished_event.is_set()
                   and self.results_queue.empty()):
            try:
                df = self.results_queue.get(timeout=1)
                batch.append(df)
                if len(batch) >= self.batch_size:
                    self._save_batch(batch)
                    batch = []
            except queue.Empty:
                continue
        if batch: self._save_batch(batch)

    def _save_batch(self, batch):
        if not batch: return
        try:
            full_df = pd.concat(batch)
            # ç¡®ä¿ code åˆ—å­˜åœ¨ (provider è¿”å›çš„æ•°æ®åº”è¯¥åŒ…å« code)
            self.db_handler.save_data(full_df, self.table_name)
        except Exception as e:
            logging.error(f"âŒ æ‰¹é‡å†™å…¥å¤±è´¥: {e}")

    def __del__(self):
        if hasattr(self, 'db_handler'):
            self.db_handler.close_connection()
