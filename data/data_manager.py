# data_manager.py (å·²ä¼˜åŒ– get_all_data_for_universe)
import pandas as pd
import backtrader as bt
from datetime import timedelta
import queue
import threading
from tqdm import tqdm
import logging
import sqlite3
import os
import sys
import numpy as np  # <- ã€ã€ã€æ–°å¢ã€‘ã€‘ã€‘ (ç”¨äºåˆ†å—)

from .database_handler import DatabaseHandler
from .trading_calendars import TushareTradingCalendar, AkshareTradingCalendar
from .data_providers import AkshareDataProvider, TushareDataProvider, SQLiteDataProvider


class DataProviderManager:
    """
    ã€ç”Ÿäº§è€…-æ¶ˆè´¹è€…é‡æ„ç‰ˆã€‘ç»Ÿä¸€æ•°æ®æä¾›è€…ç®¡ç†å™¨ã€‚
    
    ã€ã€é‡æ„æ—¥å¿—ã€‘ã€‘:
    - 2025-11-10 (æ€§èƒ½ä¼˜åŒ–):
      - ä¼˜åŒ– 'get_all_data_for_universe'ï¼š
        - ç§»é™¤ N æ¬¡æŸ¥è¯¢çš„å¾ªç¯ã€‚
        - æ›¿æ¢ä¸ºã€åˆ†å—æŸ¥è¯¢ã€‘ï¼Œä»¥é¿å… SQLite "too many SQL variables"
          (é™åˆ¶~999) çš„é”™è¯¯ã€‚
    """

    def __init__(self,
                 provider_configs,
                 symbols,
                 start_date,
                 end_date,
                 db_path='quant_data.db',
                 num_checker_threads=4,
                 num_downloader_threads=8,
                 batch_size=100,
                 auto_detect_universe: bool = True):

        self.start_date = pd.to_datetime(start_date).strftime('%Y-%m-%d')
        self.end_date = pd.to_datetime(end_date).strftime('%Y-%m-%d')
        self.provider_configs = provider_configs
        self.db_handler = DatabaseHandler(db_path)
        self.table_name = 'stock_daily_prices'

        # (å·²ä¿®æ­£ï¼šä»…åœ¨ auto_detect_universe=True æ—¶æ‰åŠ è½½å…¨å¸‚åœº)
        if not symbols and auto_detect_universe:
            logging.info("â„¹ï¸ 'symbols' åˆ—è¡¨ä¸ºç©ºã€‚å°†ä»æºæ•°æ®åº“è‡ªåŠ¨æ£€æµ‹å…¨å¸‚åœºè‚¡ç¥¨æ± ...")
            try:
                if not provider_configs:
                    raise ValueError("provider_configs ä¸ºç©º, æ— æ³•è‡ªåŠ¨æ£€æµ‹è‚¡ç¥¨æ± ã€‚")

                source_provider_config = self.provider_configs[0][1]
                source_db_path = source_provider_config.get('db_path')
                source_table_name = source_provider_config.get('table_name')

                if not source_db_path or not source_table_name:
                    raise ValueError(
                        "åœ¨ provider_configs[0] ä¸­æœªæ‰¾åˆ° 'db_path' æˆ– 'table_name'")

                logging.info(f"  > æ­£åœ¨è¿æ¥æºæ•°æ®åº“: {source_db_path}")

                conn = sqlite3.connect(source_db_path)
                query = f"SELECT DISTINCT ticker FROM {source_table_name}"
                all_tickers_df = pd.read_sql(query, conn)
                conn.close()

                self.symbols = [
                    str(ticker).zfill(6) for ticker in all_tickers_df['ticker']
                ]

                if not self.symbols:
                    raise Exception("æœªèƒ½ä»æ•°æ®åº“åŠ è½½è‚¡ç¥¨åˆ—è¡¨ (æŸ¥è¯¢ç»“æœä¸ºç©º)ã€‚")

                logging.info(f"  > âœ… æˆåŠŸåŠ è½½ {len(self.symbols)} åªè‚¡ç¥¨ä½œä¸ºå…¨å¸‚åœºè‚¡ç¥¨æ± ã€‚")

            except Exception as e:
                logging.error(f"  > âŒ åŠ¨æ€è·å–è‚¡ç¥¨æ± å¤±è´¥: {e}", exc_info=True)
                self.symbols = []

        elif not symbols and not auto_detect_universe:
            logging.debug("  > â„¹ï¸ DataProviderManager (Worker) å·²åˆå§‹åŒ– (æ— è‚¡ç¥¨æ± )ã€‚")
            self.symbols = []

        else:
            logging.info(f"  > â„¹ï¸ æ­£åœ¨ä½¿ç”¨ä¼ å…¥çš„ {len(symbols)} åªè‚¡ç¥¨çš„é™æ€è‚¡ç¥¨æ± ã€‚")
            self.symbols = symbols if isinstance(symbols, list) else [symbols]

        # (çœç•¥... çº¿ç¨‹/é˜Ÿåˆ—/æ—¥å† åˆå§‹åŒ– ...)
        self.num_checker_threads = num_checker_threads
        self.num_downloader_threads = num_downloader_threads
        self.batch_size = batch_size
        self.symbols_queue = queue.Queue()
        self.download_tasks_queue = queue.Queue()
        self.results_queue = queue.Queue()
        self.producers_finished_event = threading.Event()
        self.check_progress_bar = None
        self.download_progress_bar = None
        if self.provider_configs:
            if self.provider_configs[0][0].__name__ == 'TushareDataProvider':
                self.calendar_provider = TushareTradingCalendar(
                    token=self.provider_configs[0][1].get('token'))
            else:
                self.calendar_provider = AkshareTradingCalendar()
        else:
            self.calendar_provider = AkshareTradingCalendar()

    # (çœç•¥... _find_missing_date_ranges, _fetch_data_from_providers ...)
    # (çœç•¥... _producer_worker, _consumer_worker, _save_batch_to_db, _checker_worker ...)
    # (çœç•¥... prepare_data_for_universe ...)
    # (çœç•¥... get_bt_feed, get_dataframe, validate_data_quality, get_industry_mapping ...)
    #
    # (ä¸ºä¿æŒæ¸…æ™°ï¼Œä»…ç²˜è´´è¢«ä¿®æ”¹å’Œå¿…é¡»çš„å‡½æ•°)
    #

    def get_dataframe(self, symbol: str) -> pd.DataFrame | None:
        """ä»æ•°æ®åº“è·å–å¹¶è¿”å›å•ä¸ªæ ‡çš„çš„DataFrameã€‚"""
        query = f"SELECT * FROM {self.table_name} WHERE code = ? AND date BETWEEN ? AND ?"
        params = (symbol, self.start_date, self.end_date)
        df = self.db_handler.query_data(query, params)
        if df is not None and not df.empty:
            df.sort_index(ascending=True, inplace=True)
        return df

    # ==============================================================================
    # ã€ã€ã€ã€ã€ã€ æ ¸å¿ƒä¿®æ”¹ï¼šget_all_data_for_universe ã€‘ã€‘ã€‘ã€‘ã€‘ã€‘
    # ==============================================================================

    def get_all_data_for_universe(self, universe: list) -> pd.DataFrame | None:
        """
        è·å–è‚¡ç¥¨æ± ä¸­æ‰€æœ‰è‚¡ç¥¨çš„æ‰€æœ‰æ—¥çº¿æ•°æ®ï¼Œå¹¶åˆå¹¶ä¸ºä¸€ä¸ªå¤§çš„ MultiIndex DataFrameã€‚
        
        ã€ã€é‡æ„æ—¥å¿—ã€‘ã€‘:
        - 2025-11-10 (æ€§èƒ½ä¼˜åŒ– - æ–¹æ¡ˆC):
          - ç§»é™¤äº† N æ¬¡æŸ¥è¯¢çš„å¾ªç¯ã€‚
          - æ›¿æ¢ä¸ºã€åˆ†å—æŸ¥è¯¢ã€‘ã€‚ä¸ºé¿å… SQLite "too many SQL variables" é”™è¯¯
            (é™åˆ¶~999), æˆ‘ä»¬å°† 5000+ çš„è‚¡ç¥¨æ±  åˆ†å— (e.g., 900/å—) 
            å¹¶æ‰§è¡Œ N/900 æ¬¡æŸ¥è¯¢ã€‚
        """
        if not universe:
            logging.warning(
                "  > âš ï¸ [get_all_data_for_universe] ä¼ å…¥çš„ universe åˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•åŠ è½½æ•°æ®ã€‚")
            return None

        logging.info(
            f"--- âš™ï¸ æ­£åœ¨ä¸º {len(universe)} åªè‚¡ç¥¨åŠ è½½ã€å…¨éƒ¨ã€‘æ—¥çº¿æ•°æ® (æ‰§è¡Œåˆ†å—SQLæŸ¥è¯¢)... ---")

        all_stock_dfs = []  # ç”¨äºæ”¶é›†æ‰€æœ‰åˆ†å—çš„ DataFrame

        # (SQLite å˜é‡ä¸Šé™é€šå¸¸æ˜¯ 999ï¼Œæˆ‘ä»¬ä½¿ç”¨ 900 ä½œä¸ºå®‰å…¨å€¼)
        SQLITE_VAR_LIMIT = 900

        num_chunks = int(np.ceil(len(universe) / SQLITE_VAR_LIMIT))

        # ã€ã€ã€æ–°å¢ã€‘ã€‘ã€‘: ä½¿ç”¨ TQDM åŒ…è£¹åˆ†å—å¾ªç¯
        tqdm_loop = tqdm(
            range(num_chunks),
            desc="[æ•°æ®åŠ è½½] åˆ†å—åŠ è½½è‚¡ç¥¨æ•°æ®",
            ncols=100,
            file=sys.stdout  # (ä¿æŒä¸ logger_config.py ä¸€è‡´)
        )

        try:
            for i in tqdm_loop:
                # 1. è·å–å½“å‰åˆ†å—çš„è‚¡ç¥¨
                start_idx = i * SQLITE_VAR_LIMIT
                end_idx = (i + 1) * SQLITE_VAR_LIMIT
                chunk_universe = universe[start_idx:end_idx]

                if not chunk_universe:
                    continue

                tqdm_loop.set_description(
                    f"[æ•°æ®åŠ è½½] åˆ†å— {i+1}/{num_chunks} (å« {len(chunk_universe)} åªè‚¡ç¥¨)"
                )

                # 2. å‡†å¤‡ SQL æŸ¥è¯¢
                placeholders = ', '.join('?' for _ in chunk_universe)
                query = f"""
                    SELECT * FROM {self.table_name} 
                    WHERE code IN ({placeholders}) 
                    AND date BETWEEN ? AND ?
                """

                # 3. å‡†å¤‡å‚æ•°
                params = tuple(chunk_universe) + (self.start_date,
                                                  self.end_date)

                # 4. æ‰§è¡Œã€åˆ†å—ã€‘æŸ¥è¯¢
                # (db_handler.query_data è¿”å›ä»¥ 'date' ä¸ºç´¢å¼•çš„ DF)
                chunk_df = self.db_handler.query_data(query, params=params)

                if chunk_df is not None and not chunk_df.empty:
                    all_stock_dfs.append(chunk_df)

            if not all_stock_dfs:
                logging.error(f"  > âŒ é”™è¯¯: æœªèƒ½ä¸ºè‚¡ç¥¨æ± åŠ è½½ä»»ä½•æ—¥çº¿æ•°æ® (æ‰€æœ‰åˆ†å—æŸ¥è¯¢å‡ä¸ºç©º)ã€‚")
                return None

            # 5. åˆå¹¶æ‰€æœ‰åˆ†å—
            logging.info("  > âš™ï¸ æ­£åœ¨åˆå¹¶æ‰€æœ‰æ•°æ®åˆ†å—...")
            full_df = pd.concat(all_stock_dfs)

            # 6. è½¬æ¢ä¸º MultiIndex (date, asset)
            full_df.rename(columns={'code': 'asset'}, inplace=True)
            full_df.reset_index(inplace=True)  # é‡Šæ”¾ 'date'

            full_df['date'] = pd.to_datetime(full_df['date'])
            full_df.set_index(['date', 'asset'], inplace=True)
            full_df.sort_index(inplace=True)

            logging.info(f"  > âœ… æˆåŠŸåŠ è½½å¹¶åˆå¹¶ {len(full_df)} è¡Œæ€»æ•°æ®ã€‚")
            return full_df

        except Exception as e:
            logging.error(f"  > âŒ [get_all_data_for_universe] æ‰§è¡Œåˆ†å—æŸ¥è¯¢æ—¶å‡ºé”™: {e}",
                          exc_info=True)
            return None

    # ==============================================================================
    # ã€ã€ã€ã€ã€ã€ ä¿®æ”¹ç»“æŸ ã€‘ã€‘ã€‘ã€‘ã€‘ã€‘
    # ==============================================================================

    def __del__(self):
        """åœ¨å¯¹è±¡é”€æ¯æ—¶ï¼Œç¡®ä¿å…³é—­æ•°æ®åº“è¿æ¥ã€‚"""
        self.db_handler.close_connection()

    #
    # (ä¸ºäº†è®©è¿™ä¸ªæ–‡ä»¶å¯ä»¥è¢«å®Œæ•´æ›¿æ¢ï¼Œæˆ‘æŠŠå…¶ä»–å‡½æ•°ä¹Ÿç²˜è´´åœ¨ä¸‹é¢)
    #

    def _find_missing_date_ranges(self, symbol: str) -> list[tuple[str, str]]:
        logging.debug(
            f"  > [æ£€æŸ¥çº¿ç¨‹ {threading.get_ident()}] æ­£åœ¨ä¸º {symbol} æ£€æŸ¥æ•°æ®å®Œæ•´æ€§")
        all_trade_dates = self.all_trade_dates_set
        query = f"SELECT DISTINCT DATE(date) FROM {self.table_name} WHERE code = ? AND DATE(date) BETWEEN ? AND ?"
        existing_dates_df = self.db_handler.query_data(query,
                                                       params=(symbol,
                                                               self.start_date,
                                                               self.end_date))
        if existing_dates_df is not None and not existing_dates_df.empty:
            date_col = existing_dates_df.columns[0]
            existing_dates = set(
                pd.to_datetime(existing_dates_df[date_col]).dt.date)
        else:
            existing_dates = set()
        missing_dates = sorted(list(all_trade_dates - existing_dates))

        if not missing_dates:
            logging.info(f"  > âœ… [{symbol}] æ•°æ®å®Œæ•´ï¼Œæ— éœ€ä¸‹è½½ã€‚")
            return []

        logging.info(
            f"  > ğŸ“¥ [{symbol}] å‘ç° {len(missing_dates)} ä¸ªç¼ºå¤±çš„äº¤æ˜“æ—¥ï¼Œæ­£åœ¨åˆå¹¶ä¸ºä¸‹è½½åŒºé—´...")
        ranges = []
        if not missing_dates:
            return ranges
        start_range = missing_dates[0]
        for i in range(1, len(missing_dates)):
            if (missing_dates[i] - missing_dates[i - 1]).days > 7:
                end_range = missing_dates[i - 1]
                ranges.append((start_range.strftime('%Y-%m-%d'),
                               end_range.strftime('%Y-%m-%d')))
                start_range = missing_dates[i]
        ranges.append((start_range.strftime('%Y-%m-%d'),
                       missing_dates[-1].strftime('%Y-%m-%d')))
        for start, end in ranges:
            logging.debug(f"    -> [{symbol}] éœ€è¦ä¸‹è½½åŒºé—´: {start} to {end}")
        return ranges

    def _fetch_data_from_providers(self, symbol: str, start_date: str,
                                   end_date: str) -> pd.DataFrame | None:
        for provider_class, params in self.provider_configs:
            provider_instance = provider_class(**params)
            fetched_df = provider_instance.fetch_data(symbol, start_date,
                                                      end_date)
            if fetched_df is not None and not fetched_df.empty:
                logging.info(
                    f"  > âœ… [ä¸‹è½½è€… {threading.get_ident()}] ä» {provider_instance.__class__.__name__} æˆåŠŸè·å– {symbol} çš„ {len(fetched_df)} æ¡æ•°æ®ã€‚"
                )
                fetched_df['code'] = symbol
                return fetched_df.reset_index()
        logging.warning(
            f"  > âš ï¸ [ä¸‹è½½è€… {threading.get_ident()}] è­¦å‘Šï¼šå°è¯•æ‰€æœ‰æ•°æ®æºåï¼Œä»æœªèƒ½è·å–åˆ° {symbol} ({start_date} to {end_date}) çš„æ•°æ®ã€‚"
        )
        return None

    def _producer_worker(self):
        while True:
            try:
                symbol, missing_ranges = self.download_tasks_queue.get(
                    block=False)
            except queue.Empty:
                logging.debug(f"  > [ä¸‹è½½è€… {threading.get_ident()}] ä»»åŠ¡é˜Ÿåˆ—å·²ç©ºï¼Œé€€å‡ºã€‚")
                break
            for start_date, end_date in missing_ranges:
                result_df = self._fetch_data_from_providers(
                    symbol, start_date, end_date)
                if result_df is not None:
                    self.results_queue.put(result_df)
            self.download_tasks_queue.task_done()
            if self.download_progress_bar:
                self.download_progress_bar.update(1)

    def _consumer_worker(self):
        batch = []
        while not (self.producers_finished_event.is_set()
                   and self.results_queue.empty()):
            try:
                result_df = self.results_queue.get(timeout=1)
                batch.append(result_df)
                if len(batch) >= self.batch_size:
                    self._save_batch_to_db(batch)
                    batch = []
            except queue.Empty:
                continue
        if batch:
            self._save_batch_to_db(batch)
        logging.info("--- [å†™å…¥è€…] æ‰€æœ‰æ•°æ®å·²å¤„ç†å®Œæ¯•ï¼Œå†™å…¥çº¿ç¨‹é€€å‡ºã€‚ ---")

    def _save_batch_to_db(self, batch: list):
        if not batch:
            return
        try:
            full_df = pd.concat(batch, ignore_index=True)
            logging.info(
                f"--- [å†™å…¥è€…] æ­£åœ¨åˆå¹¶ {len(batch)} ä¸ªDataFrame ({len(full_df)} è¡Œ)ï¼Œå¹¶å­˜å…¥æ•°æ®åº“... ---"
            )
            self.db_handler.save_data(full_df, self.table_name)
        except Exception as e:
            logging.error(f"--- âŒ [å†™å…¥è€…] æ‰¹é‡æ•°æ®ä¿å­˜è‡³æ•°æ®åº“æ—¶å‡ºé”™: {e} ---", exc_info=True)

    def _checker_worker(self):
        while True:
            try:
                symbol = self.symbols_queue.get(block=False)
            except queue.Empty:
                logging.debug(f"  > [æ£€æŸ¥çº¿ç¨‹ {threading.get_ident()}] é˜Ÿåˆ—ä¸ºç©ºï¼Œé€€å‡ºã€‚")
                break
            missing_ranges = self._find_missing_date_ranges(symbol)
            if missing_ranges:
                self.download_tasks_queue.put((symbol, missing_ranges))
            self.symbols_queue.task_done()
            if self.check_progress_bar:
                self.check_progress_bar.update(1)

    def prepare_data_for_universe(self):
        if not self.provider_configs:
            logging.info("â„¹ï¸  æœªé…ç½®æ•°æ®æºï¼Œè·³è¿‡æ•°æ®ä¸‹è½½ï¼Œä»…ä½¿ç”¨æœ¬åœ°æ•°æ®åº“æ•°æ®ã€‚")
            return
        logging.info("--- ğŸ å¼€å§‹æ•°æ®å‡†å¤‡æµç¨‹ (ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼) ---")
        logging.info("ğŸ—“ï¸  æ­£åœ¨è·å–äº¤æ˜“æ—¥å†...")
        try:
            all_trade_dates_str = self.calendar_provider.get_trading_days(
                self.start_date, self.end_date)
            if not all_trade_dates_str:
                logging.critical(f"  âŒ è‡´å‘½é”™è¯¯ï¼šæ— æ³•è·å–äº¤æ˜“æ—¥å†ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
                return
            self.all_trade_dates_set = set(
                pd.to_datetime(all_trade_dates_str).date)
            logging.info(f"  âœ… æˆåŠŸè·å– {len(self.all_trade_dates_set)} ä¸ªäº¤æ˜“æ—¥ã€‚")
        except Exception as e:
            logging.critical(f"  âŒ è‡´å‘½é”™è¯¯ï¼šè·å–äº¤æ˜“æ—¥å†æ—¶å‡ºé”™: {e}ï¼Œç¨‹åºç»ˆæ­¢ã€‚", exc_info=True)
            return
        for symbol in self.symbols:
            self.symbols_queue.put(symbol)
        logging.info(
            f"ğŸ” æ­£åœ¨å¯åŠ¨ {self.num_checker_threads} ä¸ªæ£€æŸ¥çº¿ç¨‹ï¼Œæ£€æŸ¥ {len(self.symbols)} åªè‚¡ç¥¨..."
        )
        with tqdm(total=len(self.symbols),
                  desc="[æ•°æ®æ£€æŸ¥] æ£€æŸ¥è¿›åº¦",
                  ncols=100,
                  file=sys.stdout) as pbar:
            self.check_progress_bar = pbar
            checker_threads = []
            for i in range(self.num_checker_threads):
                thread = threading.Thread(target=self._checker_worker,
                                          name=f"Checker-{i}")
                thread.daemon = True
                thread.start()
                checker_threads.append(thread)
            for thread in checker_threads:
                thread.join()
        self.check_progress_bar = None
        total_downloads = self.download_tasks_queue.qsize()
        if total_downloads > 0:
            logging.info(f"ğŸ“¥ æ£€æŸ¥å®Œæ¯•ã€‚å…± {total_downloads} åªè‚¡ç¥¨éœ€è¦ä¸‹è½½æ•°æ®ã€‚")
            logging.info(
                f"ğŸš€ å³å°†å¯åŠ¨ {self.num_downloader_threads} ä¸ªä¸‹è½½çº¿ç¨‹(ç”Ÿäº§è€…) å’Œ 1 ä¸ªå†™å…¥çº¿ç¨‹(æ¶ˆè´¹è€…)..."
            )
        else:
            logging.info("âœ… æ£€æŸ¥å®Œæ¯•ã€‚æ‰€æœ‰æ•°æ®å‡å·²å®Œæ•´ï¼Œæ— éœ€ä¸‹è½½ã€‚")
            return
        consumer_thread = threading.Thread(target=self._consumer_worker,
                                           name="DB-Writer")
        consumer_thread.daemon = True
        consumer_thread.start()
        with tqdm(total=total_downloads,
                  desc="[æ•°æ®ä¸‹è½½] ä¸‹è½½è¿›åº¦",
                  ncols=100,
                  file=sys.stdout) as pbar:
            self.download_progress_bar = pbar
            producer_threads = []
            for i in range(self.num_downloader_threads):
                thread = threading.Thread(target=self._producer_worker,
                                          name=f"Downloader-{i}")
                thread.daemon = True
                thread.start()
                producer_threads.append(thread)
            for thread in producer_threads:
                thread.join()
        self.producers_finished_event.set()
        consumer_thread.join()
        logging.info("--- âœ… æ‰€æœ‰æ•°æ®å‡†å¤‡æµç¨‹æ‰§è¡Œå®Œæ¯• ---")

    def get_bt_feed(self, symbol: str) -> bt.feeds.PandasData | None:
        df = self.get_dataframe(symbol)
        if df is not None and not df.empty:
            return bt.feeds.PandasData(dataname=df)
        logging.error(f"âŒ æœªèƒ½ä¸º {symbol} è·å–æœ‰æ•ˆæ•°æ®ï¼Œæ— æ³•åˆ›å»ºBacktrader feedã€‚")
        return None

    def validate_data_quality(self, symbol: str) -> bool:
        logging.info(f"--- æ­£åœ¨æ ¡éªŒ '{symbol}' çš„æ•°æ®è´¨é‡...")
        if not self.all_trade_dates_set:
            logging.error(f"ğŸ”´ äº¤æ˜“æ—¥å†æ•°æ®æœªåŠ è½½ï¼Œæ— æ³•å¯¹ '{symbol}' è¿›è¡Œå®Œæ•´æ€§æ ¡éªŒã€‚")
            return False
        df = self.get_dataframe(symbol)
        if df is None or df.empty:
            logging.warning(
                f"ğŸŸ¡ '{symbol}' åœ¨ {self.start_date} åˆ° {self.end_date} æœŸé—´æ— æ•°æ®ï¼Œå·²å‰”é™¤ã€‚"
            )
            return False
        existing_dates = set(pd.to_datetime(df.index).date)
        missing_dates = self.all_trade_dates_set - existing_dates
        if missing_dates:
            example_missing = sorted(list(missing_dates))[:3]
            logging.warning(
                f"ğŸ”´ '{symbol}' ç¼ºå¤± {len(missing_dates)} ä¸ªäº¤æ˜“æ—¥çš„æ•°æ® (ä¾‹å¦‚: {example_missing})ï¼Œå·²å‰”é™¤ã€‚"
            )
            return False
        cols_to_check = ['open', 'high', 'low', 'close', 'volume']
        if df[cols_to_check].isnull().values.any():
            problematic_rows = df[df[cols_to_check].isnull().any(axis=1)]
            logging.warning(
                f"ğŸ”´ '{symbol}' çš„æ•°æ®ä¸­åŒ…å«ç©ºå€¼ (NaN)ï¼Œå·²å‰”å‡ºã€‚é—®é¢˜æ•°æ®å¿«ç…§:\n{problematic_rows.head(3)}"
            )
            return False
        if (df[cols_to_check] <= 0).any().any():
            problematic_rows = df[(df[cols_to_check] <= 0).any(axis=1)]
            logging.warning(
                f"ğŸ”´ '{symbol}' çš„æ•°æ®ä¸­åŒ…å«0æˆ–è´Ÿå€¼ï¼Œå·²å‰”å‡ºã€‚é—®é¢˜æ•°æ®å¿«ç…§:\n{problematic_rows.head(3)}"
            )
            return False
        logging.info(f"âœ… '{symbol}' æ•°æ®è´¨é‡æ ¡éªŒé€šè¿‡ã€‚")
        return True

    def get_industry_mapping(self) -> pd.DataFrame | None:
        logging.info("  > âš™ï¸ æ­£åœ¨ä» 'stock_kind' è¡¨åŠ è½½è¡Œä¸šæ˜ å°„æ•°æ®...")
        try:
            query = "SELECT Stkcd, Nnindnme FROM stock_kind"
            df = self.db_handler.query_data(query)
            if df is None or df.empty:
                logging.warning("  > âš ï¸ è­¦å‘Š: æœªèƒ½ä» 'stock_kind' è¡¨ä¸­åŠ è½½åˆ°æ•°æ®ã€‚")
                return None
            df['asset'] = df['Stkcd'].astype(str).str.zfill(6)
            df.rename(columns={'Nnindnme': 'industry'}, inplace=True)
            logging.info(f"  > âœ… æˆåŠŸåŠ è½½ {len(df)} æ¡è¡Œä¸šæ˜ å°„è®°å½•ã€‚")
            return df[['asset', 'industry']]
        except Exception as e:
            logging.error(f"  > âŒ åŠ è½½ 'stock_kind' æ—¶å‡ºé”™: {e}", exc_info=True)
            return None
