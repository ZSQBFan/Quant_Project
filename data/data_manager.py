# data_manager.py
import pandas as pd
import backtrader as bt
from datetime import timedelta
import queue
import threading
from tqdm import tqdm
import logging

# ä¿æŒåŸæœ‰å¯¼å…¥
from .database_handler import DatabaseHandler
from .trading_calendars import TushareTradingCalendar, AkshareTradingCalendar
from .data_providers import AkshareDataProvider, TushareDataProvider, SQLiteDataProvider


class DataProviderManager:
    """
    ã€ç”Ÿäº§è€…-æ¶ˆè´¹è€…é‡æ„ç‰ˆã€‘ç»Ÿä¸€æ•°æ®æä¾›è€…ç®¡ç†å™¨ã€‚

    æœ¬ç±»é‡‡ç”¨ç”Ÿäº§è€…-æ¶ˆè´¹è€…è®¾è®¡æ¨¡å¼ï¼Œé«˜æ•ˆåœ°å¤„ç†æ•°æ®ä¸‹è½½ä¸å­˜å‚¨ä»»åŠ¡ï¼š
    - **æ£€æŸ¥è€…(Checker)çº¿ç¨‹**: å¿«é€Ÿæ£€æŸ¥æœ¬åœ°æ•°æ®åº“ï¼Œç¡®å®šéœ€è¦ä¸‹è½½çš„æ•°æ®èŒƒå›´ã€‚
    - **ä¸‹è½½è€…(Producer)çº¿ç¨‹**: å¹¶è¡Œåœ°ä»é…ç½®çš„æ•°æ®æºè·å–æ•°æ®ï¼Œä½†ä¸ç›´æ¥å†™å…¥æ•°æ®åº“ï¼Œ
      è€Œæ˜¯å°†è·å–åˆ°çš„æ•°æ®æ”¾å…¥ä¸€ä¸ªä¸­å¤®é˜Ÿåˆ—ã€‚
    - **å†™å…¥è€…(Consumer)çº¿ç¨‹**: å•ç‹¬ä¸€ä¸ªçº¿ç¨‹ï¼Œä»ä¸­å¤®é˜Ÿåˆ—ä¸­å–å‡ºæ•°æ®ï¼Œåˆå¹¶æˆå¤§æ‰¹é‡åï¼Œ
      ä¸€æ¬¡æ€§å†™å…¥æ•°æ®åº“ã€‚

    è¯¥è®¾è®¡æ—¨åœ¨è§£å†³SQLiteçš„å¹¶å‘å†™å…¥ç“¶é¢ˆï¼Œé€šè¿‡æ‰¹é‡å†™å…¥å¤§å¹…æå‡æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—é™ä½CPUè´Ÿè½½ã€‚
    """

    def __init__(
            self,
            provider_configs,
            symbols,
            start_date,
            end_date,
            db_path='quant_data.db',
            num_checker_threads=4,
            num_downloader_threads=8,  # ä¸‹è½½çº¿ç¨‹ç°åœ¨ä½œä¸ºâ€œç”Ÿäº§è€…â€
            batch_size=100):
        """
        åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨ã€‚

        Args:
            provider_configs (list): æ•°æ®æºæä¾›è€…çš„é…ç½®åˆ—è¡¨ã€‚
            symbols (list): éœ€è¦å¤„ç†çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨ã€‚
            start_date (str): æ•°æ®å¼€å§‹æ—¥æœŸã€‚
            end_date (str): æ•°æ®ç»“æŸæ—¥æœŸã€‚
            db_path (str, optional): å›æµ‹ä¸“ç”¨æ•°æ®åº“çš„è·¯å¾„ã€‚é»˜è®¤ä¸º 'quant_data.db'ã€‚
            num_checker_threads (int, optional): æ£€æŸ¥æ•°æ®å®Œæ•´æ€§çš„çº¿ç¨‹æ•°ã€‚é»˜è®¤ä¸º 4ã€‚
            num_downloader_threads (int, optional): ä¸‹è½½æ•°æ®çš„çº¿ç¨‹æ•°ï¼ˆç”Ÿäº§è€…ï¼‰ã€‚é»˜è®¤ä¸º 8ã€‚
            batch_size (int, optional): æ¶ˆè´¹è€…ä¸€æ¬¡æ€§å†™å…¥æ•°æ®åº“çš„æœ€å¤§æ•°æ®æ‰¹æ¬¡ã€‚é»˜è®¤ä¸º 100ã€‚
        """
        self.start_date = pd.to_datetime(start_date).strftime('%Y-%m-%d')
        self.end_date = pd.to_datetime(end_date).strftime('%Y-%m-%d')
        self.provider_configs = provider_configs
        self.symbols = symbols if isinstance(symbols, list) else [symbols]
        self.db_handler = DatabaseHandler(db_path)
        self.table_name = 'stock_daily_prices'

        # --- çº¿ç¨‹ä¸é˜Ÿåˆ—é…ç½® ---
        self.num_checker_threads = num_checker_threads
        self.num_downloader_threads = num_downloader_threads  # ä¿ç•™å‘½åï¼Œå®é™…ä¸ºç”Ÿäº§è€…æ•°é‡
        self.batch_size = batch_size

        # ç”¨äºæ£€æŸ¥è€…ç¡®å®šä¸‹è½½ä»»åŠ¡çš„é˜Ÿåˆ—
        self.symbols_queue = queue.Queue()
        # ç”¨äºå­˜æ”¾å¾…ä¸‹è½½ä»»åŠ¡çš„é˜Ÿåˆ— (ç”±æ£€æŸ¥è€…å¡«å……ï¼Œä¾›ä¸‹è½½è€…æ¶ˆè´¹)
        self.download_tasks_queue = queue.Queue()
        # ç”¨äºå­˜æ”¾å·²ä¸‹è½½æ•°æ®çš„ä¸­å¤®é˜Ÿåˆ— (ç”±ä¸‹è½½è€…å¡«å……ï¼Œä¾›å†™å…¥è€…æ¶ˆè´¹)
        self.results_queue = queue.Queue()
        # ç”¨äºé€šçŸ¥æ¶ˆè´¹è€…(å†™å…¥è€…)æ‰€æœ‰ç”Ÿäº§è€…(ä¸‹è½½è€…)å·²å®Œæˆä»»åŠ¡çš„äº‹ä»¶
        self.producers_finished_event = threading.Event()

        # --- è¿›åº¦æ¡ ---
        self.check_progress_bar = None
        self.download_progress_bar = None

        # --- äº¤æ˜“æ—¥å† ---
        self.all_trade_dates_set = set()
        if self.provider_configs:
            if self.provider_configs[0][0].__name__ == 'TushareDataProvider':
                self.calendar_provider = TushareTradingCalendar(
                    token=self.provider_configs[0][1].get('token'))
            else:
                self.calendar_provider = AkshareTradingCalendar()
        else:
            # å¦‚æœæ²¡æœ‰é…ç½®åœ¨çº¿æ•°æ®æºï¼Œä¹Ÿéœ€è¦ä¸€ä¸ªæ—¥å†æä¾›è€…ç”¨äºåç»­çš„æ•°æ®è´¨é‡æ ¡éªŒ
            self.calendar_provider = AkshareTradingCalendar()

    def _find_missing_date_ranges(self, symbol: str) -> list[tuple[str, str]]:
        """æ£€æŸ¥å•ä¸ªæ ‡çš„ï¼Œè¿”å›å…¶ç¼ºå¤±æ•°æ®çš„æ—¥æœŸåŒºé—´åˆ—è¡¨ã€‚"""
        logging.debug(f"æ£€æŸ¥çº¿ç¨‹ {threading.get_ident()} æ­£åœ¨ä¸º {symbol} æ£€æŸ¥æ•°æ®å®Œæ•´æ€§")
        all_trade_dates = self.all_trade_dates_set
        query = f"SELECT DISTINCT DATE(date) FROM {self.table_name} WHERE code = ? AND DATE(date) BETWEEN ? AND ?"
        existing_dates_df = self.db_handler.query_data(query,
                                                       params=(symbol,
                                                               self.start_date,
                                                               self.end_date))
        if existing_dates_df is not None and not existing_dates_df.empty:
            date_col = existing_dates_df.columns[0]
            existing_dates = set(
                pd.to_datetime(existing_dates_df[date_col]).dt.date)
        else:
            existing_dates = set()
        missing_dates = sorted(list(all_trade_dates - existing_dates))

        if not missing_dates:
            logging.info(f"âœ… [{symbol}] æ•°æ®å®Œæ•´ï¼Œæ— éœ€ä¸‹è½½ã€‚")
            return []

        logging.info(
            f"ğŸ“¥ [{symbol}] å‘ç° {len(missing_dates)} ä¸ªç¼ºå¤±çš„äº¤æ˜“æ—¥ï¼Œæ­£åœ¨åˆå¹¶ä¸ºä¸‹è½½åŒºé—´...")
        ranges = []
        if not missing_dates:
            return ranges
        start_range = missing_dates[0]
        for i in range(1, len(missing_dates)):
            # å¦‚æœä¸¤ä¸ªç¼ºå¤±æ—¥æœŸé—´éš”è¶…è¿‡ä¸€å‘¨ï¼Œå°±è®¤ä¸ºæ˜¯ä¸è¿ç»­çš„åŒºé—´
            if (missing_dates[i] - missing_dates[i - 1]).days > 7:
                end_range = missing_dates[i - 1]
                ranges.append((start_range.strftime('%Y-%m-%d'),
                               end_range.strftime('%Y-%m-%d')))
                start_range = missing_dates[i]
        ranges.append((start_range.strftime('%Y-%m-%d'),
                       missing_dates[-1].strftime('%Y-%m-%d')))
        for start, end in ranges:
            logging.debug(f"    -> [{symbol}] éœ€è¦ä¸‹è½½åŒºé—´: {start} to {end}")
        return ranges

    def _fetch_data_from_providers(self, symbol: str, start_date: str,
                                   end_date: str) -> pd.DataFrame | None:
        """
        æŒ‰é¡ºåºå°è¯•æ‰€æœ‰é…ç½®çš„æ•°æ®æºæ¥è·å–å•ä¸ªæ ‡çš„æ•°æ®ã€‚
        æ­¤æ–¹æ³•ç”±ç”Ÿäº§è€…(ä¸‹è½½è€…)çº¿ç¨‹è°ƒç”¨ï¼Œä»…è´Ÿè´£è·å–å’Œè¿”å›æ•°æ®ï¼Œä¸ä¿å­˜ã€‚
        """
        for provider_class, params in self.provider_configs:
            provider_instance = provider_class(**params)
            fetched_df = provider_instance.fetch_data(symbol, start_date,
                                                      end_date)
            if fetched_df is not None and not fetched_df.empty:
                logging.info(
                    f"  -> [ä¸‹è½½è€…] ä» {provider_instance.__class__.__name__} æˆåŠŸè·å– {symbol} çš„ {len(fetched_df)} æ¡æ•°æ®ã€‚"
                )
                # åœ¨è¿”å›å‰ï¼Œå°†è‚¡ç¥¨ä»£ç åŠ å…¥DataFrameä¸­ï¼Œä»¥ä¾¿æ¶ˆè´¹è€…è¯†åˆ«
                fetched_df['code'] = symbol
                return fetched_df.reset_index()

        logging.warning(
            f"  â€¼ï¸ [ä¸‹è½½è€…] è­¦å‘Šï¼šå°è¯•æ‰€æœ‰æ•°æ®æºåï¼Œä»æœªèƒ½è·å–åˆ° {symbol} ({start_date} to {end_date}) çš„æ•°æ®ã€‚"
        )
        return None

    def _producer_worker(self):
        """
        ç”Ÿäº§è€…(ä¸‹è½½è€…)çº¿ç¨‹çš„å·¥ä½œé€»è¾‘ã€‚
        ä¸æ–­ä»ä¸‹è½½ä»»åŠ¡é˜Ÿåˆ—ä¸­å–å‡ºä»»åŠ¡ï¼Œè·å–æ•°æ®ï¼Œç„¶åå°†ç»“æœDataFrameæ”¾å…¥ä¸­å¤®ç»“æœé˜Ÿåˆ—ã€‚
        """
        while True:
            try:
                # éé˜»å¡åœ°è·å–ä»»åŠ¡ï¼Œå¦‚æœé˜Ÿåˆ—ä¸ºç©ºåˆ™ä¼šç«‹å³å¼•å‘ queue.Empty å¼‚å¸¸
                symbol, missing_ranges = self.download_tasks_queue.get(
                    block=False)
            except queue.Empty:
                # ä»»åŠ¡é˜Ÿåˆ—å·²ç©ºï¼Œæ­¤ç”Ÿäº§è€…çº¿ç¨‹å¯ä»¥ç»“æŸå·¥ä½œ
                break

            for start_date, end_date in missing_ranges:
                result_df = self._fetch_data_from_providers(
                    symbol, start_date, end_date)
                if result_df is not None:
                    # å°†å¤„ç†å¥½çš„DataFrameæ”¾å…¥ç»“æœé˜Ÿåˆ—
                    self.results_queue.put(result_df)

            # æ ‡è®°æ­¤ä»»åŠ¡å®Œæˆï¼Œç”¨äºä¸»çº¿ç¨‹çš„ .join() åˆ¤æ–­
            self.download_tasks_queue.task_done()
            if self.download_progress_bar:
                self.download_progress_bar.update(1)

    def _consumer_worker(self):
        """
        æ¶ˆè´¹è€…(å†™å…¥è€…)çº¿ç¨‹çš„å·¥ä½œé€»è¾‘ã€‚
        åœ¨å•ä¸€çº¿ç¨‹ä¸­è¿è¡Œï¼Œä¸æ–­ä»ä¸­å¤®ç»“æœé˜Ÿåˆ—ä¸­è·å–æ•°æ®ï¼Œæ”’æˆä¸€æ‰¹åç»Ÿä¸€å†™å…¥æ•°æ®åº“ï¼Œ
        ä»¥é¿å…å¹¶å‘å†™å…¥å†²çªã€‚
        """
        batch = []
        # å¾ªç¯æ¡ä»¶ï¼šåªè¦â€œç”Ÿäº§è€…å°šæœªå…¨éƒ¨ç»“æŸâ€æˆ–è€…â€œç»“æœé˜Ÿåˆ—é‡Œè¿˜æœ‰ä¸œè¥¿â€ï¼Œå°±ç»§ç»­å·¥ä½œ
        while not (self.producers_finished_event.is_set()
                   and self.results_queue.empty()):
            try:
                # è®¾ç½®1ç§’è¶…æ—¶ï¼Œé¿å…åœ¨ç”Ÿäº§è€…å·¥ä½œæ…¢æ—¶æ°¸ä¹…é˜»å¡ã€‚
                # è¿™ä¹Ÿè®©å¾ªç¯å¯ä»¥å‘¨æœŸæ€§åœ°æ£€æŸ¥ä¸Šé¢çš„é€€å‡ºæ¡ä»¶ã€‚
                result_df = self.results_queue.get(timeout=1)
                batch.append(result_df)

                # å½“æ”’å¤Ÿä¸€ä¸ªæ‰¹æ¬¡æ—¶ï¼Œå°±æ‰§è¡Œä¸€æ¬¡å†™å…¥
                if len(batch) >= self.batch_size:
                    self._save_batch_to_db(batch)
                    batch = []  # æ¸…ç©ºæ‰¹æ¬¡ï¼Œå‡†å¤‡ä¸‹ä¸€æ‰¹
            except queue.Empty:
                # é˜Ÿåˆ—æš‚æ—¶ä¸ºç©ºæ˜¯æ­£å¸¸ç°è±¡ï¼Œæ¶ˆè´¹è€…ä¼šç»§ç»­å¾ªç¯ç­‰å¾…ï¼Œç›´åˆ°é€€å‡ºæ¡ä»¶æ»¡è¶³
                continue

        # æ‰€æœ‰ç”Ÿäº§è€…éƒ½ç»“æŸåï¼Œå¤„ç†æœ€åä¸€æ‰¹å¯èƒ½ä¸æ»¡å°ºå¯¸çš„æ•°æ®
        if batch:
            self._save_batch_to_db(batch)
        logging.info("--- [å†™å…¥è€…] æ‰€æœ‰æ•°æ®å·²å¤„ç†å®Œæ¯•ï¼Œå†™å…¥çº¿ç¨‹é€€å‡ºã€‚ ---")

    def _save_batch_to_db(self, batch: list):
        """è¾…åŠ©æ–¹æ³•ï¼šåˆå¹¶å¤šä¸ªDataFrameå¹¶è°ƒç”¨æ•°æ®åº“å¤„ç†å™¨è¿›è¡Œä¿å­˜ã€‚"""
        if not batch:
            return
        try:
            # å°†åˆ—è¡¨ä¸­çš„æ‰€æœ‰DataFrameåˆå¹¶æˆä¸€ä¸ªå¤§çš„DataFrame
            full_df = pd.concat(batch, ignore_index=True)
            logging.info(
                f"--- [å†™å…¥è€…] æ­£åœ¨åˆå¹¶ {len(batch)} ä¸ªDataFrame ({len(full_df)} è¡Œ)ï¼Œå¹¶å­˜å…¥æ•°æ®åº“... ---"
            )
            self.db_handler.save_data(full_df, self.table_name)
        except Exception as e:
            logging.error(f"--- âŒ [å†™å…¥è€…] æ‰¹é‡æ•°æ®ä¿å­˜è‡³æ•°æ®åº“æ—¶å‡ºé”™: {e} ---")

    def _checker_worker(self):
        """æ£€æŸ¥è€…çº¿ç¨‹çš„å·¥ä½œé€»è¾‘ã€‚ä»è‚¡ç¥¨æ± é˜Ÿåˆ—å–è‚¡ç¥¨ï¼Œæ£€æŸ¥åå°†ä»»åŠ¡æ”¾å…¥ä¸‹è½½é˜Ÿåˆ—ã€‚"""
        while True:
            try:
                symbol = self.symbols_queue.get(block=False)
            except queue.Empty:
                break
            missing_ranges = self._find_missing_date_ranges(symbol)
            if missing_ranges:
                self.download_tasks_queue.put((symbol, missing_ranges))
            self.symbols_queue.task_done()
            if self.check_progress_bar:
                self.check_progress_bar.update(1)

    def prepare_data_for_universe(self):
        """
        å‡†å¤‡æ‰€æœ‰æ•°æ®çš„ä¸»å…¥å£æ–¹æ³•ï¼Œè´Ÿè´£ç¼–æ’æ•´ä¸ªå¤šçº¿ç¨‹æµç¨‹ã€‚
        """
        if not self.provider_configs:
            logging.info("â„¹ï¸  æœªé…ç½®æ•°æ®æºï¼Œè·³è¿‡æ•°æ®ä¸‹è½½ï¼Œä»…ä½¿ç”¨æœ¬åœ°æ•°æ®åº“æ•°æ®ã€‚")
            return

        logging.info("--- å¼€å§‹æ•°æ®å‡†å¤‡æµç¨‹ (ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼) ---")

        # æµç¨‹1: è·å–äº¤æ˜“æ—¥å†ï¼Œä½œä¸ºåç»­æ£€æŸ¥çš„åŸºå‡†
        logging.info("ğŸ—“ï¸  æ­£åœ¨è·å–äº¤æ˜“æ—¥å†...")
        try:
            all_trade_dates_str = self.calendar_provider.get_trading_days(
                self.start_date, self.end_date)
            if not all_trade_dates_str:
                logging.critical(f"  âŒ è‡´å‘½é”™è¯¯ï¼šæ— æ³•è·å–äº¤æ˜“æ—¥å†ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
                return
            self.all_trade_dates_set = set(
                pd.to_datetime(all_trade_dates_str).date)
            logging.info(f"  âœ… æˆåŠŸè·å– {len(self.all_trade_dates_set)} ä¸ªäº¤æ˜“æ—¥ã€‚")
        except Exception as e:
            logging.critical(f"  âŒ è‡´å‘½é”™è¯¯ï¼šè·å–äº¤æ˜“æ—¥å†æ—¶å‡ºé”™: {e}ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
            return

        # æµç¨‹2: å¯åŠ¨æ£€æŸ¥è€…çº¿ç¨‹ï¼Œå¹¶è¡Œæ£€æŸ¥æ‰€æœ‰è‚¡ç¥¨ï¼Œå¡«å……ä¸‹è½½ä»»åŠ¡é˜Ÿåˆ—
        for symbol in self.symbols:
            self.symbols_queue.put(symbol)
        logging.info("ğŸ” æ­£åœ¨æ£€æŸ¥æ•°æ®å®Œæ•´æ€§...")
        with tqdm(total=len(self.symbols), desc="æ£€æŸ¥è¿›åº¦", ncols=100) as pbar:
            self.check_progress_bar = pbar
            checker_threads = []
            for _ in range(self.num_checker_threads):
                thread = threading.Thread(target=self._checker_worker)
                thread.start()
                checker_threads.append(thread)
            # ç­‰å¾…æ‰€æœ‰æ£€æŸ¥è€…å®Œæˆå·¥ä½œ
            for thread in checker_threads:
                thread.join()
        self.check_progress_bar = None

        total_downloads = self.download_tasks_queue.qsize()
        if total_downloads > 0:
            logging.info(
                f"ğŸ“¥ å³å°†å¯åŠ¨ {self.num_downloader_threads} ä¸ªä¸‹è½½çº¿ç¨‹(ç”Ÿäº§è€…) å’Œ 1 ä¸ªå†™å…¥çº¿ç¨‹(æ¶ˆè´¹è€…)..."
            )
        else:
            logging.info("âœ… æ‰€æœ‰æ•°æ®å‡å·²å®Œæ•´ï¼Œæ— éœ€ä¸‹è½½ã€‚")
            return

        # æµç¨‹3: å¯åŠ¨å”¯ä¸€çš„æ¶ˆè´¹è€…(å†™å…¥è€…)çº¿ç¨‹ï¼Œå®ƒä¼šç«‹åˆ»å¼€å§‹ç­‰å¾…ç»“æœé˜Ÿåˆ—ä¸­çš„æ•°æ®
        consumer_thread = threading.Thread(target=self._consumer_worker)
        consumer_thread.start()

        # æµç¨‹4: å¯åŠ¨å¤šä¸ªç”Ÿäº§è€…(ä¸‹è½½è€…)çº¿ç¨‹ï¼Œå®ƒä»¬ä¼šå¼€å§‹å¤„ç†ä¸‹è½½ä»»åŠ¡
        with tqdm(total=total_downloads, desc="ä¸‹è½½è¿›åº¦", ncols=100) as pbar:
            self.download_progress_bar = pbar
            producer_threads = []
            for _ in range(self.num_downloader_threads):
                thread = threading.Thread(target=self._producer_worker)
                thread.start()
                producer_threads.append(thread)

            # ç­‰å¾…æ‰€æœ‰ç”Ÿäº§è€…(ä¸‹è½½è€…)å®Œæˆå®ƒä»¬çš„å·¥ä½œ
            for thread in producer_threads:
                thread.join()

        # æµç¨‹5: æ‰€æœ‰ç”Ÿäº§è€…å·²ç»“æŸï¼Œè®¾ç½®äº‹ä»¶ï¼Œè¿™æ˜¯ç»™æ¶ˆè´¹è€…çš„ä¿¡å·ï¼šä¸ä¼šå†æœ‰æ–°æ•°æ®äº†
        self.producers_finished_event.set()

        # æµç¨‹6: ç­‰å¾…æ¶ˆè´¹è€…å¤„ç†å®Œæ‰€æœ‰å‰©ä½™ä»»åŠ¡å¹¶æœ€ç»ˆé€€å‡º
        consumer_thread.join()

        logging.info("--- æ‰€æœ‰æ•°æ®å‡†å¤‡æµç¨‹æ‰§è¡Œå®Œæ¯• ---")

    # --- ä»¥ä¸‹ä¸ºä¾›å¤–éƒ¨è°ƒç”¨çš„æ ‡å‡†æ¥å£æ–¹æ³•ï¼Œä¿æŒä¸å˜ ---

    def get_bt_feed(self, symbol: str) -> bt.feeds.PandasData | None:
        """ä»æ•°æ®åº“è·å–å•ä¸ªæ ‡çš„çš„æ•°æ®ï¼Œå¹¶åŒ…è£…æˆBacktraderçš„feedæ ¼å¼ã€‚"""
        df = self.get_dataframe(symbol)
        if df is not None and not df.empty:
            return bt.feeds.PandasData(dataname=df)
        logging.error(f"âŒ æœªèƒ½ä¸º {symbol} è·å–æœ‰æ•ˆæ•°æ®ï¼Œæ— æ³•åˆ›å»ºBacktrader feedã€‚")
        return None

    def get_dataframe(self, symbol: str) -> pd.DataFrame | None:
        """ä»æ•°æ®åº“è·å–å¹¶è¿”å›å•ä¸ªæ ‡çš„çš„DataFrameã€‚"""
        query = f"SELECT * FROM {self.table_name} WHERE code = ? AND date BETWEEN ? AND ?"
        params = (symbol, self.start_date, self.end_date)
        df = self.db_handler.query_data(query, params)
        if df is not None and not df.empty:
            df.sort_index(ascending=True, inplace=True)
        return df

    def validate_data_quality(self, symbol: str) -> bool:
        """ä¸¥æ ¼æ ¡éªŒå•ä¸ªæ ‡çš„æ•°æ®è´¨é‡ï¼Œç¡®ä¿æ•°æ®å®Œæ•´ä¸”æœ‰æ•ˆã€‚"""
        logging.info(f"--- æ­£åœ¨æ ¡éªŒ '{symbol}' çš„æ•°æ®è´¨é‡...")
        if not self.all_trade_dates_set:
            logging.error(f"ğŸ”´ äº¤æ˜“æ—¥å†æ•°æ®æœªåŠ è½½ï¼Œæ— æ³•å¯¹ '{symbol}' è¿›è¡Œå®Œæ•´æ€§æ ¡éªŒã€‚")
            return False
        df = self.get_dataframe(symbol)
        if df is None or df.empty:
            logging.warning(
                f"ğŸŸ¡ '{symbol}' åœ¨ {self.start_date} åˆ° {self.end_date} æœŸé—´æ— æ•°æ®ï¼Œå·²å‰”é™¤ã€‚"
            )
            return False
        existing_dates = set(pd.to_datetime(df.index).date)
        missing_dates = self.all_trade_dates_set - existing_dates
        if missing_dates:
            example_missing = sorted(list(missing_dates))[:3]
            logging.warning(
                f"ğŸ”´ '{symbol}' ç¼ºå¤± {len(missing_dates)} ä¸ªäº¤æ˜“æ—¥çš„æ•°æ® (ä¾‹å¦‚: {example_missing})ï¼Œå·²å‰”é™¤ã€‚"
            )
            return False
        cols_to_check = ['open', 'high', 'low', 'close', 'volume']
        if df[cols_to_check].isnull().values.any():
            problematic_rows = df[df[cols_to_check].isnull().any(axis=1)]
            logging.warning(
                f"ğŸ”´ '{symbol}' çš„æ•°æ®ä¸­åŒ…å«ç©ºå€¼ (NaN)ï¼Œå·²å‰”å‡ºã€‚é—®é¢˜æ•°æ®å¿«ç…§:\n{problematic_rows.head(3)}"
            )
            return False
        if (df[cols_to_check] <= 0).any().any():
            problematic_rows = df[(df[cols_to_check] <= 0).any(axis=1)]
            logging.warning(
                f"ğŸ”´ '{symbol}' çš„æ•°æ®ä¸­åŒ…å«0æˆ–è´Ÿå€¼ï¼Œå·²å‰”å‡ºã€‚é—®é¢˜æ•°æ®å¿«ç…§:\n{problematic_rows.head(3)}"
            )
            return False
        logging.info(f"âœ… '{symbol}' æ•°æ®è´¨é‡æ ¡éªŒé€šè¿‡ã€‚")
        return True

    def __del__(self):
        """åœ¨å¯¹è±¡é”€æ¯æ—¶ï¼Œç¡®ä¿å…³é—­æ•°æ®åº“è¿æ¥ã€‚"""
        self.db_handler.close_connection()
