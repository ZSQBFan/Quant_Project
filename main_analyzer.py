# main_analyzer.py

import os
import logging
import pandas as pd
from tqdm import tqdm
import sys
import datetime

# ==============================================================================
# 1. ç­–ç•¥åˆ†æâ€œæ§åˆ¶é¢æ¿â€ (Strategy Analysis "Control Panel")
# ==============================================================================

# --- 1a. æ ¸å¿ƒç­–ç•¥é€‰æ‹© (Core Strategy Selection) ---
from strategy_configs import STRATEGY_REGISTRY

# ã€ã€è¯·åœ¨è¿™é‡Œé€‰æ‹©æ‚¨çš„ç­–ç•¥åç§° (ä» strategy_configs.py å¤åˆ¶)ã€‘ã€‘
# STRATEGY_NAME = "RollingICIR"
# STRATEGY_NAME = "RollingRegression"
# STRATEGY_NAME = "FixedWeights"
STRATEGY_NAME = "EqualWeights"
# STRATEGY_NAME = "DynamicSignificance"
# STRATEGY_NAME = "LightGBM_Periodic"
# STRATEGY_NAME = "AdversarialLLM"

if STRATEGY_NAME not in STRATEGY_REGISTRY:
    raise ValueError(f"ç­–ç•¥ '{STRATEGY_NAME}' æœªåœ¨ strategy_configs.py ä¸­æ³¨å†Œã€‚")
STRATEGY_CONFIG = STRATEGY_REGISTRY[STRATEGY_NAME]

# ---  1b. å› å­é€‰æ‹© (Factors to Analyze) ---
#
#   ç°åœ¨æ‚¨åªéœ€è¦åˆ—å‡ºå› å­åç§°ã€‚
#   å…·ä½“çš„å‚æ•° (params) å’Œæ•°æ®ä¾èµ– (required_columns) å·²åœ¨ factor_configs.py ä¸­ç»Ÿä¸€å®šä¹‰ã€‚
#
from factor_configs import FACTOR_REGISTRY
from factor_analysis.factors_complex import COMPLEX_FACTOR_REGISTRY

FACTORS_TO_RUN = [
    # 'Momentum',
    # 'Reversal20D',
    "IndNeu_Momentum",
    "IndNeu_Reversal20D",
    "IndNeu_VolumeCV",
    "IndNeu_EP",
    # "IndNeu_BP"
    # "IndNeu_ROE",
    "IndNeu_SalesGrowth",  #ä¼¼ä¹æœ‰ç‚¹é—®é¢˜ï¼Ÿ
    "IndNeu_CFOP",
    "IndNeu_GPM",
    # "IndNeu_AssetTurnover",
    # "IndNeu_CurrentRatio",
]

# --- 1d. æˆªé¢æ•°æ®é…ç½® (Cross-Sectional Data) ---
#   ã€å…¨å±€å¼€å…³ã€‘: æ˜¯å¦åœ¨ä»»ä½•æƒ…å†µä¸‹éƒ½å¼ºåˆ¶åŠ è½½è¡Œä¸šæ•°æ®ï¼Ÿ
LOAD_INDUSTRY_DATA = False

# --- 1e. æ ‡å‡†åŒ–å™¨ (Standardizer) ---
from strategies.standardizers import (CrossSectionalZScoreStandardizer,
                                      NoStandardizer,
                                      CrossSectionalQuantileStandardizer)

STANDARDIZER_CLASS = CrossSectionalZScoreStandardizer  #è·¨åº¦Zåˆ†æ•°æ ‡å‡†åŒ–
# STANDARDIZER_CLASS = NoStandardizer  #ä¸è¿›è¡Œæ ‡å‡†åŒ–
# STANDARDIZER_CLASS = CrossSectionalQuantileStandardizer  #æˆªé¢åˆ†ä½æ•°æ ‡å‡†åŒ–

# ==============================================================================
# 2. åŸºç¡€å›æµ‹ä¸è·¯å¾„é…ç½® (Basic Backtest & Path Settings)
# ==============================================================================

#   ã€ã€ã€é‡è¦ã€‘ã€‘ã€‘æ•°æ®ä¸‹è½½ä¸å†™å…¥å¼€å…³ï¼š
#   True: è·³è¿‡ä¸‹è½½ï¼Œç›´æ¥ä½¿ç”¨æ•°æ®åº“ (è°ƒè¯•å› å­é€»è¾‘æ—¶ç”¨)
#   False: æ£€æŸ¥å¹¶ä¸‹è½½ç¼ºå¤±æ•°æ® (æ—¥å¸¸æ›´æ–°æ•°æ®æ—¶ç”¨)
SKIP_DATA_PREPARATION = True

# --- 2a. å›æµ‹æ—¶é—´ä¸æ”¶ç›Šå‘¨æœŸ ---
START_DATE = '2023-01-01'
END_DATE = '2023-12-31'
FORWARD_RETURN_PERIODS = [1, 5, 10, 20, 30, 90]

# --- 2b. åŸºå‡†ä¸è‚¡ç¥¨æ±  ---
BENCHMARK = '600519'  # èŒ…å°
from universe_config import UNIVERSE

# å› å­è®¡ç®—è¿›ç¨‹æ•°
FACTOR_CALC_PROCESSES = 16

# --- 2c. è·¯å¾„é…ç½® ---
LOG_DIR = "logs"
OUTPUT_DIR = "factor_reports"
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)
BACKTEST_DB_PATH = './database/quant_data.db'

# --- 2d. æ•°æ®æºé…ç½® ---
from data.data_providers import SQLiteDataProvider

DATA_PROVIDERS_CONFIG = [
    (
        SQLiteDataProvider,
        {
            'db_path': './database/JY_database/sqlite/JY_database.sqlite',
            'table_name': 'JY_t_price_daily'  # æˆ–è€…æ˜¯æ‚¨çš„æ•°æ®æºè¡¨å
        }),
]

# ==============================================================================
#
#                     --- æ ¸å¿ƒç¨‹åºå¼€å§‹ (Core Logic Starts) ---
#
# ==============================================================================

from data.data_manager import DataProviderManager
from factor_analysis.factor_calculator import FactorCalculator
from factor_analysis.factor_report import FactorReport
from logger.logger_config import setup_logging

if __name__ == '__main__':

    # =====================
    # 0. åˆå§‹åŒ–ä¸é…ç½®æ ¡éªŒ
    # =====================
    setup_logging(log_dir=LOG_DIR, log_prefix='factor_analysis')
    logging.info(f"\n{'='*60}\n--- æ­¥éª¤ 0: åˆå§‹åŒ–ä¸é…ç½®æ ¡éªŒ ---\n{'='*60}")
    logging.info("ğŸ å› å­åˆ†æç¨‹åºå¯åŠ¨...")

    STANDARDIZER = STANDARDIZER_CLASS()
    logging.info(f"âœ… æ ‡å‡†åŒ–å™¨å·²åŠ è½½: {STANDARDIZER.__class__.__name__}")
    logging.info(f"âš™ï¸ æ­£åœ¨åŠ è½½ç­–ç•¥: {STRATEGY_NAME}")

    # =====================
    # 1. åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
    # =====================
    logging.info(f"\n{'='*60}\n--- æ­¥éª¤ 1: å‡†å¤‡æ•°æ® ---\n{'='*60}")
    logging.info("âš™ï¸ æ­£åœ¨åˆå§‹åŒ– DataProviderManager...")
    data_manager = DataProviderManager(provider_configs=DATA_PROVIDERS_CONFIG,
                                       symbols=UNIVERSE,
                                       start_date=START_DATE,
                                       end_date=END_DATE,
                                       db_path=BACKTEST_DB_PATH,
                                       num_checker_threads=16,
                                       num_downloader_threads=16,
                                       batch_size=200)

    if BENCHMARK not in data_manager.symbols:
        data_manager.symbols.append(BENCHMARK)

    if not SKIP_DATA_PREPARATION:
        logging.info("âš™ï¸ æ¨¡å¼: å®Œæ•´æ•°æ®å‡†å¤‡ (æ£€æŸ¥ã€ä¸‹è½½ã€å†™å…¥)...")
        data_manager.prepare_data_for_universe()
    else:
        logging.info("ğŸŸ¡ ã€ã€è·³è¿‡ã€‘ã€‘: å·²æŒ‰é…ç½®è·³è¿‡æ•°æ®ä¸‹è½½æµç¨‹ã€‚")

    # è·å–åŸºå‡†æ•°æ®
    logging.info(f"âš™ï¸ è·å–åŸºå‡† '{BENCHMARK}' æ•°æ®...")
    benchmark_df = data_manager.get_dataframe(BENCHMARK, columns=['close'])

    active_universe = data_manager.symbols.copy()
    if BENCHMARK in active_universe:
        active_universe.remove(BENCHMARK)

    # =====================
    # 1.5 ç»Ÿä¸€è®¡ç®—æœªæ¥æ”¶ç›Šç‡
    # =====================
    logging.info(f"\n{'='*60}\n--- æ­¥éª¤ 1.5: é¢„è®¡ç®—æœªæ¥æ”¶ç›Šç‡ ---\n{'='*60}")
    future_returns_df = data_manager.calculate_universe_forward_returns(
        universe=active_universe,
        forward_return_periods=FORWARD_RETURN_PERIODS)

    if future_returns_df is None or future_returns_df.empty:
        logging.critical("â›” è‡´å‘½é”™è¯¯: æœªèƒ½è®¡ç®—å‡ºæœªæ¥æ”¶ç›Šç‡ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
        sys.exit()

    future_returns_df.set_index('date', inplace=True)

    # ==============================================================================
    # ã€ã€ã€æ–°å¢æ­¥éª¤ 1.6: å› å­åˆ†ç±»ä¸è·¯ç”±ã€‘ã€‘ã€‘
    # ==============================================================================
    simple_factors_batch = []
    complex_factors_batch = []

    logging.info(f"\n{'='*60}\n--- æ­¥éª¤ 1.6: å› å­åˆ†ç±»ä¸è·¯ç”± ---\n{'='*60}")

    # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨çš„æ˜¯ç»Ÿä¸€çš„ FACTORS_TO_RUN åˆ—è¡¨
    for item in FACTORS_TO_RUN:
        # è§£æé…ç½® (æ”¯æŒ 'Name' æˆ– ('Name', params) æ ¼å¼)
        if isinstance(item, tuple):
            f_name = item[0]
            f_params = item[1]
        else:
            f_name = item
            f_params = {}

        # æ£€æŸ¥æ³¨å†Œè¡¨
        if f_name not in FACTOR_REGISTRY:
            logging.warning(f"âš ï¸ è·³è¿‡: å› å­ '{f_name}' æœªåœ¨ factor_configs.py ä¸­æ³¨å†Œã€‚")
            continue

        config = FACTOR_REGISTRY[f_name]
        # è·å–å› å­ç±»åˆ«ï¼Œé»˜è®¤ä¸º 'simple'
        category = config.get('category', 'simple')

        # åˆ†æµé€»è¾‘
        if category == 'simple':
            simple_factors_batch.append((f_name, f_params))
        elif category == 'complex':
            complex_factors_batch.append(f_name)
        else:
            logging.warning(
                f"âš ï¸ è·³è¿‡: å› å­ '{f_name}' çš„ category '{category}' æ— æ•ˆã€‚")

    logging.info(f"ğŸ“‹ ç®€å•å› å­ (å¤šè¿›ç¨‹è®¡ç®—): {[f[0] for f in simple_factors_batch]}")
    logging.info(f"ğŸ“‹ å¤åˆå› å­ (å…¨é‡è¡¨è®¡ç®—): {complex_factors_batch}")

    # ==============================================================================
    # ã€ã€ã€æ–°å¢æ­¥éª¤ 1.7: é¢„è®¡ç®—æ‰€éœ€æ•°æ®åˆ—ã€‘ã€‘ã€‘
    # ==============================================================================
    logging.info(f"\n{'='*60}\n--- æ­¥éª¤ 1.7: é¢„è®¡ç®—æ‰€æœ‰å› å­æ‰€éœ€çš„æ•°æ®åˆ— ---\n{'='*60}")

    all_required_columns = set()

    # æ”¶é›†æ‰€æœ‰æ´»è·ƒå› å­çš„åˆ—éœ€æ±‚
    all_active_factors = [f[0] for f in simple_factors_batch
                          ] + complex_factors_batch

    for factor_name in all_active_factors:
        required = FACTOR_REGISTRY[factor_name].get('required_columns', [])
        all_required_columns.update(required)

    # å¤„ç†å…¨å±€è¡Œä¸šæ•°æ®å¼€å…³
    if LOAD_INDUSTRY_DATA:
        all_required_columns.add('industry')

    sorted_cols = sorted(list(all_required_columns))
    logging.info(f"âœ… æœ¬æ¬¡è¿è¡Œä¼˜åŒ–åçš„æ•°æ®åˆ—éœ€æ±‚: {sorted_cols}")

    # =====================
    # 2. è®¡ç®—å› å­åŸå§‹å€¼
    # =====================
    all_factors_dfs = {}

    logging.info(f"\n{'='*60}\n--- æ­¥éª¤ 2: æ‰§è¡Œå› å­è®¡ç®— ---\n{'='*60}")

    # --- åˆ†æ”¯ A: æ‰§è¡Œç®€å•å› å­ (Simple Factors) ---
    if not simple_factors_batch:
        logging.info("â„¹ï¸ (æ— ç®€å•å› å­éœ€è¦è®¡ç®—)")
    else:
        for factor_name, user_params in simple_factors_batch:
            # è·å–é…ç½®
            registry_config = FACTOR_REGISTRY.get(factor_name, {})
            registry_params = registry_config.get('params', {})
            required_cols = registry_config.get('required_columns', [])

            # åˆå¹¶å‚æ•° (main é…ç½®è¦†ç›– registry é…ç½®)
            final_params = {**registry_params, **user_params}

            logging.info(f"âš™ï¸ [Simple] å¯åŠ¨è®¡ç®—å™¨: {factor_name}...")

            calculator = FactorCalculator(
                provider_configs=data_manager.provider_configs,
                db_path=BACKTEST_DB_PATH,
                universe=active_universe,
                start_date=START_DATE,
                end_date=END_DATE,
                factor_name=factor_name,
                factor_params=final_params,
                num_threads=FACTOR_CALC_PROCESSES,
                required_columns=required_cols  # æŒ‰éœ€åŠ è½½
            )

            factor_data_df = calculator.calculate_factor()

            if not factor_data_df.empty:
                factor_series = factor_data_df.set_index(
                    'asset', append=True)['factor_value']
                factor_series.name = factor_name
                all_factors_dfs[factor_name] = factor_series.sort_index()
                logging.info(f"  > âœ… å®Œæˆ: {factor_name}")

    # --- åˆ†æ”¯ B: æ‰§è¡Œå¤åˆå› å­ (Complex Factors) ---
    if not complex_factors_batch:
        logging.info("â„¹ï¸ (æ— å¤åˆå› å­éœ€è¦è®¡ç®—)")
    else:
        logging.info(
            f"âš™ï¸ [Complex] æ­£åœ¨ä¸ºå¤åˆå› å­åŠ è½½å®½è¡¨æ•°æ® (Cols: {len(sorted_cols)})...")

        # åŠ è½½æ‰€æœ‰éœ€è¦çš„åˆ—
        all_data_df = data_manager.get_all_data_for_universe(
            active_universe, required_columns=sorted_cols)

        if all_data_df is None or all_data_df.empty:
            logging.error("âŒ æ— æ³•åŠ è½½æ•°æ®ï¼Œè·³è¿‡å¤åˆå› å­è®¡ç®—ã€‚")
        else:
            for factor_name in complex_factors_batch:
                if factor_name in COMPLEX_FACTOR_REGISTRY:
                    logging.info(f"âš™ï¸ [Complex] è®¡ç®—: {factor_name}...")
                    factor_func = COMPLEX_FACTOR_REGISTRY[factor_name]

                    # å¤åˆå› å­å‡½æ•°ç›´æ¥æ¥æ”¶ DataFrame
                    factor_series = factor_func(all_data_df)

                    if factor_series is not None:
                        factor_series.name = factor_name
                        all_factors_dfs[
                            factor_name] = factor_series.sort_index()
                        logging.info(f"  > âœ… å®Œæˆ: {factor_name}")
                else:
                    logging.warning(
                        f"âš ï¸ è­¦å‘Š: å› å­ {factor_name} åœ¨ COMPLEX_FACTOR_REGISTRY ä¸­æœªæ‰¾åˆ°ã€‚"
                    )

    # =====================
    # 3. å› å­åˆå¹¶ä¸åˆ†æ
    # =====================
    final_factor_data_df = pd.DataFrame()
    final_factor_name = ""
    FACTOR_NAMES = list(all_factors_dfs.keys())

    logging.info(f"\n{'='*60}\n--- æ­¥éª¤ 3: å› å­åˆå¹¶ä¸åˆ†æ ---\n{'='*60}")
    logging.info(f"â„¹ï¸ å¾…åˆå¹¶å› å­: {FACTOR_NAMES}")

    if not FACTOR_NAMES:
        logging.warning("âš ï¸ æ²¡æœ‰è®¡ç®—å‡ºä»»ä½•å› å­ï¼Œæµç¨‹ç»ˆæ­¢ã€‚")
    elif len(FACTOR_NAMES) == 1:
        logging.info("â„¹ï¸ å•å› å­æ¨¡å¼ã€‚")
        final_factor_name = FACTOR_NAMES[0]
        combined_factors_df = all_factors_dfs[final_factor_name].to_frame()
        
        # ã€ä¿®å¤ã€‘ç¡®ä¿ç´¢å¼•ç»“æ„æ­£ç¡®ï¼šåº”è¯¥æ˜¯ ['date', 'asset'] è€Œä¸æ˜¯ ['date', 'date', 'asset']
        if combined_factors_df.index.nlevels > 2:
            logging.warning(f"âš ï¸ æ£€æµ‹åˆ°å¼‚å¸¸ç´¢å¼•ç»“æ„: {combined_factors_df.index.names}ï¼Œæ­£åœ¨ä¿®å¤...")
            # é‡ç½®æ‰€æœ‰ç´¢å¼•ï¼Œç„¶åé‡æ–°è®¾ç½®æ­£ç¡®çš„å¤šçº§ç´¢å¼•
            combined_factors_df = combined_factors_df.reset_index()
            # åˆ é™¤é‡å¤çš„ date åˆ—ï¼ˆä¿ç•™ç¬¬ä¸€ä¸ªï¼‰
            if 'date' in combined_factors_df.columns and combined_factors_df.columns.tolist().count('date') > 1:
                date_cols = [i for i, col in enumerate(combined_factors_df.columns) if col == 'date']
                # ä¿ç•™ç¬¬ä¸€ä¸ª date åˆ—ï¼Œåˆ é™¤å…¶ä»–çš„
                cols_to_drop = [combined_factors_df.columns[i] for i in date_cols[1:]]
                combined_factors_df = combined_factors_df.drop(columns=cols_to_drop)
            
            # é‡æ–°è®¾ç½®æ­£ç¡®çš„å¤šçº§ç´¢å¼•
            combined_factors_df = combined_factors_df.set_index(['date', 'asset'])
        
        # å•å› å­é€šå¸¸ä¸éœ€è¦æ ‡å‡†åŒ–ç”¨äºåˆæˆï¼Œä½†å¦‚æœéœ€è¦ç»Ÿä¸€é‡çº²å¯ä»¥æ‰“å¼€ä¸‹é¢è¿™è¡Œ
        # combined_factors_df = combined_factors_df.groupby(level='date').apply(lambda x: STANDARDIZER.standardize(x))
    else:
        logging.info("âš™ï¸ æ­¥éª¤ 3a: åˆå¹¶å› å­æ•°æ®...")

        combined_factors_df = pd.concat(all_factors_dfs.values(),
                                        axis=1,
                                        keys=all_factors_dfs.keys())
        if isinstance(combined_factors_df.columns, pd.MultiIndex):
            combined_factors_df.columns = combined_factors_df.columns.droplevel(
                1)

        # ======================================================================
        # ã€ã€ã€å…³é”®ä¿®æ­£ã€‘ã€‘ã€‘: åœ¨è¿›å…¥ç­–ç•¥åˆ†æ”¯å‰ï¼Œç»Ÿä¸€è¿›è¡Œå…¨å±€æˆªé¢æ ‡å‡†åŒ–
        # ======================================================================
        logging.info(
            f"âš™ï¸ æ­¥éª¤ 3b: æ‰§è¡Œå…¨å±€æˆªé¢æ ‡å‡†åŒ– ({STANDARDIZER.__class__.__name__})...")

        combined_factors_df = combined_factors_df.groupby(
            level='date',
            group_keys=False).apply(lambda x: STANDARDIZER.standardize(x))
        
        # ã€ä¿®å¤ã€‘æ ‡å‡†åŒ–åæ£€æŸ¥å¹¶ä¿®å¤å¯èƒ½çš„é‡å¤ç´¢å¼•é—®é¢˜
        if combined_factors_df.index.nlevels > 2:
            logging.warning(f"âš ï¸ æ ‡å‡†åŒ–åæ£€æµ‹åˆ°å¼‚å¸¸ç´¢å¼•: {combined_factors_df.index.names}")
            # ç›´æ¥åˆ é™¤é‡å¤çš„ç´¢å¼•å±‚çº§ï¼ˆä¿ç•™å‰ä¸¤ä¸ªï¼šdate, assetï¼‰
            while combined_factors_df.index.nlevels > 2:
                combined_factors_df.index = combined_factors_df.index.droplevel(-1)
            combined_factors_df.index.names = ['date', 'asset']
            logging.info(f"âœ… æ ‡å‡†åŒ–åç´¢å¼•å·²ä¿®å¤: {combined_factors_df.index.names}")
            
        logging.info("  > âœ… æ‰€æœ‰å› å­å·²å®Œæˆæ ‡å‡†åŒ–å¤„ç†ã€‚")

        # æ ¸å¿ƒç­–ç•¥é€»è¾‘
        if not STRATEGY_CONFIG.is_rolling():
            # A. é™æ€ç­–ç•¥
            logging.info(
                f"â„¹ï¸ æ¨¡å¼: é™æ€åˆæˆ (ç­–ç•¥: {STRATEGY_CONFIG.combiner_class.__name__})")
            combiner = STRATEGY_CONFIG.combiner_class(
                **STRATEGY_CONFIG.combiner_kwargs)

            logging.info("âš™ï¸ æ­¥éª¤ 3c: å› å­åˆæˆ...")
            composite_factor_series = combined_factors_df.groupby(
                level='date').apply(lambda x: combiner.combine(x))
            composite_factor_series.name = 'factor_value'
            final_factor_name = f"Composite_{STRATEGY_NAME}"
        else:
            # B. åŠ¨æ€æ»šåŠ¨ç­–ç•¥
            logging.info(f"â„¹ï¸ æ¨¡å¼: åŠ¨æ€æ»šåŠ¨ (æ¯æ—¥æƒé‡è®¡ç®—)")
            
            # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿factor_namesä¸åˆå¹¶æ•°æ®çš„åˆ—åå®Œå…¨åŒ¹é…
            logging.debug(f"ğŸ” æ‰€æœ‰å› å­åç§°: {FACTOR_NAMES}")
            logging.debug(f"ğŸ” åˆå¹¶æ•°æ®åˆ—: {list(combined_factors_df.columns)}")
            logging.debug(f"ğŸ” æ”¶ç›Šç‡æ•°æ®åˆ—: {list(future_returns_df.columns)}")
            
            # éªŒè¯å› å­åˆ—å­˜åœ¨æ€§
            missing_factor_cols = [f for f in FACTOR_NAMES if f not in combined_factors_df.columns]
            if missing_factor_cols:
                logging.error(f"âŒ é”™è¯¯: ä»¥ä¸‹å› å­åœ¨åˆå¹¶æ•°æ®ä¸­ç¼ºå¤±: {missing_factor_cols}")
                logging.error(f"âŒ å¯ç”¨åˆ—åŒ…æ‹¬: {list(combined_factors_df.columns)}")
                raise ValueError(f"å› å­åˆ—ç¼ºå¤±: {missing_factor_cols}")
            
            roller = STRATEGY_CONFIG.create_rolling_calculator(
                forward_return_periods=FORWARD_RETURN_PERIODS,
                factor_names=FACTOR_NAMES)

            if roller is None:
                logging.error("âŒ æ»šåŠ¨è®¡ç®—å™¨åˆ›å»ºå¤±è´¥ã€‚")
                sys.exit(1)

            logging.info("âš™ï¸ æ­¥éª¤ 3c: å‡†å¤‡æ»šåŠ¨æ•°æ®...")
            
            # ã€è°ƒè¯•ã€‘æ£€æŸ¥åˆå¹¶å‰çš„æ•°æ®
            logging.debug(f"ğŸ” [æ»šåŠ¨æ•°æ®] combined_factors_df å½¢çŠ¶: {combined_factors_df.shape}")
            logging.debug(f"ğŸ” [æ»šåŠ¨æ•°æ®] combined_factors_df ç´¢å¼•: {combined_factors_df.index.names}")
            logging.debug(f"ğŸ” [æ»šåŠ¨æ•°æ®] combined_factors_df åˆ—: {list(combined_factors_df.columns)}")
            logging.debug(f"ğŸ” [æ»šåŠ¨æ•°æ®] combined_factors_df æ•°æ®ç±»å‹:\n{combined_factors_df.dtypes}")
            logging.debug(f"ğŸ” [æ»šåŠ¨æ•°æ®] combined_factors_df æ ·ä¾‹:\n{combined_factors_df.head()}")
            logging.debug(f"ğŸ” [æ»šåŠ¨æ•°æ®] combined_factors_df ç»Ÿè®¡:\n{combined_factors_df.describe()}")
            
            logging.debug(f"ğŸ” [æ»šåŠ¨æ•°æ®] future_returns_df å½¢çŠ¶: {future_returns_df.shape}")
            logging.debug(f"ğŸ” [æ»šåŠ¨æ•°æ®] future_returns_df ç´¢å¼•: {future_returns_df.index.names}")
            logging.debug(f"ğŸ” [æ»šåŠ¨æ•°æ®] future_returns_df åˆ—: {list(future_returns_df.columns)}")
            logging.debug(f"ğŸ” [æ»šåŠ¨æ•°æ®] future_returns_df æ ·ä¾‹:\n{future_returns_df.head()}")
            
            # åˆå¹¶å› å­å€¼å’Œæœªæ¥æ”¶ç›Š (ç”¨äºè®¡ç®— IC/IR ç­‰)
            combined_reset = combined_factors_df.reset_index()
            returns_reset = future_returns_df.reset_index()
            
            logging.debug(f"ğŸ” [æ»šåŠ¨æ•°æ®] é‡ç½®ç´¢å¼•å - combined: {combined_reset.shape}, returns: {returns_reset.shape}")
            logging.debug(f"ğŸ” [æ»šåŠ¨æ•°æ®] åˆå¹¶é”®: date, asset")
            
            all_data_merged = pd.merge(combined_reset,
                                       returns_reset,
                                       on=['date', 'asset'],
                                       how='inner')
            
            logging.debug(f"ğŸ” [æ»šåŠ¨æ•°æ®] åˆå¹¶åæœ€ç»ˆæ•°æ®å½¢çŠ¶: {all_data_merged.shape}")
            
            if all_data_merged.empty:
                logging.error("âŒ [æ»šåŠ¨æ•°æ®] åˆå¹¶åæ•°æ®ä¸ºç©ºï¼")
                logging.error(f"âŒ [æ»šåŠ¨æ•°æ®] combined_reset åˆ—: {list(combined_reset.columns)}")
                logging.error(f"âŒ [æ»šåŠ¨æ•°æ®] returns_reset åˆ—: {list(returns_reset.columns)}")
                # å°è¯•å¤–è¿æ¥æŸ¥çœ‹é—®é¢˜
                all_data_merged_debug = pd.merge(combined_reset, returns_reset, on=['date', 'asset'], how='outer', indicator=True)
                logging.error(f"âŒ [æ»šåŠ¨æ•°æ®] å¤–è¿æ¥ç»“æœ:\n{all_data_merged_debug['_merge'].value_counts()}")
                raise ValueError("æ»šåŠ¨æ•°æ®åˆå¹¶å¤±è´¥")
                
            all_data_merged = all_data_merged.set_index(['date', 'asset']).sort_index()
            
            logging.debug(f"ğŸ” [æ»šåŠ¨æ•°æ®] æœ€ç»ˆç´¢å¼•: {all_data_merged.index.names}")
            logging.debug(f"ğŸ” [æ»šåŠ¨æ•°æ®] æœ€ç»ˆåˆ—: {list(all_data_merged.columns)}")
            logging.debug(f"ğŸ” [æ»šåŠ¨æ•°æ®] å› å­åˆ—ç»Ÿè®¡:\n{all_data_merged[FACTOR_NAMES].describe()}")

            composite_factor_series = roller.calculate_composite_factor(
                all_data_merged)
            composite_factor_series.name = 'factor_value'
            final_factor_name = f"Composite_{STRATEGY_NAME}_Rolling"

        if composite_factor_series is not None:
            combined_factors_df = composite_factor_series.to_frame()
            # ã€ç®€åŒ–ä¿®å¤ã€‘åˆæˆåä¹Ÿæ£€æŸ¥ç´¢å¼•
            if combined_factors_df.index.nlevels > 2:
                logging.warning(f"âš ï¸ åˆæˆå› å­ç´¢å¼•å¼‚å¸¸: {combined_factors_df.index.names}ï¼Œæ­£åœ¨ä¿®å¤...")
                # ä¿ç•™æœ€å2ä¸ªå±‚çº§ï¼ˆé€šå¸¸æ˜¯ date, assetï¼‰
                while combined_factors_df.index.nlevels > 2:
                    combined_factors_df.index = combined_factors_df.index.droplevel(0)
                combined_factors_df.index.names = ['date', 'asset']
                logging.info(f"âœ… åˆæˆåç´¢å¼•å·²ä¿®å¤: {combined_factors_df.index.names}")
        else:
            combined_factors_df = pd.DataFrame()

    # =====================
    # 4. ç”ŸæˆæŠ¥å‘Š
    # =====================
    if not combined_factors_df.empty:
        # ã€è°ƒè¯•æ—¥å¿—ã€‘åœ¨åˆå¹¶å‰æ£€æŸ¥æ•°æ®ç»“æ„
        logging.debug(f"\nğŸ” [è°ƒè¯•] combined_factors_df ç´¢å¼•: {combined_factors_df.index.names}")
        logging.debug(f"ğŸ” [è°ƒè¯•] combined_factors_df åˆ—: {list(combined_factors_df.columns)}")
        logging.debug(f"ğŸ” [è°ƒè¯•] combined_factors_df å½¢çŠ¶: {combined_factors_df.shape}")
        logging.debug(f"ğŸ” [è°ƒè¯•] combined_factors_df head:\n{combined_factors_df.head()}")
        
        logging.debug(f"\nğŸ” [è°ƒè¯•] future_returns_df ç´¢å¼•: {future_returns_df.index.names}")
        logging.debug(f"ğŸ” [è°ƒè¯•] future_returns_df åˆ—: {list(future_returns_df.columns)}")
        logging.debug(f"ğŸ” [è°ƒè¯•] future_returns_df å½¢çŠ¶: {future_returns_df.shape}")
        
        # åˆå¹¶æ”¶ç›Šç‡ç”¨äºæœ€ç»ˆæŠ¥å‘Š
        try:
            final_factor_data_df = pd.merge(combined_factors_df.reset_index(),
                                            future_returns_df.reset_index(),
                                            on=['date', 'asset'],
                                            how='inner')
        except Exception as e:
            logging.error(f"âŒ åˆå¹¶æ•°æ®æ—¶å‡ºé”™: {e}")
            logging.error(f"âŒ combined_factors_df ç´¢å¼•å±‚çº§: {combined_factors_df.index.nlevels}")
            logging.error(f"âŒ future_returns_df ç´¢å¼•å±‚çº§: {future_returns_df.index.nlevels}")
            raise
        final_factor_data_df.rename(
            columns={'factor_value': final_factor_name}, inplace=True)
        final_factor_data_df.set_index('date', inplace=True)

        logging.info(
            f"\n{'='*60}\n--- æ­¥éª¤ 4: ç”ŸæˆæŠ¥å‘Š ({final_factor_name}) ---\n{'='*60}")

        final_report_df = final_factor_data_df.rename(
            columns={final_factor_name: 'factor_value'})
        final_report_df.dropna(subset=['factor_value'], inplace=True)

        if not final_report_df.empty:
            report_generator = FactorReport(
                factor_name=final_factor_name,
                factor_data=final_report_df,
                forward_return_periods=FORWARD_RETURN_PERIODS,
                benchmark_data=benchmark_df
                if benchmark_df is not None else pd.DataFrame())

            std_name = STANDARDIZER.__class__.__name__

            if "Standardizer" in std_name:
                std_name = std_name.replace("Standardizer", "")

            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            output_filename = os.path.join(
                OUTPUT_DIR,
                f"report_{final_factor_name}_{std_name}_{timestamp}.html")

            logging.info(f"âš™ï¸ ç”Ÿæˆ HTML æŠ¥å‘Š: {output_filename}")
            report_generator.generate_html_report(output_filename)
        else:
            logging.warning("âš ï¸ æœ€ç»ˆå› å­æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Šã€‚")

    logging.info(f"\n{'='*60}\nğŸ åˆ†ææµç¨‹æ‰§è¡Œå®Œæ¯• ğŸ\n{'='*60}")
