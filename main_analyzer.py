# main_analyzer.py (å·²é‡æ„ - ä¿®æ­£å¤šè¿›ç¨‹è°ƒç”¨)

import os
import logging
import pandas as pd
from tqdm import tqdm
import sys

# ==============================================================================
# 1. ç­–ç•¥åˆ†æâ€œæ§åˆ¶é¢æ¿â€ (Strategy Analysis "Control Panel")
# ==============================================================================
#
#   æ¬¢è¿ä½¿ç”¨ï¼
#   æ‚¨å‡ ä¹æ‰€æœ‰çš„ã€é«˜é¢‘ã€‘è‡ªå®šä¹‰é…ç½®éƒ½å¯ä»¥åœ¨è¿™ä¸ªç‰ˆå—å®Œæˆã€‚
#   (å¯¹äºã€ä½é¢‘ã€‘çš„ç­–ç•¥å®šä¹‰ï¼Œè¯·è½¬åˆ° `strategy_configs.py` æ–‡ä»¶)
#
# ==============================================================================

# --- 1a. æ ¸å¿ƒç­–ç•¥é€‰æ‹© (Core Strategy Selection) ---
#
#   è¿™æ˜¯ã€æœ€é‡è¦ã€‘çš„ç­–ç•¥é€‰æ‹©ç‚¹ã€‚æ‰€æœ‰å¤æ‚çš„é…ç½® (æ»šåŠ¨å‘¨æœŸã€å›ºå®šæƒé‡ç­‰)
#   éƒ½å·²å°è£…åœ¨ `strategy_configs.py` æ–‡ä»¶ä¸­ã€‚
#
from strategy_configs import STRATEGY_REGISTRY

# ã€ã€è¯·åœ¨è¿™é‡Œé€‰æ‹©æ‚¨çš„ç­–ç•¥åç§° (ä» strategy_configs.py å¤åˆ¶)ã€‘ã€‘
# STRATEGY_NAME = "RollingICIR"
STRATEGY_NAME = "RollingRegression"
# STRATEGY_NAME = "FixedWeights"
# STRATEGY_NAME = "EqualWeights"
# STRATEGY_NAME = "DynamicSignificance"

# --- (è‡ªåŠ¨åŠ è½½é…ç½®) ---
if STRATEGY_NAME not in STRATEGY_REGISTRY:
    # (è¿™ä¸ªæ—¥å¿—ä¼šåœ¨ setup_logging ä¹‹å‰ï¼Œå¯èƒ½æ— æ³•è¢«æ•è·ï¼Œä½† raise ä¼šç»ˆæ­¢ç¨‹åº)
    raise ValueError(f"ç­–ç•¥ '{STRATEGY_NAME}' æœªåœ¨ strategy_configs.py ä¸­æ³¨å†Œã€‚")
# è‡ªåŠ¨åŠ è½½æ‰€é€‰ç­–ç•¥çš„å®Œæ•´é…ç½®å¯¹è±¡
STRATEGY_CONFIG = STRATEGY_REGISTRY[STRATEGY_NAME]

# --- 1b. å› å­é€‰æ‹© (Factors to Analyze) ---
#
#   ã€ã€é‡è¦é…ç½®ã€‘ã€‘
#   æ‚¨å¸Œæœ›åœ¨æœ¬æ¬¡åˆ†æä¸­è¿è¡Œå“ªäº›ã€åŸºç¡€å› å­ (Type 1)ã€‘ï¼Ÿ
#
FACTORS_TO_ANALYZE = [
    # ('RSI', {
    #     'rsi_period': 22
    # }),
    # ('BollingerBands', {
    #     'period': 30
    # }),
    # ('ADXDMI', {
    #     'period': 14,
    #     'trend_threshold': 22
    # }),
    # ('Momentum', {
    #     'period': 20
    # }),
    ('Reversal20D', {
        'period': 40,
        'decay': 20
    }),
]

# --- 1c. å¤åˆå› å­é€‰æ‹© (Complex Factors) ---
#
#   ã€ã€é«˜çº§é…ç½®ã€‘ã€‘
#   æ‚¨å¸Œæœ›åœ¨ "Type 1" åŸºç¡€å› å­è®¡ç®—ã€ä¹‹åã€‘è¿è¡Œå“ªäº› "Type 2" å¤åˆå› å­ï¼Ÿ
#
from factor_analysis.factors_complex import COMPLEX_FACTOR_REGISTRY

COMPLEX_FACTORS_TO_RUN = [
    "IndNeu_Momentum",
    # "MktNeu_RSI", # ç¤ºä¾‹: å¦‚æœæ‚¨åœ¨ `factors_complex.py` ä¸­å®šä¹‰äº†å®ƒ
]

# --- 1d. æˆªé¢æ•°æ®é…ç½® (Cross-Sectional Data) ---
#
#   ã€ã€é«˜çº§é…ç½®ã€‘ã€‘
#   æ‚¨æ˜¯å¦éœ€è¦åŠ è½½ã€é¢å¤–ã€‘çš„æˆªé¢æ•°æ® (ä¾‹å¦‚ è¡Œä¸šã€å¸‚å€¼)ï¼Ÿ
#
LOAD_INDUSTRY_DATA = True
# (å¦‚æœä¸éœ€è¦è¡Œä¸šæ•°æ®ï¼Œè¯·æ”¹ä¸º False)

# --- 1e. æ ‡å‡†åŒ–å™¨ (Standardizer) ---
#
#   ã€ã€é‡è¦é…ç½®ã€‘ã€‘
#   æ‚¨å¸Œæœ›å¦‚ä½•å¯¹å› å­å€¼è¿›è¡Œã€æˆªé¢æ ‡å‡†åŒ–ã€‘ï¼Ÿ
#
from core.factor_standardizer import (CrossSectionalZScoreStandardizer,
                                      NoStandardizer,
                                      CrossSectionalQuantileStandardizer)
# ã€ã€è¯·åœ¨è¿™é‡Œä¸‰é€‰ä¸€ã€‘ã€‘
STANDARDIZER_CLASS = CrossSectionalZScoreStandardizer
# STANDARDIZER_CLASS = CrossSectionalQuantileStandardizer
# STANDARDIZER_CLASS = NoStandardizer

# ==============================================================================
# 2. åŸºç¡€å›æµ‹ä¸è·¯å¾„é…ç½® (Basic Backtest & Path Settings)
# ==============================================================================
#
# ã€ã€ã€é‡è¦ã€‘ã€‘ã€‘æ•°æ®ä¸‹è½½ä¸å†™å…¥å¼€å…³ï¼š
#
#   å½“æ‚¨åªæ˜¯ä¿®æ”¹å› å­è®¡ç®— (factors.py) æˆ–åˆæˆé€»è¾‘ (factor_combiner.py)ï¼Œ
#   è€Œè‚¡ç¥¨æ± å’Œæ—¶é—´èŒƒå›´ä¸å˜æ—¶ï¼Œè¯·å°†æ­¤é¡¹è®¾ä¸º Trueã€‚
#   è¿™å°†è·³è¿‡è€—æ—¶çš„æ•°æ®æ£€æŸ¥å’Œä¸‹è½½æµç¨‹ï¼Œç›´æ¥ä½¿ç”¨æ•°æ®åº“ä¸­çš„ç°æœ‰æ•°æ®ã€‚
#
SKIP_DATA_PREPARATION = True
# SKIP_DATA_PREPARATION = False # (æ­£å¸¸è¿è¡Œæ—¶è®¾ä¸º False)
#
# ==============================================================================

# --- 2a. å›æµ‹æ—¶é—´ä¸æ”¶ç›Šå‘¨æœŸ ---
START_DATE = '2023-01-01'
END_DATE = '2024-12-31'
FORWARD_RETURN_PERIODS = [1, 5, 10, 30, 60]  # å¿…é¡»åŒ…å« 1d ä¸­é…ç½®çš„æ‰€æœ‰å‘¨æœŸ

# --- 2b. åŸºå‡†ä¸è‚¡ç¥¨æ±  ---
BENCHMARK = '600519'  # ç”¨äºæŠ¥å‘Šå¯¹æ¯”
from universe_config import UNIVERSE  # å¯¼å…¥æ‚¨çš„è‚¡ç¥¨æ± 

# ã€ã€ã€æ–°å¢ã€‘ã€‘ã€‘: Type 1 å› å­è®¡ç®—ã€è¿›ç¨‹æ•°ã€‘
# (è¿™åœ¨ factor_calculator.py ä¸­è¢«ç”¨ä½œ max_workers)
FACTOR_CALC_PROCESSES = 16  # (æ ¹æ®æ‚¨çš„ CPU æ ¸å¿ƒæ•°è°ƒæ•´ï¼Œä¾‹å¦‚ 8, 16)

# --- 2c. è·¯å¾„é…ç½® ---
LOG_DIR = "logs"
OUTPUT_DIR = "factor_reports"
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

BACKTEST_DB_PATH = './database/quant_data.db'

# --- 2d. æ•°æ®æºé…ç½® ---
from data.data_providers import SQLiteDataProvider

DATA_PROVIDERS_CONFIG = [
    (SQLiteDataProvider, {
        'db_path': './database/JY_database/sqlite/JY_database.sqlite',
        'table_name': 'JY_t_price_daily'
    }),
]

# ==============================================================================
#
#                     --- æ ¸å¿ƒç¨‹åºå¼€å§‹ (Core Logic Starts) ---
#                     --- (!!!) é€šå¸¸ä½ ä¸éœ€è¦ä¿®æ”¹ä»¥ä¸‹å†…å®¹ (!!!) ---
#
# ==============================================================================

# å¯¼å…¥æ‰€æœ‰å¿…éœ€çš„åˆ†ææ¨¡å—
from data.data_manager import DataProviderManager
from factor_analysis.factor_calculator import FactorCalculator
from factor_analysis.factor_report import FactorReport
from logger.logger_config import setup_logging

if __name__ == '__main__':

    # =====================
    # 0. åˆå§‹åŒ–ä¸é…ç½®æ ¡éªŒ
    # =====================
    # (setup_logging å¿…é¡»åœ¨æœ€å¼€å§‹è°ƒç”¨)
    setup_logging(log_dir=LOG_DIR, log_prefix='factor_analysis')

    logging.info(f"\n{'='*60}\n--- æ­¥éª¤ 0: åˆå§‹åŒ–ä¸é…ç½®æ ¡éªŒ ---\n{'='*60}")
    logging.info("ğŸ å› å­åˆ†æç¨‹åºå¯åŠ¨...")

    # 1. æå–å› å­åç§°åˆ—è¡¨ (æ¥è‡ª 1b)
    FACTOR_NAMES = [f[0] for f in FACTORS_TO_ANALYZE]

    # 2. å®ä¾‹åŒ–æ ‡å‡†åŒ–å™¨ (æ¥è‡ª 1e)
    STANDARDIZER = STANDARDIZER_CLASS()
    logging.info(f"âœ… æ ‡å‡†åŒ–å™¨å·²åŠ è½½: {STANDARDIZER.__class__.__name__}")

    # 3. ã€ã€é‡æ„åçš„åˆå§‹åŒ–é€»è¾‘ã€‘ã€‘ (æ¥è‡ª 1a å’Œ strategy_configs.py)
    logging.info(f"âš™ï¸ æ­£åœ¨åŠ è½½ç­–ç•¥: {STRATEGY_NAME}")

    # 3a. å®ä¾‹åŒ–åˆæˆå™¨ (Combiner)
    COMBINER = STRATEGY_CONFIG.create_combiner()
    logging.info(f"âœ… åˆæˆå™¨å·²åŠ è½½: {STRATEGY_CONFIG.combiner_class.__name__}")

    # 3b. æ£€æŸ¥æ»šåŠ¨é€»è¾‘ (Rolling)
    _run_rolling = STRATEGY_CONFIG.is_rolling()
    logging.info(f"â„¹ï¸ è‡ªåŠ¨æ£€æµ‹æ»šåŠ¨é€»è¾‘: {_run_rolling}")

    if _run_rolling:
        logging.info(f"  > æ»šåŠ¨é…ç½®: {STRATEGY_CONFIG.rolling_config}")
    else:
        logging.info(f"  > æ¨¡å¼: é™æ€ (éæ»šåŠ¨)")
    logging.info("âœ… ç­–ç•¥é…ç½®åŠ è½½å®Œæ¯•ã€‚")
    # --- ã€ã€åˆå§‹åŒ–é€»è¾‘ç»“æŸã€‘ã€‘ ---

    # =====================
    # 1. åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨å¹¶å‡†å¤‡æ•°æ®
    # =====================
    logging.info(f"\n{'='*60}\n--- æ­¥éª¤ 1: å‡†å¤‡æ•°æ® ---\n{'='*60}")

    # 1. åˆå§‹åŒ– DataProviderManager
    #    (è¿™ä¸€æ­¥ã€å¿…é¡»ã€‘æ‰§è¡Œï¼Œä»¥ä¾¿åç»­æ­¥éª¤å¯ä»¥ä»æ•°æ®åº“è¯»å–æ•°æ®)
    logging.info("âš™ï¸ æ­£åœ¨åˆå§‹åŒ– DataProviderManager...")
    data_manager = DataProviderManager(
        provider_configs=DATA_PROVIDERS_CONFIG,  # <- ã€ã€é‡è¦ã€‘ã€‘
        symbols=UNIVERSE,
        start_date=START_DATE,
        end_date=END_DATE,
        db_path=BACKTEST_DB_PATH,  # <- ã€ã€é‡è¦ã€‘ã€‘
        num_checker_threads=16,
        num_downloader_threads=16,
        batch_size=200)

    # 2. å°† BENCHMARK (åŸºå‡†) æ·»åŠ åˆ° "å¾…ä¸‹è½½" åˆ—è¡¨ä¸­
    if BENCHMARK not in data_manager.symbols:
        data_manager.symbols.append(BENCHMARK)
        logging.info(f"  > å·²å°†åŸºå‡† {BENCHMARK} æ·»åŠ åˆ°æ•°æ®ç®¡ç†å™¨ä»»åŠ¡åˆ—è¡¨ã€‚")

    # 3. ã€ã€ã€æ ¸å¿ƒä¿®æ”¹ï¼šè·³è¿‡æ•°æ®å‡†å¤‡ã€‘ã€‘ã€‘
    if not SKIP_DATA_PREPARATION:
        logging.info("âš™ï¸ æ¨¡å¼: å®Œæ•´æ•°æ®å‡†å¤‡ (æ£€æŸ¥ã€ä¸‹è½½ã€å†™å…¥)...")
        # 3. æ‰§è¡Œã€å®Œæ•´ã€‘æ•°æ®å‡†å¤‡ (ETL æµç¨‹)
        data_manager.prepare_data_for_universe()
        logging.info("âœ… å®Œæ•´æ•°æ®å‡†å¤‡æµç¨‹ (ETL) å·²å®Œæˆã€‚")
    else:
        logging.info(
            "ğŸŸ¡ ã€ã€è·³è¿‡ã€‘ã€‘: å·²æŒ‰é…ç½® (SKIP_DATA_PREPARATION=True) è·³è¿‡æ•°æ®æ£€æŸ¥ä¸ä¸‹è½½æµç¨‹ã€‚")
        logging.info("â„¹ï¸ æ¨¡å¼: ç›´æ¥ä½¿ç”¨æ•°æ®åº“ç°æœ‰æ•°æ®...")

    # 4. è·å–åŸºå‡†æ•°æ®ï¼Œç”¨äºæŠ¥å‘Šå¯¹æ¯”
    logging.info(f"âš™ï¸ æ­£åœ¨è·å–åŸºå‡† '{BENCHMARK}' æ•°æ®ç”¨äºæŠ¥å‘Šå¯¹æ¯”...")
    benchmark_df = data_manager.get_dataframe(BENCHMARK)
    if benchmark_df is None or benchmark_df.empty:
        logging.warning(f"âš ï¸ è­¦å‘Š: æœªèƒ½è·å–åˆ°åŸºå‡† '{BENCHMARK}' çš„æ•°æ®ã€‚æŠ¥å‘Šä¸­å°†ä¸åŒ…å«åŸºå‡†å¯¹æ¯”ã€‚")
    else:
        logging.info(f"âœ… æˆåŠŸè·å–åŸºå‡† '{BENCHMARK}' æ•°æ®ã€‚")

    # =====================
    # 2. è®¡ç®—æ‰€æœ‰å› å­çš„åŸå§‹å€¼å’Œæœªæ¥æ”¶ç›Š
    # =====================
    all_factors_dfs = {}  # <-- å°†å­˜å‚¨ã€æ‰€æœ‰ã€‘(Type 1 å’Œ 2) çš„å› å­ Series
    future_returns_df = None

    logging.info(
        f"\n{'='*60}\n--- æ­¥éª¤ 2: è®¡ç®—æ‰€æœ‰æŒ‡å®šå› å­çš„åŸå§‹å€¼ (Type 1 å› å­) ---\n{'='*60}")

    # 1. å‡†å¤‡è‚¡ç¥¨æ± 
    active_universe = data_manager.symbols.copy()
    if BENCHMARK in active_universe:
        active_universe.remove(BENCHMARK)
        logging.info(f"  > å·²ä»å› å­è®¡ç®—æ± ä¸­ç§»é™¤åŸºå‡† {BENCHMARK}ã€‚")

    # 2. ã€ã€ã€æ­¥éª¤ 2a: è®¡ç®—åŸºç¡€å› å­ (Type 1)ã€‘ã€‘ã€‘
    if not FACTORS_TO_ANALYZE:
        logging.info("â„¹ï¸ (è·³è¿‡: æœªåœ¨ 1b ä¸­é…ç½®åŸºç¡€å› å­)")
    else:
        for factor_name, factor_params in FACTORS_TO_ANALYZE:
            logging.info(
                f"âš™ï¸ æ­£åœ¨å¯åŠ¨ (Type 1) è®¡ç®—å™¨: {factor_name} (å‚æ•°: {factor_params})..."
            )

            # ã€ã€ã€ã€ã€ã€ æ ¸å¿ƒä¿®æ­£ 1 ã€‘ã€‘ã€‘ã€‘ã€‘ã€‘
            # å°† data_manager å®ä¾‹æ›¿æ¢ä¸ºå®ƒæ‰€åŒ…å«çš„é…ç½®
            # ä»¥åŒ¹é… factor_calculator.py çš„æ–° __init__ ç­¾å
            calculator = FactorCalculator(
                provider_configs=data_manager.provider_configs,  # <- ã€ã€ä¿®æ­£ã€‘ã€‘
                db_path=BACKTEST_DB_PATH,  # <- ã€ã€ä¿®æ­£ã€‘ã€‘
                universe=active_universe,
                start_date=START_DATE,
                end_date=END_DATE,
                factor_name=factor_name,
                factor_params=factor_params,
                forward_return_periods=FORWARD_RETURN_PERIODS,
                num_threads=FACTOR_CALC_PROCESSES  # (è¿™ä¸ªå‚æ•°ååœ¨å†…éƒ¨è¢«æ˜ å°„ä¸ºè¿›ç¨‹æ•°)
            )
            # ã€ã€ã€ã€ã€ã€ ä¿®æ­£ç»“æŸ ã€‘ã€‘ã€‘ã€‘ã€‘ã€‘

            factor_data_df = calculator.calculate_factor_and_returns()

            if factor_data_df.empty:
                logging.warning(f"âŒ è­¦å‘Š: æœªèƒ½ä¸ºå› å­ {factor_name} ç”Ÿæˆæœ‰æ•ˆæ•°æ®ï¼Œå·²è·³è¿‡ã€‚")
                continue

            # ã€é‡è¦ã€‘å°†å› å­ Series (MultiIndex) å­˜å…¥ all_factors_dfs
            factor_series = factor_data_df.set_index(
                'asset', append=True)['factor_value']
            factor_series.name = factor_name
            all_factors_dfs[factor_name] = factor_series.sort_index()
            logging.info(f"âœ… æˆåŠŸè®¡ç®—å¹¶å­˜å‚¨ (Type 1) å› å­: {factor_name}")

            if future_returns_df is None:
                logging.info("  > æ­£åœ¨ç¼“å­˜æœªæ¥æ”¶ç›Šæ•°æ®...")
                return_cols = ['asset'] + [
                    f'forward_return_{p}d' for p in FORWARD_RETURN_PERIODS
                ]
                future_returns_df = factor_data_df[return_cols].reset_index()

    # 3. ã€ã€ã€æ­¥éª¤ 2.5: è®¡ç®—å¤åˆå› å­ (Type 2)ã€‘ã€‘ã€‘
    logging.info(
        f"\n{'='*60}\n--- æ­¥éª¤ 2.5: è®¡ç®—æ‰€æœ‰æŒ‡å®šå› å­çš„å¤åˆå€¼ (Type 2 å› å­) ---\n{'='*60}")
    if not COMPLEX_FACTORS_TO_RUN:
        logging.info("â„¹ï¸ (è·³è¿‡: æœªåœ¨ 1c ä¸­é…ç½®å¤åˆå› å­)")
    else:
        logging.info("âš™ï¸ æ­¥éª¤ 2.5a: åŠ è½½ã€å…¨éƒ¨ã€‘è‚¡ç¥¨çš„æ—¥çº¿æ•°æ® (ç”¨äºå¤åˆè®¡ç®—)...")
        # (è¿›åº¦æ¡åœ¨ data_manager.get_all_data_for_universe å†…éƒ¨)
        all_data_df = data_manager.get_all_data_for_universe(active_universe)

        if all_data_df is None:
            logging.error("âŒ é”™è¯¯: æ— æ³•åŠ è½½å¤åˆå› å­æ‰€éœ€çš„åŸºç¡€æ•°æ®ï¼Œå·²è·³è¿‡ã€‚")
        else:
            # æ­¥éª¤ 2.5b: (å¯é€‰) åŠ è½½å¹¶åˆå¹¶æˆªé¢æ•°æ®
            if LOAD_INDUSTRY_DATA:
                logging.info("âš™ï¸ æ­¥éª¤ 2.5b: (æŒ‰é…ç½®) æ­£åœ¨åŠ è½½å¹¶åˆå¹¶è¡Œä¸šæ•°æ® ('stock_kind')...")
                try:
                    industry_df = data_manager.get_industry_mapping()
                    if industry_df is not None:
                        # å°† (asset, industry) åˆå¹¶åˆ° ('date', 'asset') çš„ä¸»æ•°æ®ä¸­
                        all_data_df = all_data_df.reset_index().merge(
                            industry_df, on='asset',
                            how='left').set_index(['date',
                                                   'asset']).sort_index()
                        logging.info("  > âœ… è¡Œä¸šæ•°æ®åˆå¹¶å®Œæˆã€‚")
                    else:
                        logging.warning("  > âš ï¸ è­¦å‘Š: æœªèƒ½ä» 'stock_kind' åŠ è½½è¡Œä¸šæ•°æ®ã€‚")
                except AttributeError:
                    logging.error(
                        "  > âŒ é”™è¯¯: 'get_industry_mapping' å‡½æ•°æœªåœ¨ DataProviderManager ä¸­å®šä¹‰ã€‚"
                    )

            # (æœªæ¥å¯ä»¥åœ¨æ­¤åˆå¹¶ 'market_cap' ç­‰)

            # æ­¥éª¤ 2.5c: å¾ªç¯è®¡ç®—
            logging.info(
                f"âš™ï¸ æ­¥éª¤ 2.5c: å¼€å§‹è®¡ç®— {len(COMPLEX_FACTORS_TO_RUN)} ä¸ªå¤åˆå› å­...")

            # (ä½¿ç”¨ tqdm åŒ…è£¹å¾ªç¯)
            tqdm_loop = tqdm(
                COMPLEX_FACTORS_TO_RUN,
                desc="[ä¸»å¾ªç¯] è®¡ç®—å¤åˆå› å­",
                ncols=100,
                leave=False,
                file=sys.stdout  # (ä¿æŒä¸ logger_config.py ä¸€è‡´)
            )

            for factor_key in tqdm_loop:
                tqdm_loop.set_description(f"[ä¸»å¾ªç¯] å¤åˆå› å­ ({factor_key})")

                if factor_key in COMPLEX_FACTOR_REGISTRY:
                    calc_func = COMPLEX_FACTOR_REGISTRY[factor_key]

                    # ã€æ ¸å¿ƒã€‘è°ƒç”¨å¤åˆè®¡ç®—å‡½æ•°
                    complex_factor_series = calc_func(all_data_df)

                    if complex_factor_series is not None:
                        logging.debug(f"    > âœ… æˆåŠŸè®¡ç®— (Type 2): {factor_key}")
                        all_factors_dfs[
                            factor_key] = complex_factor_series.sort_index()
                else:
                    logging.warning(f"  > âŒ è­¦å‘Š: å¤åˆå› å­ '{factor_key}' åœ¨ "
                                    f"factors_complex.py ä¸­æœªæ³¨å†Œï¼Œå·²è·³è¿‡ã€‚")

            logging.info("âœ… æ‰€æœ‰å¤åˆå› å­è®¡ç®—å®Œæ¯•ã€‚")

    # 4. ã€ã€ã€æ­¥éª¤ 2.6: æ£€æŸ¥æœªæ¥æ”¶ç›Š (å¤„ç†è¾¹ç•Œæƒ…å†µ)ã€‘ã€‘ã€‘
    if future_returns_df is None:
        if not all_factors_dfs:
            logging.critical("â›” é”™è¯¯: æœªè®¡ç®—å‡ºä»»ä½• (Type 1 æˆ– Type 2) å› å­æ•°æ®ã€‚ç¨‹åºç»ˆæ­¢ã€‚")
            exit()
        else:
            logging.warning("âš ï¸ è­¦å‘Š: æœªè®¡ç®—æœªæ¥æ”¶ç›Š (å› ä¸º Type 1 å› å­è¢«è·³è¿‡)ã€‚")
            logging.info("  > âš™ï¸ æ­£åœ¨ã€é‡æ–°ã€‘è¿è¡Œä¸€ä¸ªåŸºç¡€è®¡ç®—å™¨ä»¥è·å–æœªæ¥æ”¶ç›Š...")

            # ã€ã€ã€ã€ã€ã€ æ ¸å¿ƒä¿®æ­£ 2 ã€‘ã€‘ã€‘ã€‘ã€‘ã€‘
            temp_calc = FactorCalculator(
                provider_configs=data_manager.provider_configs,  # <- ã€ã€ä¿®æ­£ã€‘ã€‘
                db_path=BACKTEST_DB_PATH,  # <- ã€ã€ä¿®æ­£ã€‘ã€‘
                universe=active_universe,
                start_date=START_DATE,
                end_date=END_DATE,
                factor_name='RSI',
                factor_params={'rsi_period': 14},
                forward_return_periods=FORWARD_RETURN_PERIODS,
                num_threads=FACTOR_CALC_PROCESSES)
            # ã€ã€ã€ã€ã€ã€ ä¿®æ­£ç»“æŸ ã€‘ã€‘ã€‘ã€‘ã€‘ã€‘

            logging.info("  > âš™ï¸ æ­£åœ¨ä¸ºæ‰€æœ‰è‚¡ç¥¨è®¡ç®—æœªæ¥æ”¶ç›Š...")
            all_data_df_with_returns = temp_calc.calculate_factor_and_returns(
                run_factor_calc=False  # (ä»…è®¡ç®—æ”¶ç›Š)
            )

            if all_data_df_with_returns.empty:
                logging.critical("  > âŒ è‡´å‘½é”™è¯¯: æ— æ³•è¡¥ç®—æœªæ¥æ”¶ç›Šã€‚ç¨‹åºç»ˆæ­¢ã€‚")
                exit()

            return_cols = ['asset'] + [
                f'forward_return_{p}d' for p in FORWARD_RETURN_PERIODS
            ]
            future_returns_df = all_data_df_with_returns[
                return_cols].reset_index()
            logging.info("  > âœ… æœªæ¥æ”¶ç›Šå·²è¡¥ç®—ã€‚")

    # =====================
    # 3. æ ¸å¿ƒåˆ†ææµç¨‹ï¼šå•å› å­ vs å¤šå› å­ (é™æ€/æ»šåŠ¨)
    # =====================
    final_factor_data_df = pd.DataFrame()
    final_factor_name = ""

    # 1. åŠ¨æ€ç”Ÿæˆæœ€ç»ˆçš„ FACTOR_NAMES åˆ—è¡¨
    FACTOR_NAMES = list(all_factors_dfs.keys())
    logging.info(f"\n{'='*60}\n--- æ­¥éª¤ 3: å› å­åˆå¹¶ä¸åˆ†æ ---\n{'='*60}")
    logging.info(f"â„¹ï¸ å³å°†åˆå¹¶çš„ã€æ‰€æœ‰ã€‘å› å­: {FACTOR_NAMES}")

    if len(FACTOR_NAMES) > 1:
        # --- å¤šå› å­åˆæˆè·¯å¾„ ---
        logging.info("âš™ï¸ æ­¥éª¤ 3a: åˆå¹¶æ‰€æœ‰ (Type 1 å’Œ Type 2) å› å­æ•°æ®...")

        all_factors_df_list = []
        for factor_name, factor_series in all_factors_dfs.items():
            df = factor_series.to_frame().reset_index()
            all_factors_df_list.append(df)

        combined_factors_df = all_factors_df_list[0]

        if len(all_factors_df_list) > 1:
            for i in range(1, len(all_factors_df_list)):
                combined_factors_df = pd.merge(combined_factors_df,
                                               all_factors_df_list[i],
                                               on=['date', 'asset'],
                                               how='outer')
        logging.info(f"  > âœ… æˆåŠŸåˆå¹¶ {len(FACTOR_NAMES)} ä¸ªå› å­ã€‚")

        combined_factors_df = combined_factors_df.set_index(['date', 'asset'
                                                             ]).sort_index()

        # ã€ã€ã€æ ¸å¿ƒé€»è¾‘åˆ†æ”¯ï¼šé™æ€ vs æ»šåŠ¨ã€‘ã€‘ã€‘
        if not _run_rolling:
            # --- åˆ†æ”¯A: é™æ€æƒé‡é€»è¾‘ (GroupBy) ---
            logging.info(f"â„¹ï¸ æ¨¡å¼: é™æ€ (GroupBy æ¨¡å¼)")
            logging.info(
                f"âš™ï¸ æ­¥éª¤ 3b: æ‰§è¡Œæˆªé¢æ ‡å‡†åŒ– ({STANDARDIZER.__class__.__name__})...")

            standardized_factors_df = combined_factors_df.groupby(
                level='date').apply(lambda group: STANDARDIZER.standardize(
                    group.droplevel('date')[FACTOR_NAMES]))
            logging.info("    > âœ… (é™æ€) æ ‡å‡†åŒ–å®Œæˆã€‚")

            logging.info(
                f"âš™ï¸ æ­¥éª¤ 3c: æ‰§è¡Œå› å­åˆæˆ ({COMBINER.__class__.__name__})...")

            composite_factor_series = standardized_factors_df.groupby(
                level='date').apply(
                    lambda group: COMBINER.combine(group.droplevel('date')))
            logging.info("    > âœ… (é™æ€) å› å­åˆæˆå®Œæˆã€‚")

            composite_factor_series.name = 'factor_value'
            final_factor_name = f"CompositeFactor_{STRATEGY_NAME}_Static"

        else:
            # --- åˆ†æ”¯B: åŠ¨æ€æ»šåŠ¨æƒé‡é€»è¾‘ (é€æ—¥å¾ªç¯) ---
            logging.info(f"â„¹ï¸ æ¨¡å¼: åŠ¨æ€æ»šåŠ¨ (é€æ—¥å¾ªç¯æ¨¡å¼)")

            logging.info("âš™ï¸ æ­¥éª¤ 3b: åˆå§‹åŒ–æ»šåŠ¨è®¡ç®—å™¨...")
            roller = STRATEGY_CONFIG.create_rolling_calculator(
                forward_return_periods=FORWARD_RETURN_PERIODS,
                factor_names=FACTOR_NAMES)
            if roller is None:
                logging.critical(f"â›” ç­–ç•¥ {STRATEGY_NAME} é…ç½®é”™è¯¯ï¼šåº”ä¸ºæ»šåŠ¨æ¨¡å¼ä½†æ— æ³•åˆ›å»ºæ»šåŠ¨å™¨ã€‚")
                raise Exception(f"ç­–ç•¥ {STRATEGY_NAME} é…ç½®é”™è¯¯ï¼šæ— æ³•åˆ›å»ºæ»šåŠ¨å™¨ã€‚")

            logging.info("âš™ï¸ æ­¥éª¤ 3c: å‡†å¤‡æ»šåŠ¨æ•°æ® (åˆå¹¶å› å­ä¸æœªæ¥æ”¶ç›Š)...")
            all_data_merged = pd.merge(combined_factors_df.reset_index(),
                                       future_returns_df,
                                       on=['date', 'asset'],
                                       how='inner')
            all_data_merged.set_index(['date', 'asset'],
                                      inplace=True,
                                      drop=False)
            all_data_merged.sort_index(inplace=True)
            logging.info(f"  > âœ… æ»šåŠ¨æ•°æ®å‡†å¤‡å®Œæ¯•ï¼Œå…± {len(all_data_merged)} æ¡åˆå¹¶è®°å½•ã€‚")

            trading_dates = all_data_merged.index.get_level_values(
                'date').unique().sort_values()

            all_dates_series = pd.Series(index=trading_dates).index
            rebalance_freq = STRATEGY_CONFIG.get_rolling_param(
                'REBALANCE_FREQUENCY')
            rebalance_dates_ideal = pd.date_range(start=trading_dates.min(),
                                                  end=trading_dates.max(),
                                                  freq=rebalance_freq)

            rebalance_dates_idx = all_dates_series.searchsorted(
                rebalance_dates_ideal)
            rebalance_dates = all_dates_series[rebalance_dates_idx[
                rebalance_dates_idx < len(all_dates_series)]].date
            logging.info(
                f"  > â„¹ï¸ è°ƒä»“é¢‘ç‡: {rebalance_freq} (å…± {len(rebalance_dates)} ä¸ªè°ƒä»“æ—¥)"
            )

            all_composite_scores = []
            rolling_window_days = STRATEGY_CONFIG.get_rolling_param(
                'ROLLING_WINDOW_DAYS')
            logging.info(f"  > â„¹ï¸ å›çœ‹çª—å£: {rolling_window_days} å¤©")

            logging.info(f"âš™ï¸ æ­¥éª¤ 3d: æ‰§è¡Œæ»šåŠ¨æ ‡å‡†åŒ–ä¸åˆæˆ (å…± {len(trading_dates)} å¤©)...")

            # ã€ã€ã€ä½¿ç”¨ TQDM åŒ…è£¹å¾ªç¯ã€‘ã€‘ã€‘
            for current_date in tqdm(
                    trading_dates, desc="[ä¸»å¾ªç¯] æ»šåŠ¨å›æµ‹ä¸­", ncols=100,
                    file=sys.stdout):  # (ä¿æŒä¸ logger_config.py ä¸€è‡´)

                logging.debug(
                    f"  > æ­£åœ¨å¤„ç†æ—¥æœŸ: {current_date.strftime('%Y-%m-%d')}")

                # 1. ã€è°ƒä»“æ—¥ã€‘: é‡æ–°è®¡ç®—å’Œæ›´æ–°æƒé‡
                if current_date.date() in rebalance_dates:
                    # (è¿™ä¸ª INFO æ—¥å¿—ä¼šå¯¼è‡´è¿›åº¦æ¡è·³åŠ¨ï¼Œä½†å®ƒæ˜¯å¿…è¦çš„ä½é¢‘ä¿¡æ¯)
                    logging.info(
                        f"  >  pivotal: {current_date.strftime('%Y-%m-%d')} æ˜¯è°ƒä»“æ—¥ï¼Œé‡æ–°è®¡ç®—æƒé‡..."
                    )
                    window_end_date = current_date
                    window_start_date = window_end_date - pd.DateOffset(
                        days=rolling_window_days)

                    historical_window_mask = (
                        (all_data_merged.index.get_level_values('date')
                         >= window_start_date) &
                        (all_data_merged.index.get_level_values('date')
                         < window_end_date))
                    historical_window = all_data_merged.loc[
                        historical_window_mask]

                    if not historical_window.empty:
                        logging.debug(
                            f"    > æ­£åœ¨è°ƒç”¨ roller.calculate_new_weights...")
                        new_weights = roller.calculate_new_weights(
                            historical_window)
                        COMBINER.update_weights(new_weights)
                    else:
                        logging.warning(
                            f"    > âš ï¸ è­¦å‘Š: {current_date} çš„å†å²çª—å£æ•°æ®ä¸ºç©ºï¼Œæ— æ³•æ›´æ–°æƒé‡ã€‚")

                # 2. ã€æ¯æ—¥ã€‘: ä½¿ç”¨ã€å½“å‰ã€‘æƒé‡è¿›è¡Œåˆæˆ
                todays_data_slice = combined_factors_df[FACTOR_NAMES].loc[
                    current_date]
                standardized_slice = STANDARDIZER.standardize(
                    todays_data_slice)
                composite_score_series = COMBINER.combine(standardized_slice)

                composite_score_series.index = pd.MultiIndex.from_product(
                    [[current_date], composite_score_series.index],
                    names=['date', 'asset'])
                all_composite_scores.append(composite_score_series)

            logging.info("    > âœ… æ»šåŠ¨åˆæˆå®Œæˆã€‚")
            if not all_composite_scores:
                logging.error("  > âŒ é”™è¯¯ï¼šæ»šåŠ¨åˆæˆæœªäº§ç”Ÿä»»ä½•ç»“æœã€‚")
                composite_factor_series = pd.Series(name='factor_value')
            else:
                composite_factor_series = pd.concat(all_composite_scores)
                composite_factor_series.name = 'factor_value'

            final_factor_name = f"CompositeFactor_{STRATEGY_NAME}_Rolling"

        # --- æ»šåŠ¨é€»è¾‘åˆ†æ”¯ç»“æŸ ---

        logging.info("âš™ï¸ æ­¥éª¤ 3e: å‡†å¤‡æœ€ç»ˆæŠ¥å‘Šæ•°æ®...")
        if composite_factor_series.empty:
            logging.error("  > âŒ é”™è¯¯ï¼šå› å­åˆæˆç»“æœä¸ºç©ºï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Šã€‚")
        else:
            final_factor_data_df = pd.merge(
                composite_factor_series.reset_index(),
                future_returns_df,
                on=['date', 'asset'],
                how='inner')
            final_factor_data_df.set_index('date', inplace=True)
            logging.info("  > âœ… æœ€ç»ˆæŠ¥å‘Šæ•°æ®å‡†å¤‡å®Œæ¯•ã€‚")

    elif len(FACTOR_NAMES) == 1:
        # --- å•å› å­è·¯å¾„ ---
        logging.info(f"\n{'='*60}\n--- æ­¥éª¤ 3: å•å› å­è¯„æµ‹æµç¨‹ ---\n{'='*60}")
        factor_name = FACTOR_NAMES[0]
        final_factor_name = f"{factor_name}_Standardized"
        logging.info(f"âš™ï¸ æ­¥éª¤ 3a: å‡†å¤‡å•å› å­æ•°æ®: {factor_name}...")

        raw_factor_series = all_factors_dfs[factor_name]
        raw_factor_df_indexed = raw_factor_series.to_frame(name=factor_name)

        logging.info(
            f"âš™ï¸ æ­¥éª¤ 3b: æ‰§è¡Œæˆªé¢æ ‡å‡†åŒ– ({STANDARDIZER.__class__.__name__})...")

        def apply_standardization(group):
            return STANDARDIZER.standardize(
                group.droplevel('date')[[factor_name]])

        standardized_factor_df = raw_factor_df_indexed.groupby(
            level='date').apply(apply_standardization)
        logging.info("    > âœ… (å•å› å­) æ ‡å‡†åŒ–å®Œæˆã€‚")

        standardized_factor_df.rename(columns={factor_name: 'factor_value'},
                                      inplace=True)

        logging.info("âš™ï¸ æ­¥éª¤ 3c: å‡†å¤‡æœ€ç»ˆæŠ¥å‘Šæ•°æ®...")
        final_factor_data_df = pd.merge(standardized_factor_df.reset_index(),
                                        future_returns_df,
                                        on=['date', 'asset'],
                                        how='inner')
        final_factor_data_df.set_index('date', inplace=True)
        logging.info("  > âœ… æœ€ç»ˆæŠ¥å‘Šæ•°æ®å‡†å¤‡å®Œæ¯•ã€‚")

    else:
        # (æ­¤åˆ†æ”¯åœ¨ æ­¥éª¤ 2.6 ä¸­å·²è¢«å¤„ç†ï¼Œä½†ä½œä¸ºåŒé‡ä¿é™©)
        logging.critical("â›” æœªè®¡ç®—å‡ºä»»ä½•æœ‰æ•ˆçš„å› å­æ•°æ®ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
        exit()

    # =====================
    # 4. ç”Ÿæˆæœ€ç»ˆçš„å› å­åˆ†ææŠ¥å‘Š
    # =====================
    if not final_factor_data_df.empty:
        logging.info(
            f"\n{'='*60}\n--- æ­¥éª¤ 4: ä¸ºæœ€ç»ˆå› å­ '{final_factor_name}' ç”Ÿæˆåˆ†ææŠ¥å‘Š ---\n{'='*60}"
        )

        final_factor_data_df.dropna(inplace=True)
        if final_factor_data_df.empty:
            logging.warning(
                f"  > âš ï¸ è­¦å‘Š: æœ€ç»ˆå› å­ '{final_factor_name}' æ•°æ®åœ¨æ¸…ç†(dropna)åä¸ºç©ºã€‚")
        else:
            logging.info(
                f"  > âœ… æœ€ç»ˆå› å­æ•°æ®å‡†å¤‡å®Œæˆï¼Œå…± {len(final_factor_data_df)} æ¡æœ‰æ•ˆè®°å½•ã€‚")

            report_generator = FactorReport(
                factor_name=final_factor_name,
                factor_data=final_factor_data_df,
                forward_return_periods=FORWARD_RETURN_PERIODS,
                benchmark_data=benchmark_df)

            output_filename = os.path.join(OUTPUT_DIR,
                                           f"report_{final_factor_name}.html")

            logging.info(f"âš™ï¸ æ­£åœ¨ç”Ÿæˆ HTML æŠ¥å‘Š...")
            # ã€æ ¸å¿ƒã€‘ç”Ÿæˆ HTML æŠ¥å‘Š
            report_generator.generate_html_report(output_filename)
            # (æ—¥å¿—å·²ç§»è‡³ report_generator å†…éƒ¨)

    logging.info(f"\n{'='*60}")
    logging.info("ğŸ æ‰€æœ‰å› å­åˆ†ææµç¨‹æ‰§è¡Œå®Œæ¯• ğŸ")
    logging.info(f"{'='*60}")
