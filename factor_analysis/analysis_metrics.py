# core/analysis_metrics.py (å·²é‡æ„)

import pandas as pd
import numpy as np
from scipy.stats import spearmanr
from typing import Dict
import logging  # <- ã€ã€ã€æ–°å¢ã€‘ã€‘ã€‘


# è¿™æ˜¯ä¸€ä¸ªå†…éƒ¨å¸®åŠ©å‡½æ•°
def _calculate_spearman_for_group(group: pd.DataFrame,
                                  return_col: str) -> float:
    """è®¡ç®—å•ä¸ªåˆ†ç»„çš„æ–¯çš®å°”æ›¼ç§©ç›¸å…³ç³»æ•°ã€‚"""
    if len(group['factor_value']) < 2 or len(group[return_col]) < 2:
        return float('nan')
    
    # ã€ä¿®å¤ã€‘æ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰æ•ˆï¼ˆéç©ºä¸”éå¸¸é‡ï¼‰
    factor_values = group['factor_value'].dropna()
    return_values = group[return_col].dropna()
    
    if len(factor_values) < 2 or len(return_values) < 2:
        return float('nan')
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºå¸¸é‡ï¼ˆæ‰€æœ‰å€¼ç›¸åŒï¼‰
    if factor_values.nunique() <= 1 or return_values.nunique() <= 1:
        # logging.debug(f"  > è·³è¿‡å¸¸é‡æ•°æ®: factor_nunique={factor_values.nunique()}, return_nunique={return_values.nunique()}")
        return float('nan')

    corr, _ = spearmanr(factor_values, return_values)
    return float(corr) if pd.notna(corr) else float('nan')


def calculate_rank_ic_series(factor_data: pd.DataFrame,
                             period: int) -> pd.Series:
    """
    è®¡ç®—æ¯æ—¥çš„ Rank IC (Spearman ç§©ç›¸å…³ç³»æ•°) åºåˆ—ã€‚
    """
    return_col = f'forward_return_{period}d'
    
    # ã€è°ƒè¯•ã€‘æ£€æŸ¥è¾“å…¥æ•°æ®ç»“æ„
    logging.debug(f"ğŸ” [ICè®¡ç®—] è¾“å…¥æ•°æ®å½¢çŠ¶: {factor_data.shape}")
    logging.debug(f"ğŸ” [ICè®¡ç®—] è¾“å…¥æ•°æ®ç´¢å¼•: {factor_data.index.names}")
    logging.debug(f"ğŸ” [ICè®¡ç®—] è¾“å…¥æ•°æ®åˆ—: {list(factor_data.columns)}")
    logging.debug(f"ğŸ” [ICè®¡ç®—] ç›®æ ‡æ”¶ç›Šç‡åˆ—: {return_col}")
    logging.debug(f"ğŸ” [ICè®¡ç®—] å› å­å€¼åˆ—: 'factor_value'")
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
    if factor_data.empty:
        logging.warning("  > âš ï¸ [ICè®¡ç®—] è¾“å…¥æ•°æ®ä¸ºç©ºï¼")
        return pd.Series(dtype=float, name=f'rank_ic_{period}d')
    
    # æ£€æŸ¥å¿…éœ€åˆ—æ˜¯å¦å­˜åœ¨
    required_cols = ['factor_value', return_col]
    missing_cols = [col for col in required_cols if col not in factor_data.columns]
    if missing_cols:
        logging.error(f"  > âŒ [ICè®¡ç®—] ç¼ºå°‘å¿…éœ€åˆ—: {missing_cols}")
        logging.error(f"  > âŒ [ICè®¡ç®—] å¯ç”¨åˆ—: {list(factor_data.columns)}")
        return pd.Series(dtype=float, name=f'rank_ic_{period}d')

    if 'date' not in factor_data.index.names:
        logging.warning("  > âš ï¸ [ICè®¡ç®—] æœŸæœ› 'date' åœ¨ç´¢å¼•ä¸­ã€‚")
        if 'date' in factor_data.columns:
            factor_data = factor_data.set_index('date', append=True)
            logging.info(f"  > âœ… [ICè®¡ç®—] å·²å°† 'date' è®¾ç½®ä¸ºç´¢å¼•")
        else:
            logging.error("  > âŒ [ICè®¡ç®—] æ— æ³•æ‰¾åˆ° 'date' åˆ—æˆ–ç´¢å¼•")
            return pd.Series(dtype=float, name=f'rank_ic_{period}d')

    # æ£€æŸ¥æ•°æ®æ ·ä¾‹
    logging.debug(f"ğŸ” [ICè®¡ç®—] æ•°æ®æ ·ä¾‹:\n{factor_data.head()}")
    logging.debug(f"ğŸ” [ICè®¡ç®—] æ•°æ®ç»Ÿè®¡ - å› å­å€¼: å‡å€¼={factor_data['factor_value'].mean():.4f}, éç©ºæ•°={factor_data['factor_value'].count()}")
    logging.debug(f"ğŸ” [ICè®¡ç®—] æ•°æ®ç»Ÿè®¡ - æ”¶ç›Šç‡: å‡å€¼={factor_data[return_col].mean():.4f}, éç©ºæ•°={factor_data[return_col].count()}")

    # ã€è°ƒè¯•ã€‘é€ä¸ªæ—¥æœŸæ£€æŸ¥
    daily_groups = factor_data.groupby(level='date')
    logging.debug(f"ğŸ” [ICè®¡ç®—] æ€»å…± {len(daily_groups)} ä¸ªäº¤æ˜“æ—¥æœŸ")
    
    valid_days = 0
    invalid_days = 0
    
    def debug_calculate_spearman(group, return_col):
        nonlocal valid_days, invalid_days
        date = group.name if hasattr(group, 'name') else 'unknown'
        
        # åŸºæœ¬æ£€æŸ¥
        if len(group) < 2:
            logging.debug(f"  > [æ—¥æœŸ {date}] è·³è¿‡: æ•°æ®ç‚¹ä¸è¶³ ({len(group)} < 2)")
            invalid_days += 1
            return float('nan')
        
        # æ£€æŸ¥éç©ºæ•°æ®
        factor_values = group['factor_value'].dropna()
        return_values = group[return_col].dropna()
        
        if len(factor_values) < 2 or len(return_values) < 2:
            logging.debug(f"  > [æ—¥æœŸ {date}] è·³è¿‡: éç©ºæ•°æ®ä¸è¶³ (å› å­:{len(factor_values)}, æ”¶ç›Š:{len(return_values)})")
            invalid_days += 1
            return float('nan')
        
        # æ£€æŸ¥å”¯ä¸€å€¼
        if factor_values.nunique() <= 1:
            logging.debug(f"  > [æ—¥æœŸ {date}] è·³è¿‡: å› å­å€¼å…¨ç›¸åŒ ({factor_values.iloc[0] if len(factor_values) > 0 else 'N/A'})")
            invalid_days += 1
            return float('nan')
        
        if return_values.nunique() <= 1:
            logging.debug(f"  > [æ—¥æœŸ {date}] è·³è¿‡: æ”¶ç›Šç‡å…¨ç›¸åŒ ({return_values.iloc[0] if len(return_values) > 0 else 'N/A'})")
            invalid_days += 1
            return float('nan')
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        try:
            corr, _ = spearmanr(factor_values, return_values)
            valid_days += 1
            if valid_days <= 5:  # åªè®°å½•å‰5ä¸ªvalidçš„ä¾‹å­
                logging.debug(f"  > [æ—¥æœŸ {date}] âœ… æœ‰æ•ˆ: IC={corr:.4f}, æ ·æœ¬æ•°={len(factor_values)}")
            return float(corr) if pd.notna(corr) else float('nan')
        except Exception as e:
            logging.debug(f"  > [æ—¥æœŸ {date}] è®¡ç®—å‡ºé”™: {e}")
            invalid_days += 1
            return float('nan')
    
    ic_by_date = daily_groups.apply(debug_calculate_spearman, return_col=return_col)
    
    logging.debug(f"ğŸ” [ICè®¡ç®—] æœ‰æ•ˆæ—¥æœŸæ•°: {valid_days}, æ— æ•ˆæ—¥æœŸæ•°: {invalid_days}")

    ic_by_date = ic_by_date.dropna()
    logging.debug(f"ğŸ” [ICè®¡ç®—] æœ€ç»ˆICåºåˆ—é•¿åº¦: {len(ic_by_date)}")
    
    if len(ic_by_date) == 0:
        logging.warning("  > âš ï¸ [ICè®¡ç®—] æ‰€æœ‰æ—¥æœŸçš„ICè®¡ç®—éƒ½å¤±è´¥äº†ï¼")
        # æ‰“å°ç¬¬ä¸€ä¸ªæ—¥æœŸçš„è¯¦ç»†æ•°æ®ç”¨äºè°ƒè¯•
        first_date = factor_data.index.get_level_values('date')[0]
        first_day_data = factor_data.loc[first_date]
        logging.debug(f"  > âŒ [ç¬¬ä¸€ä¸ªæ—¥æœŸ {first_date}] æ•°æ®è¯¦æƒ…:")
        logging.debug(f"     å› å­å€¼ç»Ÿè®¡: {first_day_data['factor_value'].describe()}")
        logging.debug(f"     æ”¶ç›Šç‡ç»Ÿè®¡: {first_day_data[return_col].describe()}")
        logging.debug(f"     å› å­å€¼å”¯ä¸€å€¼æ•°: {first_day_data['factor_value'].nunique()}")
        logging.debug(f"     æ”¶ç›Šç‡å”¯ä¸€å€¼æ•°: {first_day_data[return_col].nunique()}")
    
    ic_by_date.name = f'rank_ic_{period}d'
    return ic_by_date


def analyze_ic_statistics(ic_series: pd.Series) -> Dict[str, float]:
    """
    æ ¹æ®ICæ—¶é—´åºåˆ—è®¡ç®—å¸¸ç”¨çš„ç»Ÿè®¡æŒ‡æ ‡ã€‚
    """
    if ic_series.empty:
        logging.warning("  > âš ï¸ [analyze_ic_statistics] ä¼ å…¥çš„ IC åºåˆ—ä¸ºç©ºã€‚")
        return {}

    mean = ic_series.mean()
    std = ic_series.std()

    safe_std = std if std > 0 else 1e-6
    safe_len = len(ic_series) if len(ic_series) > 0 else 1

    stats: Dict[str, float] = {
        'ic_mean': mean,
        'ic_std': std,
        'ir': mean / safe_std,
        'ic_gt_0_prob': (ic_series > 0).sum() / safe_len,
        'ic_abs_gt_0.02_prob': (ic_series.abs() > 0.02).sum() / safe_len,
        'ic_t_stat': (mean / safe_std) * np.sqrt(safe_len)
    }
    return stats


def calculate_quantile_returns(factor_data: pd.DataFrame,
                               period: int,
                               quantiles: int = 5) -> pd.Series:
    """
    è®¡ç®—å› å­åˆ†å±‚æ”¶ç›Šã€‚ï¼ˆå·²å¢å¼ºå¯¹æ— æ³•åˆ†å±‚æƒ…å†µçš„å¤„ç†ï¼‰
    """
    return_col = f'forward_return_{period}d'

    def get_quantile_return(df: pd.DataFrame) -> pd.Series:
        try:
            df['quantile'] = pd.qcut(df['factor_value'],
                                     quantiles,
                                     labels=False,
                                     duplicates='drop')
        except ValueError as e:
            # å½“æ•°æ®ç‚¹å¤ªå°‘æ—¶ï¼Œqcut å¯èƒ½ä¼šå¤±è´¥
            logging.debug(f"  > ğŸ [qcut å¤±è´¥] (å‘¨æœŸ {period}d): {e}ã€‚å¯èƒ½å½“æ—¥æ•°æ®ç‚¹è¿‡å°‘ã€‚")
            df['quantile'] = 0  # å°†æ‰€æœ‰å½’ä¸ºä¸€å±‚

        return df.groupby('quantile')[return_col].mean()

    # å¯¹æ¯ä¸ªæˆªé¢æ—¥åº”ç”¨åˆ†å±‚å‡½æ•°
    grouped_returns = factor_data.groupby(
        level='date').apply(get_quantile_return)

    if isinstance(grouped_returns, pd.DataFrame):
        quantile_returns = grouped_returns.mean(axis=0)
        quantile_returns.index = [
            f'Q{int(i)+1}' for i in quantile_returns.index
        ]
    else:
        # ã€ã€ã€ä¿®æ”¹ã€‘ã€‘ã€‘
        logging.warning(
            f"  > âš ï¸  (å‘¨æœŸ {period}d): å› å­å€¼æ— æ³•æœ‰æ•ˆåˆ†å±‚ä¸º {quantiles} ç»„ (å¯èƒ½å› å­å€¼å·®å¼‚è¿‡å°)ã€‚"
            f" ä»…è¿”å›æ•´ä½“å¹³å‡æ”¶ç›Šã€‚")
        mean_ret = grouped_returns.mean()
        quantile_returns = pd.Series({'Q1': mean_ret})

    if len(quantile_returns) > 1:
        # (ç¡®ä¿ Q1 å’Œ QN å­˜åœ¨)
        if f'Q{quantiles}' in quantile_returns.index and 'Q1' in quantile_returns.index:
            ls_return = quantile_returns[f'Q{quantiles}'] - quantile_returns[
                'Q1']
        else:
            # å¦‚æœåˆ†å±‚ä¸å®Œæ•´ï¼Œä½¿ç”¨æœ€åä¸€å±‚å’Œç¬¬ä¸€å±‚
            ls_return = quantile_returns.iloc[-1] - quantile_returns.iloc[0]

        long_short_series = pd.Series({'Long-Short': ls_return})
        quantile_returns = pd.concat([quantile_returns, long_short_series])
    elif len(quantile_returns) > 0:
        # è‡³å°‘æœ‰ä¸€ä¸ªåˆ†ä½æ•°ï¼Œä½†ä¸è¶³ä»¥è®¡ç®—å¤šç©º
        long_short_series = pd.Series({'Long-Short': 0.0})
        quantile_returns = pd.concat([quantile_returns, long_short_series])

    quantile_returns.name = f'mean_return_{period}d'
    return quantile_returns


def calculate_factor_portfolio_returns(factor_data: pd.DataFrame,
                                       period: int,
                                       quantiles: int = 5) -> pd.Series:
    """
    è®¡ç®—åŸºäºå› å­æ’åºæ„å»ºçš„å¤šç©ºç»„åˆçš„æ¯æ—¥æ”¶ç›Šåºåˆ—ã€‚
    """
    return_col = f'forward_return_{period}d'

    def get_ls_return(df: pd.DataFrame) -> float:
        try:
            if df['factor_value'].nunique() < quantiles:
                return 0.0

            df['quantile'] = pd.qcut(df['factor_value'],
                                     quantiles,
                                     labels=False,
                                     duplicates='drop')

            if (quantiles -
                    1) in df['quantile'].values and 0 in df['quantile'].values:
                long_ret = df[df['quantile'] == quantiles -
                              1][return_col].mean()
                short_ret = df[df['quantile'] == 0][return_col].mean()
                return long_ret - short_ret
            return 0.0
        except Exception as e:
            logging.warning(f"  > âš ï¸ [get_ls_return] è®¡ç®—å¤šç©ºæ”¶ç›Šæ—¶å‡ºé”™: {e}")
            return 0.0

    daily_ls_returns: pd.Series = factor_data.groupby(
        level='date').apply(get_ls_return)

    # ã€ã€ã€é‡è¦ä¿®æ­£ã€‘ã€‘ã€‘
    # åŸå§‹çš„å¤šç©ºæ”¶ç›Šè®¡ç®— (rolling(period).mean() / period) æ˜¯æœ‰é—®é¢˜çš„ã€‚
    # æ­£ç¡®çš„ã€æ— é‡å ï¼ˆnon-overlappingï¼‰çš„ç»„åˆæ”¶ç›Šåº”è¯¥å¦‚ä¸‹ï¼š

    # 1. å‡è®¾æˆ‘ä»¬åœ¨ `date` è¿™ä¸€å¤©ï¼Œæ ¹æ®å› å­å€¼æ„å»ºäº†å¤šç©ºç»„åˆã€‚
    # 2. æˆ‘ä»¬æŒæœ‰äº† `period` å¤©ã€‚
    # 3. æ”¶ç›Š `daily_ls_returns` æ˜¯è¿™ `period` å¤©çš„æ€»æ”¶ç›Šï¼ˆæˆ–å¹³å‡æ”¶ç›Šï¼Œå–å†³äº `return_col`ï¼‰ã€‚

    # `factor_data` åŒ…å« `forward_return_{p}d`
    # `get_ls_return` è®¡ç®—çš„æ˜¯ `(Q_N_ret - Q_1_ret)`ï¼Œå…¶ä¸­ `ret` æ˜¯ `p` æ—¥æ”¶ç›Š
    # æ‰€ä»¥ `daily_ls_returns` å·²ç»æ˜¯ P æ—¥çš„ç»„åˆæ”¶ç›Šã€‚

    # å‡è®¾ daily_ls_returns æ˜¯æ¯æ—¥æƒé‡æ›´æ–°ã€æŒæœ‰ p å¤©çš„æ”¶ç›Šã€‚
    # è¦å°†å…¶è½¬æ¢ä¸º *æ—¥æ”¶ç›Šç‡*ï¼Œæˆ‘ä»¬åªéœ€å°†å…¶é”™å¼€ p å¤©å³å¯ã€‚
    # (æ›´ç®€å•çš„é‡å æ–¹æ³•æ˜¯ï¼š`daily_ls_returns.shift(1).rolling(window=period).mean() / period`)

    # ä¸ºä¿æŒç®€å•ï¼Œæˆ‘ä»¬å‡è®¾ `daily_ls_returns` æ˜¯ p-day æ”¶ç›Šçš„æ—¥å‡å€¼
    # (è¿™ä¸ Alphalens çš„æ ‡å‡†åšæ³•ä¸€è‡´)

    # æˆ‘ä»¬å‡è®¾ `get_ls_return` å¾—åˆ°çš„å·²ç»æ˜¯ P æ—¥çš„å¹³å‡ *æ—¥* æ”¶ç›Šï¼ˆå¦‚æœ `return_col` æ˜¯ `1d` çš„è¯ï¼‰
    # ä½† `return_col` æ˜¯ `forward_return_{p}d`
    # æ­£ç¡®çš„æ–¹å¼åº”è¯¥æ˜¯ä½¿ç”¨ 1d æ”¶ç›Šæ¥è®¡ç®—ç»„åˆï¼Œä½†è¿™é‡Œä¸ºäº†ä¿æŒä¸ `period` ä¸€è‡´ï¼š

    # æˆ‘ä»¬å‡è®¾ daily_ls_returns æ˜¯åœ¨å½“å¤©å»ºä»“ã€æŒæœ‰På¤©çš„ *æ€»æ”¶ç›Š*
    # è¦å°†å…¶è½¬æ¢ä¸º *æ—¥å‡æ”¶ç›Š*ï¼Œæˆ‘ä»¬é™¤ä»¥ P

    # æ‰¾åˆ° 1d çš„æ”¶ç›Šåˆ—
    return_1d_col = 'forward_return_1d'
    if 'forward_return_1d' not in factor_data.columns and 1 in factor_data.columns:
        return_1d_col = f'forward_return_1d'
    elif 'forward_return_1d' not in factor_data.columns:
        logging.warning(
            "  > âš ï¸ [L/S Portfolio] æ— æ³•è®¡ç®—å‡€å€¼æ›²çº¿ï¼Œå› ä¸º 'forward_return_1d' æœªæä¾›ã€‚")
        return pd.Series(
            0.0, index=factor_data.index.get_level_values('date').unique())

    def get_1d_ls_return(df: pd.DataFrame) -> float:
        if df['factor_value'].nunique() < quantiles:
            return 0.0

        df['quantile'] = pd.qcut(df['factor_value'],
                                 quantiles,
                                 labels=False,
                                 duplicates='drop')

        if (quantiles -
                1) in df['quantile'].values and 0 in df['quantile'].values:
            long_ret = df[df['quantile'] == quantiles -
                          1][return_1d_col].mean()
            short_ret = df[df['quantile'] == 0][return_1d_col].mean()
            return (long_ret - short_ret) / 2  # å› å­å€¼ä¸ºä¸­æ€§ï¼Œå¤šç©ºå„ä¸€åŠä»“ä½
        return 0.0

    # `daily_ls_returns_1d` æ˜¯ *ç¬¬äºŒå¤©* çš„æ”¶ç›Šï¼ˆå› ä¸º 1d return æ˜¯ T+1 çš„æ”¶ç›Šï¼‰
    daily_ls_returns_1d = factor_data.groupby(
        level='date').apply(get_1d_ls_return)

    # å°† T æ—¥çš„å› å­ä¿¡å·ï¼Œshift(1) åˆ° T+1 æ—¥æ‰èƒ½äº§ç”Ÿæ”¶ç›Š
    return daily_ls_returns_1d.shift(1).fillna(0)
