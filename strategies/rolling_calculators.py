# strategies/rolling_calculators.py

import pandas as pd
import logging
import json
import requests
from typing import Any, Dict, List
from sklearn.linear_model import LinearRegression
from core.abstractions import RollingCalculatorBase, AITrainerBase
from factor_analysis import analysis_metrics as metrics


class RollingICIRCalculator(RollingCalculatorBase):

    def __init__(self, factor_weight_config: Dict[str, str],
                 forward_return_periods: List[int], **kwargs):
        super().__init__(**kwargs)
        self.config, self.periods = factor_weight_config, forward_return_periods

    def _parse_metric_str(self, s: str) -> tuple[str, int]:
        p = s.split('_')[-1]
        k = '_'.join(s.split('_')[:-1]) or 'ir'
        if p.endswith('d'): return k, int(p[:-1])
        return s, self.periods[0]

    def _calculate_payload_for_day(self,
                                   hist_df: pd.DataFrame) -> Dict[str, float]:
        w = {}
        
        # ã€å…³é”®è°ƒè¯•ã€‘æ‰“å°æ‰€æœ‰å¯ç”¨çš„åˆ—å
        available_cols = hist_df.columns.tolist()
        logging.debug(f"ğŸ” [RollingICIR] å†å²æ•°æ®çª—å£å¯ç”¨åˆ—: {available_cols}")
        logging.debug(f"ğŸ” [RollingICIR] æœŸæœ›çš„å› å­åç§°: {self.factor_names}")
        
        for fname in self.factor_names:
            m_str = self.config.get(fname, f'ir_{self.periods[0]}d')
            m_key, p = self._parse_metric_str(m_str)
            
            # ã€è°ƒè¯•ã€‘æ£€æŸ¥è¾“å…¥æ•°æ®
            logging.debug(f"ğŸ” [RollingICIR] å› å­ {fname}: è®¡ç®— {p}d IC")
            logging.debug(f"ğŸ” [RollingICIR] å†å²æ•°æ®å½¢çŠ¶: {hist_df.shape}")
            
            # ã€å…³é”®ä¿®å¤ã€‘æ›´åŠ ä¸¥æ ¼çš„åˆ—å­˜åœ¨æ€§æ£€æŸ¥
            if fname not in hist_df.columns:
                # æŸ¥æ‰¾åŒ¹é…ç›¸ä¼¼åº¦çš„åˆ—
                similar_cols = [col for col in available_cols if fname.lower() in col.lower() or col.lower() in fname.lower()]
                logging.error(f"âŒ [RollingICIR] å› å­ {fname}: åœ¨å†å²æ•°æ®ä¸­æœªæ‰¾åˆ°ï¼")
                logging.error(f"âŒ [RollingICIR] å¯ç”¨åˆ—åŒ…æ‹¬: {available_cols}")
                if similar_cols:
                    logging.warning(f"  > âš ï¸ å‘ç°ç›¸ä¼¼åˆ—: {similar_cols}ï¼Œæ£€æŸ¥æ˜¯å¦å‘½åä¸ä¸€è‡´")
                w[fname] = 0.0
                continue
                
            return_col_p = f'forward_return_{p}d'
            if return_col_p not in hist_df.columns:
                logging.error(f"âŒ [RollingICIR] æ”¶ç›Šç‡åˆ— {return_col_p}: åœ¨å†å²æ•°æ®ä¸­æœªæ‰¾åˆ°ï¼")
                available_return_cols = [col for col in available_cols if col.startswith('forward_return_')]
                logging.error(f"âŒ [RollingICIR] å¯ç”¨çš„æ”¶ç›Šç‡åˆ—: {available_return_cols}")
                w[fname] = 0.0
                continue
            
            # éªŒè¯æ•°æ®
            factor_data = hist_df[fname]
            return_data = hist_df[return_col_p]
            logging.debug(f"ğŸ” [RollingICIR] å› å­ {fname}: éç©ºå€¼æ•°={factor_data.count()}/{len(factor_data)}, å‡å€¼={factor_data.mean():.4f}")
            logging.debug(f"ğŸ” [RollingICIR] æ”¶ç›Šç‡ {return_col_p}: éç©ºå€¼æ•°={return_data.count()}/{len(return_data)}, å‡å€¼={return_data.mean():.4f}")
            
            # å¦‚æœå¤§å¤šæ•°å€¼ä¸º0æˆ–NAï¼Œæç¤ºè­¦å‘Š
            if factor_data.count() < len(factor_data) * 0.1:
                logging.warning(f"  > âš ï¸ [RollingICIR] å› å­ {fname}: è¶…è¿‡90%çš„å€¼ä¸ºç©ºæˆ–NAï¼")
                
            ic_data = hist_df[[fname, return_col_p
                               ]].rename(columns={fname: 'factor_value'})
            logging.debug(f"ğŸ” [RollingICIR] ICè®¡ç®—æ•°æ®å½¢çŠ¶: {ic_data.shape}")
            logging.debug(f"ğŸ” [RollingICIR] æ•°æ®æ ·ä¾‹:\n{ic_data.head()}")
            
            ic_s = metrics.calculate_rank_ic_series(ic_data.dropna(), p)
            logging.debug(f"ğŸ” [RollingICIR] è®¡ç®—å¾—åˆ°çš„ICåºåˆ—: {len(ic_s)} ä¸ªå€¼")
            if len(ic_s) > 0:
                logging.debug(f"ğŸ” [RollingICIR] ICç»Ÿè®¡: {metrics.analyze_ic_statistics(ic_s)}")
            
            w[fname] = metrics.analyze_ic_statistics(ic_s).get(m_key, 0.0)
            logging.debug(f"ğŸ” [RollingICIR] å› å­ {fname} æƒé‡: {w[fname]:.4f}")
            
        tot_s = sum(abs(v) for v in w.values())
        final_weights = {f: v / tot_s if tot_s else 0.0 for f, v in w.items()}
        logging.debug(f"ğŸ” [RollingICIR] æœ€ç»ˆæƒé‡: {final_weights}")
        
        # å¦‚æœæ‰€æœ‰æƒé‡éƒ½æ˜¯0ï¼Œè®°å½•é‡å¤§è­¦å‘Š
        if all(v == 0.0 for v in final_weights.values()):
            logging.warning(f"âš ï¸ [RollingICIR] æ‰€æœ‰å› å­æƒé‡éƒ½ä¸º0ï¼ä½¿ç”¨ç­‰æƒå›é€€")
            # ä¸´æ—¶ç­‰æƒ
            equal_weight = 1.0 / len(self.factor_names) if self.factor_names else 1.0
            final_weights = {f: equal_weight for f in self.factor_names}
            
        return final_weights

    def _combine_factors_for_day(self, payload: Dict[str, float],
                                 daily_df: pd.DataFrame) -> pd.Series:
        weights = pd.Series(payload).reindex(daily_df.columns, fill_value=0)
        return (daily_df * weights).sum(axis=1)


class RollingRegressionCalculator(RollingCalculatorBase):

    def __init__(self, target_return_period: int, **kwargs):
        super().__init__(**kwargs)
        self.return_col = f'forward_return_{target_return_period}d'

    def _calculate_payload_for_day(
            self, historical_data_window: pd.DataFrame) -> Dict[str, float]:
        data = historical_data_window[self.factor_names +
                                      [self.return_col]].dropna()
        if len(data) < len(self.factor_names) + 2:
            return {f: 0.0 for f in self.factor_names}
        X, y = data[self.factor_names], data[self.return_col]
        model = LinearRegression().fit(X, y)
        w = {n: c for n, c in zip(self.factor_names, model.coef_)}
        tot_s = sum(abs(v) for v in w.values())
        return {f: v / tot_s if tot_s else 0.0 for f, v in w.items()}

    def _combine_factors_for_day(self, payload: Dict[str, float],
                                 daily_factors: pd.DataFrame) -> pd.Series:
        weights = pd.Series(payload).reindex(daily_factors.columns,
                                             fill_value=0)
        return (daily_factors * weights).sum(axis=1)


class RollingAITrainer(RollingCalculatorBase):

    def __init__(self, training_calculator: AITrainerBase, **kwargs):
        super().__init__(**kwargs)
        self.trainer = training_calculator

    def _calculate_payload_for_day(
            self, historical_data_window: pd.DataFrame) -> Any:
        return self.trainer.train_model(historical_data_window,
                                        self.factor_names)

    def _combine_factors_for_day(self, payload: Any,
                                 daily_factors: pd.DataFrame) -> pd.Series:
        model = payload
        try:
            features = model.feature_name_ if hasattr(
                model, 'feature_name_') else daily_factors.columns
            X_today = daily_factors[features]
            return pd.Series(model.predict(X_today), index=X_today.index)
        except Exception as e:
            logging.error(f"  > âŒ [RollingAITrainer] é¢„æµ‹å¤±è´¥: {e}")
            return pd.Series(dtype=float)


class AdversarialLLMCombiner(RollingCalculatorBase):
    """
    é€šè¿‡å¤šæ™ºèƒ½ä½“å¯¹æŠ—ï¼ˆMulti-Agent Adversarial Debateï¼‰æœºåˆ¶æ¥åŠ¨æ€å†³å®šå› å­çš„æƒé‡ã€‚
    
    è¯¥ç±»ä½¿ç”¨ä¸¤ä¸ªå…·æœ‰ä¸åŒæŠ•èµ„ç†å¿µçš„AIä»£ç†è¿›è¡Œè¾©è®ºï¼š
    - Agent A (æ¿€è¿›å‹æŠ•èµ„ç»ç†): å…³æ³¨Alphaæ”¶ç›Šæœ€å¤§åŒ–
    - Agent B (ä¿å®ˆå‹é£é™©ç»ç†): å…³æ³¨é£é™©æ§åˆ¶å’Œå›æ’¤ç®¡ç†
    
    é€šè¿‡å¤šè½®è¾©è®ºè¾¾åˆ°æœ€ä¼˜æƒé‡åˆ†é…ï¼Œæ”¯æŒæ­£è´Ÿæƒé‡ä»¥è¡¨è¾¾å¯¹å› å­çš„ä¸åŒè§‚ç‚¹ã€‚
    
    æ³¨æ„ï¼šè¿™é‡Œçš„"è°ƒä»“"å®é™…ä¸Šæ˜¯æŒ‡å› å­æƒé‡çš„æ›´æ–°é¢‘ç‡ï¼Œè€ŒéæŠ•èµ„ç»„åˆçš„å®é™…è°ƒä»“ã€‚
    """

    def __init__(self,
                 api_url: str = "https://api.openai.com/v1/chat/completions",
                 api_key: str | None = None,
                 max_rounds: int = 2,
                 include_factor_values: bool = False,
                 include_conversation_history: bool = False,
                 allow_negative_weights: bool = True,
                 **kwargs):
        """
        åˆå§‹åŒ–AdversarialLLMCombiner
        
        Args:
            api_url: OpenAIå…¼å®¹APIçš„URL
            api_key: APIå¯†é’¥
            max_rounds: æœ€å¤§è¾©è®ºè½®æ•°
            include_factor_values: æ˜¯å¦åŒ…å«å½“å‰å› å­å€¼
            include_conversation_history: æ˜¯å¦åŒ…å«å¯¹è¯å†å²
            allow_negative_weights: æ˜¯å¦å…è®¸è´Ÿæƒé‡ï¼ˆåšç©ºå› å­è§‚ç‚¹ï¼‰
        """
        super().__init__(**kwargs)
        self.api_url = api_url
        self.api_key = api_key
        self.max_rounds = max_rounds
        self.include_factor_values = include_factor_values
        self.include_conversation_history = include_conversation_history
        self.allow_negative_weights = allow_negative_weights
        self.conversation_history = []

    def _call_llm(self, prompt: str, system_prompt: str | None = None) -> str:
        """
        è°ƒç”¨LLM APIè·å–å“åº”
        
        Args:
            prompt: ç”¨æˆ·æç¤º
            system_prompt: ç³»ç»Ÿæç¤º
            
        Returns:
            LLMçš„å“åº”æ–‡æœ¬
        """
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}" if self.api_key else ""
        }

        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        # æ·»åŠ å¯¹è¯å†å²ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.include_conversation_history and self.conversation_history:
            messages.extend(self.conversation_history)

        payload = {
            "model": "qwen-flash",
            "messages": messages,
            "temperature": 0.7,
            "max_tokens": 5000
        }

        # å¦‚æœå…è®¸è´Ÿæƒé‡ï¼Œåœ¨æç¤ºä¸­æ˜ç¡®è¯´æ˜
        if self.allow_negative_weights:
            payload["temperature"] = 0.8  # ç¨å¾®å¢åŠ éšæœºæ€§ä»¥ä¿ƒè¿›åˆ›æ–°æ€ç»´

        try:
            response = requests.post(self.api_url,
                                     headers=headers,
                                     json=payload,
                                     timeout=30)
            response.raise_for_status()
            result = response.json()
            return result["choices"][0]["message"]["content"]
        except Exception as e:
            logging.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            raise

    def _parse_json_response(self,
                             response_text: str,
                             max_retries: int = 3) -> Dict[str, float]:
        """
        è§£æLLMçš„JSONå“åº”ï¼ŒåŒ…å«é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
        
        Args:
            response_text: LLMçš„å“åº”æ–‡æœ¬
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            è§£æåçš„æƒé‡å­—å…¸
        """
        for attempt in range(max_retries):
            try:
                # å°è¯•ä»å“åº”ä¸­æå–JSONéƒ¨åˆ†
                json_start = response_text.find('{')
                json_end = response_text.rfind('}') + 1

                if json_start != -1 and json_end > json_start:
                    json_str = response_text[json_start:json_end]
                    parsed_response = json.loads(json_str)

                    # å¦‚æœå“åº”åŒ…å«ç†ç”±å­—æ®µï¼Œæå–æƒé‡éƒ¨åˆ†
                    if isinstance(parsed_response, dict):
                        # æ£€æŸ¥æ˜¯å¦æ˜¯å¸¦æœ‰ç†ç”±çš„æ ¼å¼
                        if all(
                                isinstance(v, dict) and 'weight' in v
                                for v in parsed_response.values()):
                            # æå–æƒé‡
                            weights = {
                                k: v['weight']
                                for k, v in parsed_response.items()
                            }
                            # è®°å½•ç†ç”±ä¿¡æ¯
                            reasons = {
                                k: v.get('reason', 'æœªæä¾›ç†ç”±')
                                for k, v in parsed_response.items()
                            }
                            logging.info(
                                f"[AdversarialLLM] æƒé‡ç†ç”±: {json.dumps(reasons, ensure_ascii=False)}"
                            )
                        else:
                            # æ ‡å‡†æ ¼å¼
                            weights = parsed_response

                        # éªŒè¯æƒé‡å­—å…¸çš„æ ¼å¼
                        if isinstance(weights, dict) and all(
                                isinstance(k, str)
                                and isinstance(v, (int, float))
                                for k, v in weights.items()):
                            return weights

                # å¦‚æœè§£æå¤±è´¥ï¼Œè®°å½•æ—¥å¿—å¹¶å‡†å¤‡é‡è¯•
                logging.warning(
                    f"JSONè§£æå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {response_text}"
                )

                if attempt < max_retries - 1:  # ä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•
                    # è¯·æ±‚LLMé‡æ–°æ ¼å¼åŒ–è¾“å‡º
                    retry_prompt = f"è¯·å°†ä»¥ä¸‹å†…å®¹é‡æ–°æ ¼å¼åŒ–ä¸ºæ ‡å‡†JSONæ ¼å¼çš„å­—å…¸ï¼Œé”®ä¸ºå› å­åç§°ï¼Œå€¼ä¸ºæƒé‡æ•°å€¼ï¼ˆå¯æ­£å¯è´Ÿï¼‰ï¼š\n\n{response_text}"
                    response_text = self._call_llm(retry_prompt)

            except json.JSONDecodeError as e:
                logging.warning(
                    f"JSONè§£ç é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")

                if attempt < max_retries - 1:  # ä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•
                    retry_prompt = f"è¯·å°†ä»¥ä¸‹å†…å®¹é‡æ–°æ ¼å¼åŒ–ä¸ºæ ‡å‡†JSONæ ¼å¼çš„å­—å…¸ï¼Œé”®ä¸ºå› å­åç§°ï¼Œå€¼ä¸ºæƒé‡æ•°å€¼ï¼š\n\n{response_text}"
                    response_text = self._call_llm(retry_prompt)

            except Exception as e:
                logging.error(f"è§£æè¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
                break

        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
        raise ValueError(f"æ— æ³•è§£æLLMå“åº”ä¸ºæœ‰æ•ˆçš„JSONæ ¼å¼: {response_text}")

    def _calculate_payload_for_day(
            self, historical_data_window: pd.DataFrame) -> Dict[str, float]:
        """
        ä½¿ç”¨å¤šæ™ºèƒ½ä½“å¯¹æŠ—æœºåˆ¶è®¡ç®—å› å­æƒé‡
        
        Args:
            historical_data_window: å†å²æ•°æ®çª—å£
            
        Returns:
            å› å­æƒé‡å­—å…¸
        """
        # å‡†å¤‡å› å­æŒ‡æ ‡æ•°æ®
        factor_metrics = {}
        for fname in self.factor_names:
            # è·å–å› å­æ•°æ®å’Œå¯¹åº”çš„forward return
            factor_cols = [
                col for col in historical_data_window.columns
                if col.startswith(fname)
            ]
            if not factor_cols:
                continue

            # è·å–é»˜è®¤çš„forward returnå‘¨æœŸï¼ˆå¦‚æœæœ‰é…ç½®çš„è¯ï¼‰
            return_period = 30  # é»˜è®¤30å¤©å‘¨æœŸ
            return_col = f'forward_return_{return_period}d'

            # æ£€æŸ¥æ˜¯å¦æœ‰è¯¥å‘¨æœŸçš„forward returnæ•°æ®
            if return_col not in historical_data_window.columns:
                # å°è¯•æ‰¾åˆ°æœ€æ¥è¿‘çš„forward returnåˆ—
                forward_cols = [
                    col for col in historical_data_window.columns
                    if col.startswith('forward_return_')
                ]
                if forward_cols:
                    return_col = forward_cols[0]
                else:
                    continue

            # è®¡ç®—ICåºåˆ—å’Œç»Ÿè®¡æŒ‡æ ‡
            ic_data = historical_data_window[[
                fname, return_col
            ]].rename(columns={fname: 'factor_value'})
            ic_series = metrics.calculate_rank_ic_series(
                ic_data.dropna(), return_period)
            ic_stats = metrics.analyze_ic_statistics(ic_series)

            factor_metrics[fname] = ic_stats

        # å¦‚æœæ²¡æœ‰å› å­æŒ‡æ ‡æ•°æ®ï¼Œè¿”å›ç­‰æƒåˆ†é…
        if not factor_metrics:
            return {
                fname: 1.0 / len(self.factor_names)
                for fname in self.factor_names
            }
        """
        ä½¿ç”¨å¤šæ™ºèƒ½ä½“å¯¹æŠ—æœºåˆ¶è®¡ç®—å› å­æƒé‡
        
        Args:
            historical_data_window: å†å²æ•°æ®çª—å£
            
        Returns:
            å› å­æƒé‡å­—å…¸
        """
        # å‡†å¤‡å› å­æŒ‡æ ‡æ•°æ®
        factor_metrics = {}
        for fname in self.factor_names:
            # è·å–å› å­æ•°æ®å’Œå¯¹åº”çš„forward return
            factor_cols = [
                col for col in historical_data_window.columns
                if col.startswith(fname)
            ]
            if not factor_cols:
                continue

            # è·å–é»˜è®¤çš„forward returnå‘¨æœŸï¼ˆå¦‚æœæœ‰é…ç½®çš„è¯ï¼‰
            return_period = 30  # é»˜è®¤30å¤©å‘¨æœŸ
            return_col = f'forward_return_{return_period}d'

            # æ£€æŸ¥æ˜¯å¦æœ‰è¯¥å‘¨æœŸçš„forward returnæ•°æ®
            if return_col not in historical_data_window.columns:
                # å°è¯•æ‰¾åˆ°æœ€æ¥è¿‘çš„forward returnåˆ—
                forward_cols = [
                    col for col in historical_data_window.columns
                    if col.startswith('forward_return_')
                ]
                if forward_cols:
                    return_col = forward_cols[0]
                else:
                    continue

            # è®¡ç®—ICåºåˆ—å’Œç»Ÿè®¡æŒ‡æ ‡
            ic_data = historical_data_window[[
                fname, return_col
            ]].rename(columns={fname: 'factor_value'})
            ic_series = metrics.calculate_rank_ic_series(
                ic_data.dropna(), return_period)
            ic_stats = metrics.analyze_ic_statistics(ic_series)

            factor_metrics[fname] = ic_stats

        # å¦‚æœæ²¡æœ‰å› å­æŒ‡æ ‡æ•°æ®ï¼Œè¿”å›ç­‰æƒåˆ†é…
        if not factor_metrics:
            return {
                fname: 1.0 / len(self.factor_names)
                for fname in self.factor_names
            }

        # æ„å»ºAgent A (Portfolio Manager)çš„æç¤º
        agent_a_system_prompt = (
            "ä½ æ˜¯ä¸€åä¸ºé¡¶çº§å¯¹å†²åŸºé‡‘å·¥ä½œçš„é‡åŒ–æŠ•èµ„ç»„åˆç»ç†(Alpha PM)ã€‚"
            "ä½ çš„æ ¸å¿ƒç›®æ ‡æ˜¯æœ€å¤§åŒ–æŠ•èµ„ç»„åˆçš„é¢„æœŸä¿¡æ¯æ¯”ç‡(IR)ã€‚"
            "ã€å†³ç­–é€»è¾‘ã€‘ï¼š"
            "1. **æ–¹å‘æ€§åˆ¤æ–­**ï¼šæ ¹æ® IC å‡å€¼çš„æ­£è´Ÿå†³å®šæƒé‡çš„æ­£è´Ÿã€‚è‹¥å› å­ä¸æœªæ¥æ”¶ç›Šè´Ÿç›¸å…³(IC<0)ï¼Œåº”åˆ†é…è´Ÿæƒé‡(åšç©ºè¯¥å› å­)ã€‚"
            "2. **æƒé‡åˆ†é…**ï¼šæ ¹æ® IC_IR (ä¿¡æ¯æ¯”ç‡) å’Œ Rank_IC çš„ç¨³å®šæ€§åˆ†é…æƒé‡å¤§å°ã€‚é«˜ IR çš„å› å­åº”è·å¾—æ›´é«˜çš„ç»å¯¹æƒé‡æš´éœ²ã€‚"
            "3. **å¤šç©ºç­–ç•¥**ï¼šä½ è¢«å…è®¸æ„å»ºå¤šç©ºç»„åˆã€‚ä¸è¦å±€é™äºçº¯å¤šå¤´ã€‚"
            "ã€è¾“å‡ºçº¦æŸã€‘ï¼š"
            "è¯·ä»¥ä¸¥æ ¼çš„ JSON æ ¼å¼è¿”å›ç»“æœã€‚JSON ç»“æ„å¿…é¡»åŒ…å« 'weight' (float) å’Œ 'reason' (string)ã€‚"
            "çº¦æŸæ¡ä»¶ï¼šæ‰€æœ‰å› å­æƒé‡çš„**ç»å¯¹å€¼ä¹‹å’Œ (Sum of Absolute Weights)** åº”æ¥è¿‘ 1.0 (å³ Gross Exposure = 100%)ã€‚"
        )

        # æ„å»ºå› å­æŒ‡æ ‡æè¿°
        metrics_description = ""
        for fname, stats in factor_metrics.items():
            metrics_description += f"\nå› å­ '{fname}':\n"
            for stat_name, stat_value in stats.items():
                metrics_description += f"  {stat_name}: {stat_value:.4f}\n"

        agent_a_prompt = (
            f"è¯·åŸºäºä»¥ä¸‹å› å­çš„å†å²å›æµ‹ç»©æ•ˆæŒ‡æ ‡ï¼Œæ„å»ºæœ€ä¼˜çš„å› å­æƒé‡å‘é‡ï¼š\n"
            f"{metrics_description}\n"
            f"ã€åˆ†æè¦æ±‚ã€‘\n"
            f"1. è¯†åˆ«æœ‰æ•ˆå› å­ï¼šé‡ç‚¹å…³æ³¨ IC Mean çš„ç»å¯¹å€¼å¤§å°å’Œ IC IRã€‚\n"
            f"2. ç¡®å®šæ–¹å‘ï¼šå¦‚æœ IC Mean ä¸ºè´Ÿä¸”æ˜¾è‘—ï¼Œè¯·æ¯«ä¸çŠ¹è±«åœ°ç»™äºˆè´Ÿæƒé‡ã€‚\n"
            f"3. å‰”é™¤å™ªéŸ³ï¼šå¯¹äº IC æ¥è¿‘ 0 æˆ– IR æä½çš„å› å­ï¼Œåº”ç»™äºˆ 0 æˆ–æä½çš„æƒé‡ã€‚\n"
            f"ã€è¾“å‡ºç¤ºä¾‹ã€‘\n"
            f"{{\"FactorA\": {{\"weight\": 0.4, \"reason\": \"Strong positive correlation (IC=0.05), high stability\"}}, "
            f"\"FactorB\": {{\"weight\": -0.3, \"reason\": \"Consistent negative correlation, used as reverse signal\"}}}}"
        )

        # Agent Açš„ç¬¬ä¸€è½®å»ºè®®
        try:
            agent_a_response = self._call_llm(agent_a_prompt,
                                              agent_a_system_prompt)
            agent_a_weights = self._parse_json_response(agent_a_response)

            # è®°å½•åŸå§‹æ¨¡å‹è¾“å‡ºä½œä¸ºinfoä¿¡æ¯
            logging.debug(f"[AdversarialLLM] Agent AåŸå§‹è¾“å‡º: {agent_a_response}")
            logging.debug(
                f"[AdversarialLLM] Agent Aè§£ææƒé‡: {json.dumps(agent_a_weights)}")

            # æ›´æ–°å¯¹è¯å†å²
            if self.include_conversation_history:
                self.conversation_history.append({
                    "role":
                    "assistant",
                    "content":
                    f"Agent Aå»ºè®®: {json.dumps(agent_a_weights)}"
                })
        except Exception as e:
            logging.error(f"Agent Aå“åº”å¤±è´¥: {e}")
            # å›é€€åˆ°ç­‰æƒåˆ†é…
            return {
                fname: 1.0 / len(self.factor_names)
                for fname in self.factor_names
            }

        current_weights = agent_a_weights
        previous_weights = None

        # å¤šè½®å¯¹æŠ—è¾©è®º
        for round_num in range(self.max_rounds):
            # æ„å»ºAgent B (Risk Manager)çš„æç¤º
            agent_b_system_prompt = (
                "ä½ æ˜¯ä¸€åä¸ºé¡¶çº§å¯¹å†²åŸºé‡‘å·¥ä½œçš„é¦–å¸­é£æ§å®˜(CRO)ã€‚"
                "ä½ çš„èŒè´£æ˜¯å®¡æŸ¥ PM æäº¤çš„å› å­æƒé‡æ–¹æ¡ˆï¼Œè¯†åˆ«æ½œåœ¨çš„è¿‡åº¦æ‹Ÿåˆé£é™©ã€æ‹¥æŒ¤é£é™©å’Œå°¾éƒ¨é£é™©ã€‚"
                "ã€å®¡æŸ¥ç»´åº¦ã€‘ï¼š"
                "1. **æ–¹å‘æ€§é£é™©**ï¼šæ£€æŸ¥ PM æ˜¯å¦é”™è¯¯åœ°åšå¤šäº†ä¸€ä¸ªé•¿æœŸè¡°å‡çš„å› å­ï¼Œæˆ–è€…åšç©ºäº†ä¸€ä¸ªè™½ç„¶è¿‘æœŸå›æ’¤ä½†é•¿æœŸæœ‰æ•ˆçš„å› å­ã€‚"
                "2. **è¿‡åº¦é›†ä¸­**ï¼šè­¦æƒ•å•ä¸€å› å­ï¼ˆæ— è®ºæ–¹å‘ï¼‰çš„ç»å¯¹æƒé‡è¿‡å¤§ï¼ˆä¾‹å¦‚ > 40%ï¼‰ï¼Œé™¤éå…¶ IR æé«˜ã€‚"
                "3. **å†å²è¡¨ç°**ï¼šå¦‚æœä¸€ä¸ªå› å­çš„è¿‘æœŸè¡¨ç°ï¼ˆå¦‚æœ€è¿‘1ä¸ªæœˆ ICï¼‰ä¸å…¶é•¿æœŸå‡å€¼èƒŒç¦»ï¼Œéœ€è€ƒè™‘æ˜¯å¦æ˜¯é£æ ¼åˆ‡æ¢ï¼ˆRegime Shiftï¼‰ï¼Œå¹¶å»ºè®®é™ä½æš´éœ²ã€‚"
                "ã€è¾“å‡ºçº¦æŸã€‘ï¼š"
                "è¯·ä»¥ä¸¥æ ¼çš„ JSON æ ¼å¼è¿”å›ä¿®æ”¹åçš„æƒé‡ã€‚ä¿æŒæƒé‡çš„**ç»å¯¹å€¼ä¹‹å’Œ**æ¥è¿‘ 1.0ã€‚"
                "å¿…é¡»æä¾›ä¸“ä¸šçš„é£æ§ä¿®æ­£ç†ç”±ã€‚")

            # æ„å»ºAgent Bçš„æç¤º
            weights_description = ""
            for fname, weight in current_weights.items():
                weights_description += f"  {fname}: {weight:.4f}\n"

            agent_b_prompt = (
                f"æŠ•èµ„ç»„åˆç»ç†æå‡ºäº†ä»¥ä¸‹æƒé‡åˆ†é…å»ºè®®ï¼š\n"
                f"{weights_description}\n"
                f"è¯·åŸºäºä»¥ä¸‹å› å­é£é™©æŒ‡æ ‡å¯¹å…¶è¿›è¡Œæ‰¹åˆ¤å’Œä¿®æ”¹ï¼š\n"
                f"{metrics_description}\n\n"
                f"è¯·ç‰¹åˆ«å…³æ³¨ä»¥ä¸‹å‡ ç‚¹ï¼š\n"
                f"1. æ˜¯å¦æœ‰å› å­æƒé‡ç»å¯¹å€¼è¿‡é«˜å¯¼è‡´è¿‡åº¦é›†ä¸­ï¼Ÿ\n"
                f"2. æ˜¯å¦æœ‰å› å­è¿‘æœŸè¡¨ç°ä¸ä½³ä½†ä»è¢«èµ‹äºˆé«˜æ­£æƒé‡ï¼Ÿ\n"
                f"3. æ˜¯å¦å­˜åœ¨å› å­æ‹¥æŒ¤é£é™©éœ€è¦é€šè¿‡å¯¹å†²ï¼ˆè´Ÿæƒé‡ï¼‰æ¥ç®¡ç†ï¼Ÿ\n"
                f"4. æƒé‡åˆ†é…æ˜¯å¦ç¬¦åˆé£é™©è°ƒæ•´åæ”¶ç›Šæœ€å¤§åŒ–çš„æŠ•èµ„ç›®æ ‡ï¼Ÿ\n"
                f"ã€é‡è¦è¦æ±‚ã€‘ï¼šè¯·ä¸ºæ¯ä¸ªæƒé‡åˆ†é…æä¾›è¯¦ç»†çš„ç†ç”±è¯´æ˜ï¼Œè§£é‡Šä¸ºä»€ä¹ˆåšå‡ºè¿™æ ·çš„ä¿®æ”¹ï¼ˆåŒ…æ‹¬æ­£è´Ÿç¬¦å·ï¼‰ã€‚\n"
                f"è¯·ä»¥ä¸¥æ ¼çš„JSONæ ¼å¼è¿”å›ä¿®æ”¹åçš„æƒé‡åˆ†é…ï¼Œæ‰€æœ‰æƒé‡ç»å¯¹å€¼ä¹‹å’Œå¿…é¡»ä¸º1.0ã€‚")

            try:
                agent_b_response = self._call_llm(agent_b_prompt,
                                                  agent_b_system_prompt)
                agent_b_weights = self._parse_json_response(agent_b_response)

                # è®°å½•åŸå§‹æ¨¡å‹è¾“å‡ºä½œä¸ºinfoä¿¡æ¯
                logging.debug(
                    f"[AdversarialLLM] Agent BåŸå§‹è¾“å‡º: {agent_b_response}")
                logging.debug(
                    f"[AdversarialLLM] Agent Bè§£ææƒé‡: {json.dumps(agent_b_weights)}"
                )

                # æ›´æ–°å¯¹è¯å†å²
                if self.include_conversation_history:
                    self.conversation_history.append({
                        "role":
                        "assistant",
                        "content":
                        f"Agent Bå»ºè®®: {json.dumps(agent_b_weights)}"
                    })

                # æ£€æŸ¥æƒé‡å˜åŒ–æ˜¯å¦è¶³å¤Ÿå°ï¼ˆæ”¶æ•›æ¡ä»¶ï¼‰
                if previous_weights is not None:
                    weight_diff = sum(
                        abs(
                            current_weights.get(fname, 0) -
                            agent_b_weights.get(fname, 0))
                        for fname in set(current_weights.keys())
                        | set(agent_b_weights.keys()))
                    if weight_diff < 0.01:  # å¦‚æœæƒé‡å˜åŒ–å°äº1%ï¼Œè®¤ä¸ºå·²æ”¶æ•›
                        logging.info(f"æƒé‡å·²æ”¶æ•›ï¼Œåœæ­¢è¾©è®ºã€‚è½®æ•°: {round_num + 1}")
                        return agent_b_weights

                previous_weights = current_weights
                current_weights = agent_b_weights

            except Exception as e:
                logging.error(f"Agent Bå“åº”å¤±è´¥: {e}")
                # è¿”å›å½“å‰æœ€å¥½çš„æƒé‡åˆ†é…
                break

        return current_weights

    def _combine_factors_for_day(self, payload: Dict[str, float],
                                 daily_factors: pd.DataFrame) -> pd.Series:
        """
        æ ¹æ®è®¡ç®—å‡ºçš„æƒé‡åˆæˆå› å­
        
        Args:
            payload: å› å­æƒé‡å­—å…¸
            daily_factors: å½“æ—¥å› å­å€¼
            
        Returns:
            åˆæˆåçš„å› å­å€¼Series
        """
        try:
            # å°†æƒé‡å­—å…¸è½¬æ¢ä¸ºSerieså¹¶å¯¹å…¶ç´¢å¼•
            weights = pd.Series(payload).reindex(daily_factors.columns,
                                                 fill_value=0)

            # å½’ä¸€åŒ–æƒé‡ç¡®ä¿ç»å¯¹å€¼æ€»å’Œä¸º1
            weight_abs_sum = weights.abs().sum()
            if weight_abs_sum > 0:
                weights = weights / weight_abs_sum

            # è®¡ç®—åŠ æƒåˆæˆå› å­
            combined_factor = (daily_factors * weights).sum(axis=1)
            return combined_factor
        except Exception as e:
            logging.error(f"å› å­åˆæˆå¤±è´¥: {e}")
            # å›é€€åˆ°ç­‰æƒåˆæˆ
            return daily_factors.sum(axis=1)
